-- Logs begin at Thu 2025-03-13 01:19:03 UTC, end at Fri 2025-03-14 05:58:32 UTC. --
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:53.019Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="5.8 GiB" free_swap="0 B"
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:53.019Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:53.019Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.key_length default=64
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:53.019Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.value_length default=64
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:53.019Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:53.019Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=13 layers.offload=0 layers.split="" memory.available="[5.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="352.9 MiB" memory.required.partial="0 B" memory.required.kv="24.0 MiB" memory.required.allocations="[352.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 1
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: model type       = ?B
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_load: vocab only - skipping tensors
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:53.054Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 1 --port 39529"
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:53.055Z level=INFO source=sched.go:450 msg="loaded runners" count=1
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:53.055Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:53.055Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:53.066Z level=INFO source=runner.go:931 msg="starting go runner"
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:53.108Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:53.109Z level=INFO source=runner.go:991 msg="Server listening on 127.0.0.1:39529"
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 0
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: n_ctx_train      = 2048
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: n_embd           = 768
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: n_layer          = 12
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: n_head           = 12
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: n_head_kv        = 12
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: n_rot            = 64
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: n_swa            = 0
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_k    = 64
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_v    = 64
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: n_gqa            = 1
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: n_embd_k_gqa     = 768
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: n_embd_v_gqa     = 768
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: f_norm_eps       = 1.0e-12
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: f_norm_rms_eps   = 0.0e+00
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: f_clamp_kqv      = 0.0e+00
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: f_max_alibi_bias = 0.0e+00
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: f_logit_scale    = 0.0e+00
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: n_ff             = 3072
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: n_expert         = 0
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: n_expert_used    = 0
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: causal attn      = 0
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: pooling type     = 1
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: rope type        = 2
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: rope scaling     = linear
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: freq_base_train  = 1000.0
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: freq_scale_train = 1
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: n_ctx_orig_yarn  = 2048
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: rope_finetuned   = unknown
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: ssm_d_conv       = 0
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: ssm_d_inner      = 0
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: ssm_d_state      = 0
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_rank      = 0
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_b_c_rms   = 0
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: model type       = 137M
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: load_tensors: loading model tensors, this can take a while... (mmap = false)
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: load_tensors:          CPU model buffer size =   260.86 MiB
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_seq_max     = 1
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx         = 8192
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq = 8192
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_batch       = 512
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ubatch      = 512
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_init_from_model: flash_attn    = 0
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_base     = 1000.0
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_scale    = 1
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_init_from_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU  output buffer size =     0.00 MiB
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU compute buffer size =    23.00 MiB
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph nodes  = 453
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph splits = 1
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:53.306Z level=INFO source=server.go:624 msg="llama runner started in 0.25 seconds"
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 05:28:53 | 200 |  406.977128ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:53.573Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="5.3 GiB" free_swap="0 B"
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:53.573Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=27 layers.offload=0 layers.split="" memory.available="[5.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="1.5 GiB" memory.required.partial="0 B" memory.required.kv="208.0 MiB" memory.required.allocations="[1.5 GiB]" memory.weights.total="664.5 MiB" memory.weights.repeating="358.5 MiB" memory.weights.nonrepeating="306.0 MiB" memory.graph.full="514.2 MiB" memory.graph.partial="750.5 MiB"
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:53.678Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:53.680Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:53.684Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:53.684Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:53.684Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.num_channels default=0
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:53.684Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.block_count default=0
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:53.684Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.embedding_length default=0
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:53.684Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.head_count default=0
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:53.684Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:53.684Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:53.684Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.layer_norm_epsilon default=0
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:53.684Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:53.689Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.freq_scale default=1
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:53.689Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.mm_tokens_per_image default=256
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:53.689Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --ollama-engine --model /usr/share/ollama/.ollama/models/blobs/sha256-dbe81da1e4bad3cc6ccb77540915ffe53e7a3ac0745ffa5e9d4626d39c15e09a --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 4 --port 38093"
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:53.690Z level=INFO source=sched.go:450 msg="loaded runners" count=2
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:53.690Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:53.690Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:53.703Z level=INFO source=runner.go:882 msg="starting ollama engine"
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:53.704Z level=INFO source=runner.go:938 msg="Server listening on 127.0.0.1:38093"
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:53.814Z level=WARN source=ggml.go:149 msg="key not found" key=general.name default=""
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:53.814Z level=WARN source=ggml.go:149 msg="key not found" key=general.description default=""
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:53.815Z level=INFO source=ggml.go:67 msg="" architecture=gemma3 file_type=Q4_K_M name="" description="" num_tensors=340 num_key_values=32
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:53.838Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:53.839Z level=INFO source=ggml.go:289 msg="model weights" buffer=CPU size="1.0 GiB"
Mar 13 05:28:53 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:53.943Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server loading model"
Mar 13 05:28:54 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:54.296Z level=INFO source=ggml.go:356 msg="compute graph" backend=CPU buffer_type=CPU
Mar 13 05:28:54 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:54.296Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 05:28:54 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:54.298Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
Mar 13 05:28:54 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:54.302Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 05:28:54 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:54.302Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 05:28:54 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:54.302Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.num_channels default=0
Mar 13 05:28:54 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:54.302Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.block_count default=0
Mar 13 05:28:54 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:54.302Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.embedding_length default=0
Mar 13 05:28:54 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:54.302Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.head_count default=0
Mar 13 05:28:54 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:54.302Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 05:28:54 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:54.302Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 05:28:54 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:54.302Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.layer_norm_epsilon default=0
Mar 13 05:28:54 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:54.302Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 05:28:54 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:54.308Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.freq_scale default=1
Mar 13 05:28:54 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:54.308Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.mm_tokens_per_image default=256
Mar 13 05:28:54 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:28:54.445Z level=INFO source=server.go:624 msg="llama runner started in 0.76 seconds"
Mar 13 05:30:26 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 05:30:26 | 200 |         1m33s |       127.0.0.1 | POST     "/api/generate"
Mar 13 05:30:46 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 05:30:46 | 200 |      30.668Âµs |       127.0.0.1 | HEAD     "/"
Mar 13 05:30:46 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 05:30:46 | 200 |   46.352944ms |       127.0.0.1 | POST     "/api/show"
Mar 13 05:30:46 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 05:30:46 | 200 |   45.907734ms |       127.0.0.1 | POST     "/api/generate"
Mar 13 05:31:01 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 05:31:01 | 200 |  6.619979904s |       127.0.0.1 | POST     "/api/chat"
Mar 13 05:31:25 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 05:31:25 | 200 | 11.740422882s |       127.0.0.1 | POST     "/api/chat"
Mar 13 05:32:02 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 05:32:02 | 200 | 19.881128401s |       127.0.0.1 | POST     "/api/chat"
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:35:36.233Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:35:36.233Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.key_length default=64
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:35:36.233Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.value_length default=64
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:35:36.233Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:35:36.233Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="4.3 GiB" free_swap="0 B"
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:35:36.233Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:35:36.233Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.key_length default=64
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:35:36.233Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.value_length default=64
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:35:36.234Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:35:36.234Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=13 layers.offload=0 layers.split="" memory.available="[4.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="352.9 MiB" memory.required.partial="0 B" memory.required.kv="24.0 MiB" memory.required.allocations="[352.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 1
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: model type       = ?B
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_load: vocab only - skipping tensors
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:35:36.272Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 1 --port 41233"
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:35:36.273Z level=INFO source=sched.go:450 msg="loaded runners" count=2
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:35:36.273Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:35:36.273Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:35:36.283Z level=INFO source=runner.go:931 msg="starting go runner"
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:35:36.324Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:35:36.325Z level=INFO source=runner.go:991 msg="Server listening on 127.0.0.1:41233"
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 0
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: n_ctx_train      = 2048
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: n_embd           = 768
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: n_layer          = 12
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: n_head           = 12
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: n_head_kv        = 12
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: n_rot            = 64
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: n_swa            = 0
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_k    = 64
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_v    = 64
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: n_gqa            = 1
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: n_embd_k_gqa     = 768
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: n_embd_v_gqa     = 768
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: f_norm_eps       = 1.0e-12
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: f_norm_rms_eps   = 0.0e+00
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: f_clamp_kqv      = 0.0e+00
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: f_max_alibi_bias = 0.0e+00
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: f_logit_scale    = 0.0e+00
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: n_ff             = 3072
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: n_expert         = 0
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: n_expert_used    = 0
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: causal attn      = 0
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: pooling type     = 1
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: rope type        = 2
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: rope scaling     = linear
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: freq_base_train  = 1000.0
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: freq_scale_train = 1
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: n_ctx_orig_yarn  = 2048
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: rope_finetuned   = unknown
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: ssm_d_conv       = 0
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: ssm_d_inner      = 0
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: ssm_d_state      = 0
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_rank      = 0
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_b_c_rms   = 0
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: model type       = 137M
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: load_tensors: loading model tensors, this can take a while... (mmap = false)
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: load_tensors:          CPU model buffer size =   260.86 MiB
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:35:36.525Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server loading model"
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_seq_max     = 1
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx         = 8192
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq = 8192
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_batch       = 512
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ubatch      = 512
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_init_from_model: flash_attn    = 0
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_base     = 1000.0
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_scale    = 1
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_init_from_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU  output buffer size =     0.00 MiB
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU compute buffer size =    23.00 MiB
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph nodes  = 453
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph splits = 1
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:35:36.776Z level=INFO source=server.go:624 msg="llama runner started in 0.50 seconds"
Mar 13 05:35:36 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 05:35:36 | 200 |  672.654077ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 05:36:53 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 05:36:53 | 200 |         1m16s |       127.0.0.1 | POST     "/api/generate"
Mar 13 05:37:39 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 05:37:39 | 200 |  130.104783ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 05:38:54 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 05:38:54 | 200 |         1m15s |       127.0.0.1 | POST     "/api/generate"
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:16.664Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="5.8 GiB" free_swap="0 B"
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:16.664Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:16.664Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.key_length default=64
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:16.664Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.value_length default=64
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:16.664Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:16.664Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=13 layers.offload=0 layers.split="" memory.available="[5.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="352.9 MiB" memory.required.partial="0 B" memory.required.kv="24.0 MiB" memory.required.allocations="[352.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 1
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: model type       = ?B
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_load: vocab only - skipping tensors
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:16.697Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 1 --port 37499"
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:16.697Z level=INFO source=sched.go:450 msg="loaded runners" count=1
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:16.697Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:16.698Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:16.707Z level=INFO source=runner.go:931 msg="starting go runner"
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:16.748Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:16.749Z level=INFO source=runner.go:991 msg="Server listening on 127.0.0.1:37499"
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 0
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: n_ctx_train      = 2048
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: n_embd           = 768
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: n_layer          = 12
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: n_head           = 12
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: n_head_kv        = 12
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: n_rot            = 64
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: n_swa            = 0
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_k    = 64
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_v    = 64
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: n_gqa            = 1
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: n_embd_k_gqa     = 768
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: n_embd_v_gqa     = 768
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: f_norm_eps       = 1.0e-12
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: f_norm_rms_eps   = 0.0e+00
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: f_clamp_kqv      = 0.0e+00
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: f_max_alibi_bias = 0.0e+00
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: f_logit_scale    = 0.0e+00
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: n_ff             = 3072
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: n_expert         = 0
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: n_expert_used    = 0
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: causal attn      = 0
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: pooling type     = 1
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: rope type        = 2
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: rope scaling     = linear
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: freq_base_train  = 1000.0
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: freq_scale_train = 1
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: n_ctx_orig_yarn  = 2048
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: rope_finetuned   = unknown
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: ssm_d_conv       = 0
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: ssm_d_inner      = 0
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: ssm_d_state      = 0
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_rank      = 0
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_b_c_rms   = 0
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: model type       = 137M
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: load_tensors: loading model tensors, this can take a while... (mmap = false)
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: load_tensors:          CPU model buffer size =   260.86 MiB
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_seq_max     = 1
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx         = 8192
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq = 8192
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_batch       = 512
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ubatch      = 512
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_init_from_model: flash_attn    = 0
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_base     = 1000.0
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_scale    = 1
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_init_from_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU  output buffer size =     0.00 MiB
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU compute buffer size =    23.00 MiB
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph nodes  = 453
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph splits = 1
Mar 13 05:44:16 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:16.949Z level=INFO source=server.go:624 msg="llama runner started in 0.25 seconds"
Mar 13 05:44:17 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 05:44:17 | 200 |  404.480303ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 05:44:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:17.251Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="5.2 GiB" free_swap="0 B"
Mar 13 05:44:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:17.251Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=27 layers.offload=0 layers.split="" memory.available="[5.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="1.5 GiB" memory.required.partial="0 B" memory.required.kv="208.0 MiB" memory.required.allocations="[1.5 GiB]" memory.weights.total="664.5 MiB" memory.weights.repeating="358.5 MiB" memory.weights.nonrepeating="306.0 MiB" memory.graph.full="514.2 MiB" memory.graph.partial="750.5 MiB"
Mar 13 05:44:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:17.357Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 05:44:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:17.359Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
Mar 13 05:44:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:17.363Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 05:44:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:17.363Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 05:44:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:17.363Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.num_channels default=0
Mar 13 05:44:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:17.363Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.block_count default=0
Mar 13 05:44:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:17.363Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.embedding_length default=0
Mar 13 05:44:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:17.363Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.head_count default=0
Mar 13 05:44:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:17.363Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 05:44:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:17.363Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 05:44:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:17.363Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.layer_norm_epsilon default=0
Mar 13 05:44:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:17.363Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 05:44:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:17.381Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.freq_scale default=1
Mar 13 05:44:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:17.381Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.mm_tokens_per_image default=256
Mar 13 05:44:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:17.381Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --ollama-engine --model /usr/share/ollama/.ollama/models/blobs/sha256-dbe81da1e4bad3cc6ccb77540915ffe53e7a3ac0745ffa5e9d4626d39c15e09a --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 4 --port 32895"
Mar 13 05:44:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:17.382Z level=INFO source=sched.go:450 msg="loaded runners" count=2
Mar 13 05:44:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:17.382Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 05:44:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:17.382Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 05:44:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:17.399Z level=INFO source=runner.go:882 msg="starting ollama engine"
Mar 13 05:44:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:17.399Z level=INFO source=runner.go:938 msg="Server listening on 127.0.0.1:32895"
Mar 13 05:44:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:17.502Z level=WARN source=ggml.go:149 msg="key not found" key=general.name default=""
Mar 13 05:44:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:17.502Z level=WARN source=ggml.go:149 msg="key not found" key=general.description default=""
Mar 13 05:44:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:17.502Z level=INFO source=ggml.go:67 msg="" architecture=gemma3 file_type=Q4_K_M name="" description="" num_tensors=340 num_key_values=32
Mar 13 05:44:17 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 05:44:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:17.544Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 05:44:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:17.546Z level=INFO source=ggml.go:289 msg="model weights" buffer=CPU size="1.0 GiB"
Mar 13 05:44:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:17.644Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server loading model"
Mar 13 05:44:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:17.997Z level=INFO source=ggml.go:356 msg="compute graph" backend=CPU buffer_type=CPU
Mar 13 05:44:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:17.997Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 05:44:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:17.999Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
Mar 13 05:44:18 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:18.004Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 05:44:18 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:18.004Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 05:44:18 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:18.004Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.num_channels default=0
Mar 13 05:44:18 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:18.004Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.block_count default=0
Mar 13 05:44:18 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:18.004Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.embedding_length default=0
Mar 13 05:44:18 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:18.004Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.head_count default=0
Mar 13 05:44:18 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:18.004Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 05:44:18 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:18.004Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 05:44:18 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:18.004Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.layer_norm_epsilon default=0
Mar 13 05:44:18 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:18.004Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 05:44:18 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:18.010Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.freq_scale default=1
Mar 13 05:44:18 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:18.010Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.mm_tokens_per_image default=256
Mar 13 05:44:18 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:44:18.146Z level=INFO source=server.go:624 msg="llama runner started in 0.76 seconds"
Mar 13 05:45:41 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 05:45:41 | 200 |         1m24s |       127.0.0.1 | POST     "/api/generate"
Mar 13 05:48:12 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 05:48:12 | 200 |  110.041109ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 05:49:17 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 05:49:17 | 200 |  143.072943ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 05:50:08 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 05:50:08 | 200 |         1m55s |       127.0.0.1 | POST     "/api/generate"
Mar 13 05:50:08 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 05:50:08 | 200 | 51.387097248s |       127.0.0.1 | POST     "/api/generate"
Mar 13 05:50:41 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 05:50:41 | 200 |   42.508443ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 05:52:21 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 05:52:21 | 200 |         1m39s |       127.0.0.1 | POST     "/api/generate"
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:57:13.884Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:57:13.884Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.key_length default=64
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:57:13.884Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.value_length default=64
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:57:13.884Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:57:13.884Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="3.4 GiB" free_swap="0 B"
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:57:13.884Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:57:13.884Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.key_length default=64
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:57:13.884Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.value_length default=64
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:57:13.884Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:57:13.884Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=13 layers.offload=0 layers.split="" memory.available="[3.4 GiB]" memory.gpu_overhead="0 B" memory.required.full="352.9 MiB" memory.required.partial="0 B" memory.required.kv="24.0 MiB" memory.required.allocations="[352.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 1
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: print_info: model type       = ?B
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_load: vocab only - skipping tensors
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:57:13.919Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 1 --port 38861"
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:57:13.919Z level=INFO source=sched.go:450 msg="loaded runners" count=2
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:57:13.919Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:57:13.919Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:57:13.929Z level=INFO source=runner.go:931 msg="starting go runner"
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:57:13.972Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:57:13.973Z level=INFO source=runner.go:991 msg="Server listening on 127.0.0.1:38861"
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 05:57:13 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 0
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: n_ctx_train      = 2048
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: n_embd           = 768
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: n_layer          = 12
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: n_head           = 12
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: n_head_kv        = 12
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: n_rot            = 64
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: n_swa            = 0
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_k    = 64
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_v    = 64
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: n_gqa            = 1
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: n_embd_k_gqa     = 768
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: n_embd_v_gqa     = 768
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: f_norm_eps       = 1.0e-12
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: f_norm_rms_eps   = 0.0e+00
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: f_clamp_kqv      = 0.0e+00
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: f_max_alibi_bias = 0.0e+00
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: f_logit_scale    = 0.0e+00
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: n_ff             = 3072
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: n_expert         = 0
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: n_expert_used    = 0
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: causal attn      = 0
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: pooling type     = 1
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: rope type        = 2
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: rope scaling     = linear
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: freq_base_train  = 1000.0
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: freq_scale_train = 1
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: n_ctx_orig_yarn  = 2048
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: rope_finetuned   = unknown
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: ssm_d_conv       = 0
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: ssm_d_inner      = 0
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: ssm_d_state      = 0
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_rank      = 0
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_b_c_rms   = 0
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: model type       = 137M
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: load_tensors: loading model tensors, this can take a while... (mmap = false)
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: load_tensors:          CPU model buffer size =   260.86 MiB
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_seq_max     = 1
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx         = 8192
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq = 8192
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_batch       = 512
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ubatch      = 512
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: llama_init_from_model: flash_attn    = 0
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_base     = 1000.0
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_scale    = 1
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: llama_init_from_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU  output buffer size =     0.00 MiB
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU compute buffer size =    23.00 MiB
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph nodes  = 453
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph splits = 1
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: time=2025-03-13T05:57:14.170Z level=INFO source=server.go:624 msg="llama runner started in 0.25 seconds"
Mar 13 05:57:14 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 05:57:14 | 200 |  356.876804ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 05:58:53 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 05:58:53 | 200 |         1m39s |       127.0.0.1 | POST     "/api/generate"
Mar 13 05:59:20 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 05:59:20 | 200 |   34.516245ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 06:00:40 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 06:00:40 | 200 |         1m19s |       127.0.0.1 | POST     "/api/generate"
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:57.818Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="5.8 GiB" free_swap="0 B"
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:57.818Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:57.818Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.key_length default=64
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:57.818Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.value_length default=64
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:57.818Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:57.818Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=13 layers.offload=0 layers.split="" memory.available="[5.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="352.9 MiB" memory.required.partial="0 B" memory.required.kv="24.0 MiB" memory.required.allocations="[352.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 1
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: model type       = ?B
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_load: vocab only - skipping tensors
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:57.853Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 1 --port 40087"
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:57.854Z level=INFO source=sched.go:450 msg="loaded runners" count=1
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:57.854Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:57.854Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:57.866Z level=INFO source=runner.go:931 msg="starting go runner"
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:57.908Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:57.909Z level=INFO source=runner.go:991 msg="Server listening on 127.0.0.1:40087"
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 0
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: n_ctx_train      = 2048
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: n_embd           = 768
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: n_layer          = 12
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: n_head           = 12
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: n_head_kv        = 12
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: n_rot            = 64
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: n_swa            = 0
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_k    = 64
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_v    = 64
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: n_gqa            = 1
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: n_embd_k_gqa     = 768
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: n_embd_v_gqa     = 768
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: f_norm_eps       = 1.0e-12
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: f_norm_rms_eps   = 0.0e+00
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: f_clamp_kqv      = 0.0e+00
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: f_max_alibi_bias = 0.0e+00
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: f_logit_scale    = 0.0e+00
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: n_ff             = 3072
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: n_expert         = 0
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: n_expert_used    = 0
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: causal attn      = 0
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: pooling type     = 1
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: rope type        = 2
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: rope scaling     = linear
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: freq_base_train  = 1000.0
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: freq_scale_train = 1
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: n_ctx_orig_yarn  = 2048
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: rope_finetuned   = unknown
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: ssm_d_conv       = 0
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: ssm_d_inner      = 0
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: ssm_d_state      = 0
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_rank      = 0
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_b_c_rms   = 0
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: model type       = 137M
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: load_tensors: loading model tensors, this can take a while... (mmap = false)
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: load_tensors:          CPU model buffer size =   260.86 MiB
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_seq_max     = 1
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx         = 8192
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq = 8192
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_batch       = 512
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ubatch      = 512
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_init_from_model: flash_attn    = 0
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_base     = 1000.0
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_scale    = 1
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
Mar 13 06:17:57 ubuntu-abhishek ollama[471085]: llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: llama_init_from_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU  output buffer size =     0.00 MiB
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU compute buffer size =    23.00 MiB
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph nodes  = 453
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph splits = 1
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:58.105Z level=INFO source=server.go:624 msg="llama runner started in 0.25 seconds"
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 06:17:58 | 200 |   364.94274ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:58.327Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="5.2 GiB" free_swap="0 B"
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:58.328Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=27 layers.offload=0 layers.split="" memory.available="[5.2 GiB]" memory.gpu_overhead="0 B" memory.required.full="1.5 GiB" memory.required.partial="0 B" memory.required.kv="208.0 MiB" memory.required.allocations="[1.5 GiB]" memory.weights.total="664.5 MiB" memory.weights.repeating="358.5 MiB" memory.weights.nonrepeating="306.0 MiB" memory.graph.full="514.2 MiB" memory.graph.partial="750.5 MiB"
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:58.439Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:58.443Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:58.447Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:58.447Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:58.447Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.num_channels default=0
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:58.447Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.block_count default=0
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:58.447Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.embedding_length default=0
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:58.447Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.head_count default=0
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:58.447Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:58.447Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:58.447Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.layer_norm_epsilon default=0
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:58.447Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:58.454Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.freq_scale default=1
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:58.454Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.mm_tokens_per_image default=256
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:58.454Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --ollama-engine --model /usr/share/ollama/.ollama/models/blobs/sha256-dbe81da1e4bad3cc6ccb77540915ffe53e7a3ac0745ffa5e9d4626d39c15e09a --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 4 --port 43097"
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:58.455Z level=INFO source=sched.go:450 msg="loaded runners" count=2
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:58.455Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:58.455Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:58.465Z level=INFO source=runner.go:882 msg="starting ollama engine"
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:58.466Z level=INFO source=runner.go:938 msg="Server listening on 127.0.0.1:43097"
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:58.570Z level=WARN source=ggml.go:149 msg="key not found" key=general.name default=""
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:58.570Z level=WARN source=ggml.go:149 msg="key not found" key=general.description default=""
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:58.570Z level=INFO source=ggml.go:67 msg="" architecture=gemma3 file_type=Q4_K_M name="" description="" num_tensors=340 num_key_values=32
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:58.575Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:58.576Z level=INFO source=ggml.go:289 msg="model weights" buffer=CPU size="1.0 GiB"
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:58.715Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server loading model"
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:58.924Z level=INFO source=ggml.go:356 msg="compute graph" backend=CPU buffer_type=CPU
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:58.924Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:58.928Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:58.932Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:58.932Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:58.932Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.num_channels default=0
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:58.932Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.block_count default=0
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:58.932Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.embedding_length default=0
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:58.932Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.head_count default=0
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:58.932Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:58.932Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:58.932Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.layer_norm_epsilon default=0
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:58.932Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:58.939Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.freq_scale default=1
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:58.939Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.mm_tokens_per_image default=256
Mar 13 06:17:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:17:58.966Z level=INFO source=server.go:624 msg="llama runner started in 0.51 seconds"
Mar 13 06:18:06 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 06:18:06 | 200 |  7.981449221s |       127.0.0.1 | POST     "/api/generate"
Mar 13 06:18:17 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 06:18:17 | 200 |  152.274692ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: ggml-alloc.c:819: GGML_ASSERT(talloc->buffer_id >= 0) failed
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: /usr/local/bin/ollama(+0x10f44e8)[0x55e5e929c4e8]
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: /usr/local/bin/ollama(+0x10f4866)[0x55e5e929c866]
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: /usr/local/bin/ollama(+0x10e1cc5)[0x55e5e9289cc5]
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: /usr/local/bin/ollama(+0x10e335b)[0x55e5e928b35b]
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: /usr/local/bin/ollama(+0x11081e5)[0x55e5e92b01e5]
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: /usr/local/bin/ollama(+0x110863b)[0x55e5e92b063b]
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: /usr/local/bin/ollama(+0x1158ffb)[0x55e5e9300ffb]
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: /usr/local/bin/ollama(+0x331ca1)[0x55e5e84d9ca1]
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: SIGABRT: abort
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: PC=0x7fe5169d000b m=9 sigcode=18446744073709551610
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: signal arrived during cgo execution
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: goroutine 10 gp=0xc000102a80 m=9 mp=0xc0002fa008 [syscall]:
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.cgocall(0x55e5e9300fe0, 0xc000167b00)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/cgocall.go:167 +0x4b fp=0xc000167ad8 sp=0xc000167aa0 pc=0x55e5e84cf60b
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: github.com/ollama/ollama/ml/backend/ggml._Cfunc_ggml_backend_sched_graph_compute_async(0x7fe4bc000d40, 0x7fe46108aa60)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         _cgo_gotypes.go:485 +0x4a fp=0xc000167b00 sp=0xc000167ad8 pc=0x55e5e88ba1aa
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: github.com/ollama/ollama/ml/backend/ggml.Context.Compute.func1(...)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         github.com/ollama/ollama/ml/backend/ggml/ggml.go:497
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: github.com/ollama/ollama/ml/backend/ggml.Context.Compute({0xc000132000, 0x7fe46033af10, 0x7fe46108aa60, 0x0, 0x2000}, {0xc0486894b0, 0x1, 0x55e5e97bc210?})
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         github.com/ollama/ollama/ml/backend/ggml/ggml.go:497 +0xbd fp=0xc000167b90 sp=0xc000167b00 pc=0x55e5e88c2a7d
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: github.com/ollama/ollama/ml/backend/ggml.(*Context).Compute(0xc0486b5c80?, {0xc0486894b0?, 0x200?, 0x0?})
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         <autogenerated>:1 +0x72 fp=0xc000167c08 sp=0xc000167b90 pc=0x55e5e88c84f2
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: github.com/ollama/ollama/model.Forward({0x55e5e97b3c00, 0xc0486b5c80}, {0x55e5e97ab2b0, 0xc0002ca000}, {{0xc048738000, 0x200, 0x200}, {0x0, 0x0, 0x0}, ...})
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         github.com/ollama/ollama/model/model.go:303 +0x218 fp=0xc000167cf0 sp=0xc000167c08 pc=0x55e5e88ef718
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: github.com/ollama/ollama/runner/ollamarunner.(*Server).processBatch(0xc00015f0e0)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         github.com/ollama/ollama/runner/ollamarunner/runner.go:395 +0x3bb fp=0xc000167f98 sp=0xc000167cf0 pc=0x55e5e895bdfb
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: github.com/ollama/ollama/runner/ollamarunner.(*Server).run(0xc00015f0e0, {0x55e5e97ac5e0, 0xc00049d540})
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         github.com/ollama/ollama/runner/ollamarunner/runner.go:321 +0x4e fp=0xc000167fb8 sp=0xc000167f98 pc=0x55e5e895b9ee
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: github.com/ollama/ollama/runner/ollamarunner.Execute.gowrap2()
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         github.com/ollama/ollama/runner/ollamarunner/runner.go:919 +0x28 fp=0xc000167fe0 sp=0xc000167fb8 pc=0x55e5e8960668
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.goexit({})
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc000167fe8 sp=0xc000167fe0 pc=0x55e5e84da021
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: created by github.com/ollama/ollama/runner/ollamarunner.Execute in goroutine 1
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         github.com/ollama/ollama/runner/ollamarunner/runner.go:919 +0xa9c
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: goroutine 1 gp=0xc000002380 m=nil [IO wait]:
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/proc.go:435 +0xce fp=0xc000163648 sp=0xc000163628 pc=0x55e5e84d28ee
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.netpollblock(0xc000163698?, 0xe846c226?, 0xe5?)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/netpoll.go:575 +0xf7 fp=0xc000163680 sp=0xc000163648 pc=0x55e5e84976f7
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: internal/poll.runtime_pollWait(0x7fe4c7fc7eb0, 0x72)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/netpoll.go:351 +0x85 fp=0xc0001636a0 sp=0xc000163680 pc=0x55e5e84d1b05
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: internal/poll.(*pollDesc).wait(0xc0004b8880?, 0x900475cfe?, 0x0)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         internal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc0001636c8 sp=0xc0001636a0 pc=0x55e5e8558f87
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: internal/poll.(*pollDesc).waitRead(...)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         internal/poll/fd_poll_runtime.go:89
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: internal/poll.(*FD).Accept(0xc0004b8880)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         internal/poll/fd_unix.go:620 +0x295 fp=0xc000163770 sp=0xc0001636c8 pc=0x55e5e855e355
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: net.(*netFD).accept(0xc0004b8880)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         net/fd_unix.go:172 +0x29 fp=0xc000163828 sp=0xc000163770 pc=0x55e5e85d1169
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: net.(*TCPListener).accept(0xc000406000)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         net/tcpsock_posix.go:159 +0x1b fp=0xc000163878 sp=0xc000163828 pc=0x55e5e85e6b1b
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: net.(*TCPListener).Accept(0xc000406000)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         net/tcpsock.go:380 +0x30 fp=0xc0001638a8 sp=0xc000163878 pc=0x55e5e85e59d0
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: net/http.(*onceCloseListener).Accept(0xc0000dddd0?)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         <autogenerated>:1 +0x24 fp=0xc0001638c0 sp=0xc0001638a8 pc=0x55e5e87fcb44
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: net/http.(*Server).Serve(0xc000542100, {0x55e5e97aa318, 0xc000406000})
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         net/http/server.go:3424 +0x30c fp=0xc0001639f0 sp=0xc0001638c0 pc=0x55e5e87d440c
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: github.com/ollama/ollama/runner/ollamarunner.Execute({0xc000034130, 0xd, 0xd})
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         github.com/ollama/ollama/runner/ollamarunner/runner.go:939 +0xe6a fp=0xc000163d08 sp=0xc0001639f0 pc=0x55e5e89603aa
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: github.com/ollama/ollama/runner.Execute({0xc000034110?, 0x0?, 0x0?})
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         github.com/ollama/ollama/runner/runner.go:20 +0xc9 fp=0xc000163d30 sp=0xc000163d08 pc=0x55e5e8960f09
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: github.com/ollama/ollama/cmd.NewCLI.func2(0xc000035500?, {0x55e5e931c054?, 0x4?, 0x55e5e931c058?})
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         github.com/ollama/ollama/cmd/cmd.go:1285 +0x45 fp=0xc000163d58 sp=0xc000163d30 pc=0x55e5e90d16a5
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: github.com/spf13/cobra.(*Command).execute(0xc0004bd208, {0xc00014cd20, 0xe, 0xe})
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         github.com/spf13/cobra@v1.7.0/command.go:940 +0x85c fp=0xc000163e78 sp=0xc000163d58 pc=0x55e5e864a2fc
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: github.com/spf13/cobra.(*Command).ExecuteC(0xc00015c608)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         github.com/spf13/cobra@v1.7.0/command.go:1068 +0x3a5 fp=0xc000163f30 sp=0xc000163e78 pc=0x55e5e864ab45
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: github.com/spf13/cobra.(*Command).Execute(...)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         github.com/spf13/cobra@v1.7.0/command.go:992
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: github.com/spf13/cobra.(*Command).ExecuteContext(...)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         github.com/spf13/cobra@v1.7.0/command.go:985
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: main.main()
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         github.com/ollama/ollama/main.go:12 +0x4d fp=0xc000163f50 sp=0xc000163f30 pc=0x55e5e90d1a0d
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.main()
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/proc.go:283 +0x29d fp=0xc000163fe0 sp=0xc000163f50 pc=0x55e5e849ecfd
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.goexit({})
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc000163fe8 sp=0xc000163fe0 pc=0x55e5e84da021
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: goroutine 2 gp=0xc000002e00 m=nil [force gc (idle)]:
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.gopark(0x0?, 0x0?, 0x0?, 0x0?, 0x0?)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/proc.go:435 +0xce fp=0xc000060fa8 sp=0xc000060f88 pc=0x55e5e84d28ee
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.goparkunlock(...)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/proc.go:441
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.forcegchelper()
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/proc.go:348 +0xb8 fp=0xc000060fe0 sp=0xc000060fa8 pc=0x55e5e849f038
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.goexit({})
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc000060fe8 sp=0xc000060fe0 pc=0x55e5e84da021
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: created by runtime.init.7 in goroutine 1
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/proc.go:336 +0x1a
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: goroutine 3 gp=0xc000003340 m=nil [GC sweep wait]:
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.gopark(0x1?, 0x0?, 0x0?, 0x0?, 0x0?)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/proc.go:435 +0xce fp=0xc000061780 sp=0xc000061760 pc=0x55e5e84d28ee
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.goparkunlock(...)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/proc.go:441
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.bgsweep(0xc00008a000)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/mgcsweep.go:316 +0xdf fp=0xc0000617c8 sp=0xc000061780 pc=0x55e5e848985f
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.gcenable.gowrap1()
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/mgc.go:204 +0x25 fp=0xc0000617e0 sp=0xc0000617c8 pc=0x55e5e847dc45
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.goexit({})
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc0000617e8 sp=0xc0000617e0 pc=0x55e5e84da021
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: created by runtime.gcenable in goroutine 1
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/mgc.go:204 +0x66
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: goroutine 4 gp=0xc000003500 m=nil [GC scavenge wait]:
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.gopark(0x10000?, 0x55e5e94d2a00?, 0x0?, 0x0?, 0x0?)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/proc.go:435 +0xce fp=0xc000061f78 sp=0xc000061f58 pc=0x55e5e84d28ee
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.goparkunlock(...)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/proc.go:441
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.(*scavengerState).park(0x55e5ea010b40)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/mgcscavenge.go:425 +0x49 fp=0xc000061fa8 sp=0xc000061f78 pc=0x55e5e84872a9
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.bgscavenge(0xc00008a000)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/mgcscavenge.go:658 +0x59 fp=0xc000061fc8 sp=0xc000061fa8 pc=0x55e5e8487839
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.gcenable.gowrap2()
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/mgc.go:205 +0x25 fp=0xc000061fe0 sp=0xc000061fc8 pc=0x55e5e847dbe5
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.goexit({})
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc000061fe8 sp=0xc000061fe0 pc=0x55e5e84da021
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: created by runtime.gcenable in goroutine 1
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/mgc.go:205 +0xa5
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: goroutine 5 gp=0xc000003dc0 m=nil [finalizer wait]:
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.gopark(0x1b8?, 0xc000002380?, 0x1?, 0x23?, 0xc000060688?)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/proc.go:435 +0xce fp=0xc000060630 sp=0xc000060610 pc=0x55e5e84d28ee
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.runfinq()
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/mfinal.go:196 +0x107 fp=0xc0000607e0 sp=0xc000060630 pc=0x55e5e847cc07
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.goexit({})
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc0000607e8 sp=0xc0000607e0 pc=0x55e5e84da021
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: created by runtime.createfing in goroutine 1
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/mfinal.go:166 +0x3d
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: goroutine 6 gp=0xc0001c48c0 m=nil [chan receive]:
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.gopark(0xc000215900?, 0xc000216030?, 0x60?, 0x27?, 0x55e5e85b7ea8?)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/proc.go:435 +0xce fp=0xc000062718 sp=0xc0000626f8 pc=0x55e5e84d28ee
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.chanrecv(0xc000098380, 0x0, 0x1)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/chan.go:664 +0x445 fp=0xc000062790 sp=0xc000062718 pc=0x55e5e846ee05
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.chanrecv1(0x0?, 0x0?)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/chan.go:506 +0x12 fp=0xc0000627b8 sp=0xc000062790 pc=0x55e5e846e992
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.unique_runtime_registerUniqueMapCleanup.func2(...)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/mgc.go:1796
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.unique_runtime_registerUniqueMapCleanup.gowrap1()
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/mgc.go:1799 +0x2f fp=0xc0000627e0 sp=0xc0000627b8 pc=0x55e5e8480def
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.goexit({})
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc0000627e8 sp=0xc0000627e0 pc=0x55e5e84da021
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: created by unique.runtime_registerUniqueMapCleanup in goroutine 1
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/mgc.go:1794 +0x85
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: goroutine 7 gp=0xc0001c5340 m=nil [GC worker (idle)]:
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.gopark(0x4c3a4569479d2?, 0x0?, 0x0?, 0x0?, 0x0?)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/proc.go:435 +0xce fp=0xc000062f38 sp=0xc000062f18 pc=0x55e5e84d28ee
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.gcBgMarkWorker(0xc000099960)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/mgc.go:1423 +0xe9 fp=0xc000062fc8 sp=0xc000062f38 pc=0x55e5e8480109
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.gcBgMarkStartWorkers.gowrap1()
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/mgc.go:1339 +0x25 fp=0xc000062fe0 sp=0xc000062fc8 pc=0x55e5e847ffe5
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.goexit({})
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc000062fe8 sp=0xc000062fe0 pc=0x55e5e84da021
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: created by runtime.gcBgMarkStartWorkers in goroutine 1
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/mgc.go:1339 +0x105
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: goroutine 18 gp=0xc000504000 m=nil [GC worker (idle)]:
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.gopark(0x4c3a4532bbd3e?, 0x0?, 0x0?, 0x0?, 0x0?)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/proc.go:435 +0xce fp=0xc00005c738 sp=0xc00005c718 pc=0x55e5e84d28ee
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.gcBgMarkWorker(0xc000099960)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/mgc.go:1423 +0xe9 fp=0xc00005c7c8 sp=0xc00005c738 pc=0x55e5e8480109
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.gcBgMarkStartWorkers.gowrap1()
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/mgc.go:1339 +0x25 fp=0xc00005c7e0 sp=0xc00005c7c8 pc=0x55e5e847ffe5
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.goexit({})
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc00005c7e8 sp=0xc00005c7e0 pc=0x55e5e84da021
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: created by runtime.gcBgMarkStartWorkers in goroutine 1
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/mgc.go:1339 +0x105
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: goroutine 34 gp=0xc000102380 m=nil [GC worker (idle)]:
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.gopark(0x4c3a4599e34c0?, 0x1?, 0xc1?, 0xb?, 0x0?)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/proc.go:435 +0xce fp=0xc00011a738 sp=0xc00011a718 pc=0x55e5e84d28ee
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.gcBgMarkWorker(0xc000099960)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/mgc.go:1423 +0xe9 fp=0xc00011a7c8 sp=0xc00011a738 pc=0x55e5e8480109
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.gcBgMarkStartWorkers.gowrap1()
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/mgc.go:1339 +0x25 fp=0xc00011a7e0 sp=0xc00011a7c8 pc=0x55e5e847ffe5
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.goexit({})
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc00011a7e8 sp=0xc00011a7e0 pc=0x55e5e84da021
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: created by runtime.gcBgMarkStartWorkers in goroutine 1
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/mgc.go:1339 +0x105
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: goroutine 8 gp=0xc0001c5500 m=nil [GC worker (idle)]:
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.gopark(0x55e5ea0bf2c0?, 0x1?, 0x11?, 0xd7?, 0x0?)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/proc.go:435 +0xce fp=0xc000063738 sp=0xc000063718 pc=0x55e5e84d28ee
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.gcBgMarkWorker(0xc000099960)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/mgc.go:1423 +0xe9 fp=0xc0000637c8 sp=0xc000063738 pc=0x55e5e8480109
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.gcBgMarkStartWorkers.gowrap1()
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/mgc.go:1339 +0x25 fp=0xc0000637e0 sp=0xc0000637c8 pc=0x55e5e847ffe5
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.goexit({})
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc0000637e8 sp=0xc0000637e0 pc=0x55e5e84da021
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: created by runtime.gcBgMarkStartWorkers in goroutine 1
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/mgc.go:1339 +0x105
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: goroutine 386 gp=0xc0030b8700 m=nil [select]:
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.gopark(0xc000165a68?, 0x2?, 0x0?, 0xda?, 0xc00016580c?)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/proc.go:435 +0xce fp=0xc000165620 sp=0xc000165600 pc=0x55e5e84d28ee
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.selectgo(0xc000165a68, 0xc000165808, 0x579?, 0x0, 0x1?, 0x1)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/select.go:351 +0x837 fp=0xc000165758 sp=0xc000165620 pc=0x55e5e84b11f7
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: github.com/ollama/ollama/runner/ollamarunner.(*Server).completion(0xc00015f0e0, {0x55e5e97aa4f8, 0xc047be4000}, 0xc00051c140)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         github.com/ollama/ollama/runner/ollamarunner/runner.go:649 +0xad0 fp=0xc000165ac0 sp=0xc000165758 pc=0x55e5e895de70
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: github.com/ollama/ollama/runner/ollamarunner.(*Server).completion-fm({0x55e5e97aa4f8?, 0xc047be4000?}, 0xc000165b40?)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         <autogenerated>:1 +0x36 fp=0xc000165af0 sp=0xc000165ac0 pc=0x55e5e8960a36
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: net/http.HandlerFunc.ServeHTTP(0xc000482000?, {0x55e5e97aa4f8?, 0xc047be4000?}, 0xc000165b60?)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         net/http/server.go:2294 +0x29 fp=0xc000165b18 sp=0xc000165af0 pc=0x55e5e87d0a49
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: net/http.(*ServeMux).ServeHTTP(0x55e5e8477125?, {0x55e5e97aa4f8, 0xc047be4000}, 0xc00051c140)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         net/http/server.go:2822 +0x1c4 fp=0xc000165b68 sp=0xc000165b18 pc=0x55e5e87d2944
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: net/http.serverHandler.ServeHTTP({0x55e5e97a6b10?}, {0x55e5e97aa4f8?, 0xc047be4000?}, 0x1?)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         net/http/server.go:3301 +0x8e fp=0xc000165b98 sp=0xc000165b68 pc=0x55e5e87f03ce
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: net/http.(*conn).serve(0xc0000dddd0, {0x55e5e97ac5a8, 0xc00020e1e0})
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         net/http/server.go:2102 +0x625 fp=0xc000165fb8 sp=0xc000165b98 pc=0x55e5e87cef45
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: net/http.(*Server).Serve.gowrap3()
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         net/http/server.go:3454 +0x28 fp=0xc000165fe0 sp=0xc000165fb8 pc=0x55e5e87d4808
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.goexit({})
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc000165fe8 sp=0xc000165fe0 pc=0x55e5e84da021
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: created by net/http.(*Server).Serve in goroutine 1
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         net/http/server.go:3454 +0x485
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: goroutine 387 gp=0xc01639fc00 m=nil [IO wait]:
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.gopark(0xc0163a45f8?, 0x55e5e84d9c7c?, 0x20?, 0x46?, 0xb?)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/proc.go:435 +0xce fp=0xc0163a45d8 sp=0xc0163a45b8 pc=0x55e5e84d28ee
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.netpollblock(0x55e5e84f5d78?, 0xe846c226?, 0xe5?)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/netpoll.go:575 +0xf7 fp=0xc0163a4610 sp=0xc0163a45d8 pc=0x55e5e84976f7
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: internal/poll.runtime_pollWait(0x7fe4c7fc7d98, 0x72)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/netpoll.go:351 +0x85 fp=0xc0163a4630 sp=0xc0163a4610 pc=0x55e5e84d1b05
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: internal/poll.(*pollDesc).wait(0xc047b96100?, 0xc0478fa9d1?, 0x0)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         internal/poll/fd_poll_runtime.go:84 +0x27 fp=0xc0163a4658 sp=0xc0163a4630 pc=0x55e5e8558f87
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: internal/poll.(*pollDesc).waitRead(...)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         internal/poll/fd_poll_runtime.go:89
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: internal/poll.(*FD).Read(0xc047b96100, {0xc0478fa9d1, 0x1, 0x1})
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         internal/poll/fd_unix.go:165 +0x27a fp=0xc0163a46f0 sp=0xc0163a4658 pc=0x55e5e855a27a
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: net.(*netFD).Read(0xc047b96100, {0xc0478fa9d1?, 0x55e5e88b8e89?, 0x0?})
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         net/fd_posix.go:55 +0x25 fp=0xc0163a4738 sp=0xc0163a46f0 pc=0x55e5e85cf1c5
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: net.(*conn).Read(0xc000064090, {0xc0478fa9d1?, 0xc003083a40?, 0x55e5e88b8e40?})
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         net/net.go:194 +0x45 fp=0xc0163a4780 sp=0xc0163a4738 pc=0x55e5e85dd585
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: net/http.(*connReader).backgroundRead(0xc0478fa9c0)
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         net/http/server.go:690 +0x37 fp=0xc0163a47c8 sp=0xc0163a4780 pc=0x55e5e87c8e17
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: net/http.(*connReader).startBackgroundRead.gowrap2()
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         net/http/server.go:686 +0x25 fp=0xc0163a47e0 sp=0xc0163a47c8 pc=0x55e5e87c8d45
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: runtime.goexit({})
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         runtime/asm_amd64.s:1700 +0x1 fp=0xc0163a47e8 sp=0xc0163a47e0 pc=0x55e5e84da021
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: created by net/http.(*connReader).startBackgroundRead in goroutine 386
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]:         net/http/server.go:686 +0xb6
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: rax    0x0
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: rbx    0x7fe4c7fbf700
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: rcx    0x7fe5169d000b
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: rdx    0x0
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: rdi    0x2
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: rsi    0x7fe4c7fbe930
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: rbp    0x55e5e94f02dd
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: rsp    0x7fe4c7fbe930
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: r8     0x0
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: r9     0x7fe4c7fbe930
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: r10    0x8
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: r11    0x246
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: r12    0x55e5e951560b
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: r13    0x333
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: r14    0x46f
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: r15    0x7fe4bc01a210
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: rip    0x7fe5169d000b
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: rflags 0x246
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: cs     0x33
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: fs     0x0
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: gs     0x0
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:18:36.441Z level=ERROR source=server.go:449 msg="llama runner terminated" error="exit status 2"
Mar 13 06:18:36 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 06:18:36 | 200 | 19.133021547s |       127.0.0.1 | POST     "/api/generate"
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:40.444Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="5.7 GiB" free_swap="0 B"
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:40.444Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:40.444Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.key_length default=64
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:40.444Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.value_length default=64
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:40.445Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:40.445Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=13 layers.offload=0 layers.split="" memory.available="[5.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="352.9 MiB" memory.required.partial="0 B" memory.required.kv="24.0 MiB" memory.required.allocations="[352.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 1
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: model type       = ?B
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_load: vocab only - skipping tensors
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:40.479Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 1 --port 38859"
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:40.479Z level=INFO source=sched.go:450 msg="loaded runners" count=1
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:40.479Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:40.480Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:40.492Z level=INFO source=runner.go:931 msg="starting go runner"
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:40.532Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:40.533Z level=INFO source=runner.go:991 msg="Server listening on 127.0.0.1:38859"
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 0
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: n_ctx_train      = 2048
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: n_embd           = 768
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: n_layer          = 12
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: n_head           = 12
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: n_head_kv        = 12
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: n_rot            = 64
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: n_swa            = 0
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_k    = 64
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_v    = 64
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: n_gqa            = 1
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: n_embd_k_gqa     = 768
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: n_embd_v_gqa     = 768
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: f_norm_eps       = 1.0e-12
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: f_norm_rms_eps   = 0.0e+00
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: f_clamp_kqv      = 0.0e+00
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: f_max_alibi_bias = 0.0e+00
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: f_logit_scale    = 0.0e+00
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: n_ff             = 3072
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: n_expert         = 0
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: n_expert_used    = 0
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: causal attn      = 0
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: pooling type     = 1
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: rope type        = 2
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: rope scaling     = linear
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: freq_base_train  = 1000.0
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: freq_scale_train = 1
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: n_ctx_orig_yarn  = 2048
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: rope_finetuned   = unknown
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: ssm_d_conv       = 0
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: ssm_d_inner      = 0
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: ssm_d_state      = 0
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_rank      = 0
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_b_c_rms   = 0
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: model type       = 137M
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: load_tensors: loading model tensors, this can take a while... (mmap = false)
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: load_tensors:          CPU model buffer size =   260.86 MiB
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_seq_max     = 1
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx         = 8192
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq = 8192
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_batch       = 512
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ubatch      = 512
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_init_from_model: flash_attn    = 0
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_base     = 1000.0
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_scale    = 1
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_init_from_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU  output buffer size =     0.00 MiB
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU compute buffer size =    23.00 MiB
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph nodes  = 453
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph splits = 1
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:40.731Z level=INFO source=server.go:624 msg="llama runner started in 0.25 seconds"
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 06:26:40 | 200 |  357.596278ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:40.946Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="5.2 GiB" free_swap="0 B"
Mar 13 06:26:40 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:40.947Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=27 layers.offload=0 layers.split="" memory.available="[5.2 GiB]" memory.gpu_overhead="0 B" memory.required.full="1.5 GiB" memory.required.partial="0 B" memory.required.kv="208.0 MiB" memory.required.allocations="[1.5 GiB]" memory.weights.total="664.5 MiB" memory.weights.repeating="358.5 MiB" memory.weights.nonrepeating="306.0 MiB" memory.graph.full="514.2 MiB" memory.graph.partial="750.5 MiB"
Mar 13 06:26:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:41.060Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 06:26:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:41.062Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
Mar 13 06:26:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:41.066Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 06:26:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:41.066Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 06:26:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:41.066Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.num_channels default=0
Mar 13 06:26:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:41.066Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.block_count default=0
Mar 13 06:26:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:41.066Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.embedding_length default=0
Mar 13 06:26:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:41.066Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.head_count default=0
Mar 13 06:26:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:41.066Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 06:26:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:41.066Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 06:26:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:41.066Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.layer_norm_epsilon default=0
Mar 13 06:26:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:41.066Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 06:26:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:41.072Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.freq_scale default=1
Mar 13 06:26:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:41.072Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.mm_tokens_per_image default=256
Mar 13 06:26:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:41.072Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --ollama-engine --model /usr/share/ollama/.ollama/models/blobs/sha256-dbe81da1e4bad3cc6ccb77540915ffe53e7a3ac0745ffa5e9d4626d39c15e09a --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 4 --port 35911"
Mar 13 06:26:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:41.072Z level=INFO source=sched.go:450 msg="loaded runners" count=2
Mar 13 06:26:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:41.072Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 06:26:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:41.073Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 06:26:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:41.084Z level=INFO source=runner.go:882 msg="starting ollama engine"
Mar 13 06:26:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:41.085Z level=INFO source=runner.go:938 msg="Server listening on 127.0.0.1:35911"
Mar 13 06:26:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:41.184Z level=WARN source=ggml.go:149 msg="key not found" key=general.name default=""
Mar 13 06:26:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:41.184Z level=WARN source=ggml.go:149 msg="key not found" key=general.description default=""
Mar 13 06:26:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:41.184Z level=INFO source=ggml.go:67 msg="" architecture=gemma3 file_type=Q4_K_M name="" description="" num_tensors=340 num_key_values=32
Mar 13 06:26:41 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 06:26:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:41.206Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 06:26:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:41.207Z level=INFO source=ggml.go:289 msg="model weights" buffer=CPU size="1.0 GiB"
Mar 13 06:26:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:41.324Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server loading model"
Mar 13 06:26:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:41.588Z level=INFO source=ggml.go:356 msg="compute graph" backend=CPU buffer_type=CPU
Mar 13 06:26:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:41.589Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 06:26:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:41.590Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
Mar 13 06:26:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:41.595Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 06:26:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:41.595Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 06:26:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:41.595Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.num_channels default=0
Mar 13 06:26:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:41.595Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.block_count default=0
Mar 13 06:26:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:41.595Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.embedding_length default=0
Mar 13 06:26:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:41.595Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.head_count default=0
Mar 13 06:26:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:41.595Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 06:26:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:41.595Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 06:26:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:41.595Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.layer_norm_epsilon default=0
Mar 13 06:26:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:41.595Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 06:26:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:41.602Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.freq_scale default=1
Mar 13 06:26:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:41.602Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.mm_tokens_per_image default=256
Mar 13 06:26:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T06:26:41.826Z level=INFO source=server.go:624 msg="llama runner started in 0.75 seconds"
Mar 13 06:27:52 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 06:27:52 | 200 |         1m11s |       127.0.0.1 | POST     "/api/generate"
Mar 13 06:28:48 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 06:28:48 | 200 |  125.269811ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 06:29:45 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 06:29:45 | 200 |   54.778386ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 06:31:21 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 06:31:21 | 200 |         2m32s |       127.0.0.1 | POST     "/api/generate"
Mar 13 06:31:21 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 06:31:21 | 200 |         1m35s |       127.0.0.1 | POST     "/api/generate"
Mar 13 06:32:02 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 06:32:02 | 200 |   52.995912ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 06:33:17 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 06:33:17 | 200 |         1m15s |       127.0.0.1 | POST     "/api/generate"
Mar 13 06:35:11 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 06:35:11 | 200 |   90.080747ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 06:36:50 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 06:36:50 | 200 |         1m39s |       127.0.0.1 | POST     "/api/generate"
Mar 13 06:37:08 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 06:37:08 | 200 |   38.499293ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 06:38:42 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 06:38:42 | 200 |         1m34s |       127.0.0.1 | POST     "/api/generate"
Mar 13 06:39:38 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 06:39:38 | 200 |   37.908514ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 06:41:05 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 06:41:05 | 200 |         1m26s |       127.0.0.1 | POST     "/api/generate"
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:05.368Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="5.9 GiB" free_swap="0 B"
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:05.369Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:05.369Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.key_length default=64
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:05.369Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.value_length default=64
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:05.369Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:05.369Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=13 layers.offload=0 layers.split="" memory.available="[5.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="352.9 MiB" memory.required.partial="0 B" memory.required.kv="24.0 MiB" memory.required.allocations="[352.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 1
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: model type       = ?B
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_load: vocab only - skipping tensors
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:05.404Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 1 --port 34647"
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:05.404Z level=INFO source=sched.go:450 msg="loaded runners" count=1
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:05.404Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:05.404Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:05.416Z level=INFO source=runner.go:931 msg="starting go runner"
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:05.456Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:05.457Z level=INFO source=runner.go:991 msg="Server listening on 127.0.0.1:34647"
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 0
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: n_ctx_train      = 2048
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: n_embd           = 768
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: n_layer          = 12
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: n_head           = 12
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: n_head_kv        = 12
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: n_rot            = 64
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: n_swa            = 0
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_k    = 64
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_v    = 64
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: n_gqa            = 1
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: n_embd_k_gqa     = 768
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: n_embd_v_gqa     = 768
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: f_norm_eps       = 1.0e-12
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: f_norm_rms_eps   = 0.0e+00
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: f_clamp_kqv      = 0.0e+00
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: f_max_alibi_bias = 0.0e+00
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: f_logit_scale    = 0.0e+00
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: n_ff             = 3072
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: n_expert         = 0
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: n_expert_used    = 0
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: causal attn      = 0
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: pooling type     = 1
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: rope type        = 2
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: rope scaling     = linear
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: freq_base_train  = 1000.0
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: freq_scale_train = 1
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: n_ctx_orig_yarn  = 2048
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: rope_finetuned   = unknown
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: ssm_d_conv       = 0
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: ssm_d_inner      = 0
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: ssm_d_state      = 0
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_rank      = 0
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_b_c_rms   = 0
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: model type       = 137M
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: load_tensors: loading model tensors, this can take a while... (mmap = false)
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: load_tensors:          CPU model buffer size =   260.86 MiB
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_seq_max     = 1
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx         = 8192
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq = 8192
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_batch       = 512
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ubatch      = 512
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_init_from_model: flash_attn    = 0
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_base     = 1000.0
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_scale    = 1
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_init_from_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU  output buffer size =     0.00 MiB
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU compute buffer size =    23.00 MiB
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph nodes  = 453
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph splits = 1
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:05.655Z level=INFO source=server.go:624 msg="llama runner started in 0.25 seconds"
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 07:16:05 | 200 |  326.737664ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:05.837Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="5.3 GiB" free_swap="0 B"
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:05.838Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=27 layers.offload=0 layers.split="" memory.available="[5.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="1.5 GiB" memory.required.partial="0 B" memory.required.kv="208.0 MiB" memory.required.allocations="[1.5 GiB]" memory.weights.total="664.5 MiB" memory.weights.repeating="358.5 MiB" memory.weights.nonrepeating="306.0 MiB" memory.graph.full="514.2 MiB" memory.graph.partial="750.5 MiB"
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:05.940Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:05.942Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:05.946Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:05.946Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:05.946Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.num_channels default=0
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:05.946Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.block_count default=0
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:05.946Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.embedding_length default=0
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:05.946Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.head_count default=0
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:05.946Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:05.946Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:05.946Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.layer_norm_epsilon default=0
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:05.946Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:05.952Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.freq_scale default=1
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:05.952Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.mm_tokens_per_image default=256
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:05.952Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --ollama-engine --model /usr/share/ollama/.ollama/models/blobs/sha256-dbe81da1e4bad3cc6ccb77540915ffe53e7a3ac0745ffa5e9d4626d39c15e09a --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 4 --port 46649"
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:05.952Z level=INFO source=sched.go:450 msg="loaded runners" count=2
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:05.952Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:05.952Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:05.965Z level=INFO source=runner.go:882 msg="starting ollama engine"
Mar 13 07:16:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:05.966Z level=INFO source=runner.go:938 msg="Server listening on 127.0.0.1:46649"
Mar 13 07:16:06 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:06.066Z level=WARN source=ggml.go:149 msg="key not found" key=general.name default=""
Mar 13 07:16:06 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:06.066Z level=WARN source=ggml.go:149 msg="key not found" key=general.description default=""
Mar 13 07:16:06 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:06.066Z level=INFO source=ggml.go:67 msg="" architecture=gemma3 file_type=Q4_K_M name="" description="" num_tensors=340 num_key_values=32
Mar 13 07:16:06 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 07:16:06 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:06.088Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 07:16:06 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:06.090Z level=INFO source=ggml.go:289 msg="model weights" buffer=CPU size="1.0 GiB"
Mar 13 07:16:06 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:06.204Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server loading model"
Mar 13 07:16:06 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:06.409Z level=INFO source=ggml.go:356 msg="compute graph" backend=CPU buffer_type=CPU
Mar 13 07:16:06 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:06.409Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 07:16:06 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:06.413Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
Mar 13 07:16:06 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:06.417Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 07:16:06 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:06.417Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 07:16:06 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:06.417Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.num_channels default=0
Mar 13 07:16:06 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:06.417Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.block_count default=0
Mar 13 07:16:06 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:06.417Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.embedding_length default=0
Mar 13 07:16:06 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:06.417Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.head_count default=0
Mar 13 07:16:06 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:06.417Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 07:16:06 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:06.417Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 07:16:06 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:06.417Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.layer_norm_epsilon default=0
Mar 13 07:16:06 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:06.417Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 07:16:06 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:06.425Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.freq_scale default=1
Mar 13 07:16:06 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:06.425Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.mm_tokens_per_image default=256
Mar 13 07:16:06 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:16:06.455Z level=INFO source=server.go:624 msg="llama runner started in 0.50 seconds"
Mar 13 07:17:19 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 07:17:19 | 200 |         1m13s |       127.0.0.1 | POST     "/api/generate"
Mar 13 07:17:42 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 07:17:42 | 200 |   38.003021ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 07:18:11 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 07:18:11 | 200 | 29.708188434s |       127.0.0.1 | POST     "/api/generate"
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:21.748Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="5.9 GiB" free_swap="0 B"
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:21.748Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:21.748Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.key_length default=64
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:21.748Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.value_length default=64
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:21.748Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:21.748Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=13 layers.offload=0 layers.split="" memory.available="[5.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="352.9 MiB" memory.required.partial="0 B" memory.required.kv="24.0 MiB" memory.required.allocations="[352.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 1
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: model type       = ?B
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_load: vocab only - skipping tensors
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:21.781Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 1 --port 39015"
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:21.781Z level=INFO source=sched.go:450 msg="loaded runners" count=1
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:21.781Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:21.781Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:21.793Z level=INFO source=runner.go:931 msg="starting go runner"
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:21.836Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:21.837Z level=INFO source=runner.go:991 msg="Server listening on 127.0.0.1:39015"
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 0
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: n_ctx_train      = 2048
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: n_embd           = 768
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: n_layer          = 12
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: n_head           = 12
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: n_head_kv        = 12
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: n_rot            = 64
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: n_swa            = 0
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_k    = 64
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_v    = 64
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: n_gqa            = 1
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: n_embd_k_gqa     = 768
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: n_embd_v_gqa     = 768
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: f_norm_eps       = 1.0e-12
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: f_norm_rms_eps   = 0.0e+00
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: f_clamp_kqv      = 0.0e+00
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: f_max_alibi_bias = 0.0e+00
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: f_logit_scale    = 0.0e+00
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: n_ff             = 3072
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: n_expert         = 0
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: n_expert_used    = 0
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: causal attn      = 0
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: pooling type     = 1
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: rope type        = 2
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: rope scaling     = linear
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: freq_base_train  = 1000.0
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: freq_scale_train = 1
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: n_ctx_orig_yarn  = 2048
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: rope_finetuned   = unknown
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: ssm_d_conv       = 0
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: ssm_d_inner      = 0
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: ssm_d_state      = 0
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_rank      = 0
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_b_c_rms   = 0
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: model type       = 137M
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: load_tensors: loading model tensors, this can take a while... (mmap = false)
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: load_tensors:          CPU model buffer size =   260.86 MiB
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_seq_max     = 1
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx         = 8192
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq = 8192
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_batch       = 512
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ubatch      = 512
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_init_from_model: flash_attn    = 0
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_base     = 1000.0
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_scale    = 1
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_init_from_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU  output buffer size =     0.00 MiB
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU compute buffer size =    23.00 MiB
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph nodes  = 453
Mar 13 07:27:21 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph splits = 1
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:22.033Z level=INFO source=server.go:624 msg="llama runner started in 0.25 seconds"
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 07:27:22 | 200 |  358.856381ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:22.251Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="5.3 GiB" free_swap="0 B"
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:22.252Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=27 layers.offload=0 layers.split="" memory.available="[5.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="1.5 GiB" memory.required.partial="0 B" memory.required.kv="208.0 MiB" memory.required.allocations="[1.5 GiB]" memory.weights.total="664.5 MiB" memory.weights.repeating="358.5 MiB" memory.weights.nonrepeating="306.0 MiB" memory.graph.full="514.2 MiB" memory.graph.partial="750.5 MiB"
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:22.360Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:22.362Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:22.366Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:22.366Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:22.366Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.num_channels default=0
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:22.366Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.block_count default=0
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:22.366Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.embedding_length default=0
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:22.366Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.head_count default=0
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:22.366Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:22.366Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:22.366Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.layer_norm_epsilon default=0
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:22.366Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:22.372Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.freq_scale default=1
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:22.372Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.mm_tokens_per_image default=256
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:22.372Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --ollama-engine --model /usr/share/ollama/.ollama/models/blobs/sha256-dbe81da1e4bad3cc6ccb77540915ffe53e7a3ac0745ffa5e9d4626d39c15e09a --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 4 --port 35607"
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:22.373Z level=INFO source=sched.go:450 msg="loaded runners" count=2
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:22.373Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:22.373Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:22.384Z level=INFO source=runner.go:882 msg="starting ollama engine"
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:22.385Z level=INFO source=runner.go:938 msg="Server listening on 127.0.0.1:35607"
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:22.496Z level=WARN source=ggml.go:149 msg="key not found" key=general.name default=""
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:22.496Z level=WARN source=ggml.go:149 msg="key not found" key=general.description default=""
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:22.496Z level=INFO source=ggml.go:67 msg="" architecture=gemma3 file_type=Q4_K_M name="" description="" num_tensors=340 num_key_values=32
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:22.520Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:22.522Z level=INFO source=ggml.go:289 msg="model weights" buffer=CPU size="1.0 GiB"
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:22.629Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server loading model"
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:22.778Z level=INFO source=ggml.go:356 msg="compute graph" backend=CPU buffer_type=CPU
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:22.778Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:22.780Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:22.783Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:22.783Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:22.784Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.num_channels default=0
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:22.784Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.block_count default=0
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:22.784Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.embedding_length default=0
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:22.784Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.head_count default=0
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:22.784Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:22.784Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:22.784Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.layer_norm_epsilon default=0
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:22.784Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:22.789Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.freq_scale default=1
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:22.789Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.mm_tokens_per_image default=256
Mar 13 07:27:22 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:27:22.880Z level=INFO source=server.go:624 msg="llama runner started in 0.51 seconds"
Mar 13 07:28:31 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 07:28:31 | 200 |          1m9s |       127.0.0.1 | POST     "/api/generate"
Mar 13 07:33:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:41.966Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="5.8 GiB" free_swap="0 B"
Mar 13 07:33:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:41.967Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 07:33:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:41.967Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.key_length default=64
Mar 13 07:33:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:41.967Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.value_length default=64
Mar 13 07:33:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:41.967Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 07:33:41 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:41.967Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=13 layers.offload=0 layers.split="" memory.available="[5.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="352.9 MiB" memory.required.partial="0 B" memory.required.kv="24.0 MiB" memory.required.allocations="[352.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
Mar 13 07:33:41 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 07:33:41 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 07:33:41 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 07:33:41 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 07:33:41 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 07:33:41 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 07:33:41 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 07:33:41 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 07:33:41 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 07:33:41 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 07:33:41 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 07:33:41 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 07:33:41 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 07:33:41 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 07:33:41 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 07:33:41 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 07:33:41 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 07:33:41 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 07:33:41 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 07:33:41 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 07:33:41 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 07:33:41 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 07:33:41 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 07:33:41 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 07:33:41 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 07:33:41 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 07:33:41 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 07:33:41 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 07:33:41 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 07:33:41 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 07:33:41 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 07:33:41 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 07:33:41 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 1
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: model type       = ?B
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: llama_model_load: vocab only - skipping tensors
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.002Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 1 --port 42321"
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.002Z level=INFO source=sched.go:450 msg="loaded runners" count=1
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.002Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.002Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.012Z level=INFO source=runner.go:931 msg="starting go runner"
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.060Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.061Z level=INFO source=runner.go:991 msg="Server listening on 127.0.0.1:42321"
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 0
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: n_ctx_train      = 2048
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: n_embd           = 768
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: n_layer          = 12
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: n_head           = 12
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: n_head_kv        = 12
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: n_rot            = 64
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: n_swa            = 0
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_k    = 64
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_v    = 64
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: n_gqa            = 1
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: n_embd_k_gqa     = 768
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: n_embd_v_gqa     = 768
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: f_norm_eps       = 1.0e-12
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: f_norm_rms_eps   = 0.0e+00
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: f_clamp_kqv      = 0.0e+00
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: f_max_alibi_bias = 0.0e+00
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: f_logit_scale    = 0.0e+00
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: n_ff             = 3072
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: n_expert         = 0
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: n_expert_used    = 0
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: causal attn      = 0
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: pooling type     = 1
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: rope type        = 2
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: rope scaling     = linear
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: freq_base_train  = 1000.0
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: freq_scale_train = 1
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: n_ctx_orig_yarn  = 2048
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: rope_finetuned   = unknown
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: ssm_d_conv       = 0
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: ssm_d_inner      = 0
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: ssm_d_state      = 0
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_rank      = 0
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_b_c_rms   = 0
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: model type       = 137M
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: load_tensors: loading model tensors, this can take a while... (mmap = false)
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: load_tensors:          CPU model buffer size =   260.86 MiB
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_seq_max     = 1
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx         = 8192
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq = 8192
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_batch       = 512
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ubatch      = 512
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: llama_init_from_model: flash_attn    = 0
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_base     = 1000.0
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_scale    = 1
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: llama_init_from_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU  output buffer size =     0.00 MiB
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU compute buffer size =    23.00 MiB
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph nodes  = 453
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph splits = 1
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.253Z level=INFO source=server.go:624 msg="llama runner started in 0.25 seconds"
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 07:33:42 | 200 |  379.813082ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.495Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="5.2 GiB" free_swap="0 B"
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.495Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=27 layers.offload=0 layers.split="" memory.available="[5.2 GiB]" memory.gpu_overhead="0 B" memory.required.full="1.5 GiB" memory.required.partial="0 B" memory.required.kv="208.0 MiB" memory.required.allocations="[1.5 GiB]" memory.weights.total="664.5 MiB" memory.weights.repeating="358.5 MiB" memory.weights.nonrepeating="306.0 MiB" memory.graph.full="514.2 MiB" memory.graph.partial="750.5 MiB"
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.606Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.608Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.612Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.612Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.612Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.num_channels default=0
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.612Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.block_count default=0
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.612Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.embedding_length default=0
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.612Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.head_count default=0
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.612Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.612Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.612Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.layer_norm_epsilon default=0
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.612Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.618Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.freq_scale default=1
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.618Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.mm_tokens_per_image default=256
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.618Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --ollama-engine --model /usr/share/ollama/.ollama/models/blobs/sha256-dbe81da1e4bad3cc6ccb77540915ffe53e7a3ac0745ffa5e9d4626d39c15e09a --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 4 --port 46251"
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.619Z level=INFO source=sched.go:450 msg="loaded runners" count=2
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.619Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.619Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.629Z level=INFO source=runner.go:882 msg="starting ollama engine"
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.630Z level=INFO source=runner.go:938 msg="Server listening on 127.0.0.1:46251"
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.727Z level=WARN source=ggml.go:149 msg="key not found" key=general.name default=""
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.727Z level=WARN source=ggml.go:149 msg="key not found" key=general.description default=""
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.727Z level=INFO source=ggml.go:67 msg="" architecture=gemma3 file_type=Q4_K_M name="" description="" num_tensors=340 num_key_values=32
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.733Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.734Z level=INFO source=ggml.go:289 msg="model weights" buffer=CPU size="1.0 GiB"
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.877Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server loading model"
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.955Z level=INFO source=ggml.go:356 msg="compute graph" backend=CPU buffer_type=CPU
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.955Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.957Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.961Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.961Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.961Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.num_channels default=0
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.961Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.block_count default=0
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.961Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.embedding_length default=0
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.961Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.head_count default=0
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.961Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.961Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.961Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.layer_norm_epsilon default=0
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.961Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.966Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.freq_scale default=1
Mar 13 07:33:42 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:42.967Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.mm_tokens_per_image default=256
Mar 13 07:33:43 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:33:43.127Z level=INFO source=server.go:624 msg="llama runner started in 0.51 seconds"
Mar 13 07:34:28 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 07:34:28 | 200 | 46.210684791s |       127.0.0.1 | POST     "/api/generate"
Mar 13 07:35:00 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 07:35:00 | 200 |   72.085677ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 07:35:37 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 07:35:37 | 200 | 37.048124739s |       127.0.0.1 | POST     "/api/generate"
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:31.297Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="5.8 GiB" free_swap="0 B"
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:31.297Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:31.297Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.key_length default=64
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:31.297Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.value_length default=64
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:31.297Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:31.297Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=13 layers.offload=0 layers.split="" memory.available="[5.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="352.9 MiB" memory.required.partial="0 B" memory.required.kv="24.0 MiB" memory.required.allocations="[352.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 1
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: model type       = ?B
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_load: vocab only - skipping tensors
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:31.330Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 1 --port 36791"
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:31.330Z level=INFO source=sched.go:450 msg="loaded runners" count=1
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:31.330Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:31.330Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:31.342Z level=INFO source=runner.go:931 msg="starting go runner"
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:31.388Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:31.389Z level=INFO source=runner.go:991 msg="Server listening on 127.0.0.1:36791"
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 0
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: n_ctx_train      = 2048
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: n_embd           = 768
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: n_layer          = 12
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: n_head           = 12
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: n_head_kv        = 12
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: n_rot            = 64
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: n_swa            = 0
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_k    = 64
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_v    = 64
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: n_gqa            = 1
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: n_embd_k_gqa     = 768
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: n_embd_v_gqa     = 768
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: f_norm_eps       = 1.0e-12
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: f_norm_rms_eps   = 0.0e+00
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: f_clamp_kqv      = 0.0e+00
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: f_max_alibi_bias = 0.0e+00
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: f_logit_scale    = 0.0e+00
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: n_ff             = 3072
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: n_expert         = 0
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: n_expert_used    = 0
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: causal attn      = 0
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: pooling type     = 1
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: rope type        = 2
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: rope scaling     = linear
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: freq_base_train  = 1000.0
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: freq_scale_train = 1
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: n_ctx_orig_yarn  = 2048
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: rope_finetuned   = unknown
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: ssm_d_conv       = 0
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: ssm_d_inner      = 0
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: ssm_d_state      = 0
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_rank      = 0
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_b_c_rms   = 0
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: model type       = 137M
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: load_tensors: loading model tensors, this can take a while... (mmap = false)
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: load_tensors:          CPU model buffer size =   260.86 MiB
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_seq_max     = 1
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx         = 8192
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq = 8192
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_batch       = 512
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ubatch      = 512
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_init_from_model: flash_attn    = 0
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_base     = 1000.0
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_scale    = 1
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_init_from_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU  output buffer size =     0.00 MiB
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU compute buffer size =    23.00 MiB
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph nodes  = 453
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph splits = 1
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:31.582Z level=INFO source=server.go:624 msg="llama runner started in 0.25 seconds"
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 07:46:31 | 200 |  342.390292ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:31.782Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="5.2 GiB" free_swap="0 B"
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:31.783Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=27 layers.offload=0 layers.split="" memory.available="[5.2 GiB]" memory.gpu_overhead="0 B" memory.required.full="1.5 GiB" memory.required.partial="0 B" memory.required.kv="208.0 MiB" memory.required.allocations="[1.5 GiB]" memory.weights.total="664.5 MiB" memory.weights.repeating="358.5 MiB" memory.weights.nonrepeating="306.0 MiB" memory.graph.full="514.2 MiB" memory.graph.partial="750.5 MiB"
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:31.891Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:31.893Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:31.897Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:31.897Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:31.897Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.num_channels default=0
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:31.897Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.block_count default=0
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:31.897Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.embedding_length default=0
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:31.897Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.head_count default=0
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:31.897Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:31.897Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:31.897Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.layer_norm_epsilon default=0
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:31.897Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:31.904Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.freq_scale default=1
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:31.904Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.mm_tokens_per_image default=256
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:31.904Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --ollama-engine --model /usr/share/ollama/.ollama/models/blobs/sha256-dbe81da1e4bad3cc6ccb77540915ffe53e7a3ac0745ffa5e9d4626d39c15e09a --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 4 --port 45663"
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:31.904Z level=INFO source=sched.go:450 msg="loaded runners" count=2
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:31.904Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:31.905Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:31.916Z level=INFO source=runner.go:882 msg="starting ollama engine"
Mar 13 07:46:31 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:31.917Z level=INFO source=runner.go:938 msg="Server listening on 127.0.0.1:45663"
Mar 13 07:46:32 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:32.019Z level=WARN source=ggml.go:149 msg="key not found" key=general.name default=""
Mar 13 07:46:32 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:32.019Z level=WARN source=ggml.go:149 msg="key not found" key=general.description default=""
Mar 13 07:46:32 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:32.019Z level=INFO source=ggml.go:67 msg="" architecture=gemma3 file_type=Q4_K_M name="" description="" num_tensors=340 num_key_values=32
Mar 13 07:46:32 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 07:46:32 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:32.023Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 07:46:32 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:32.024Z level=INFO source=ggml.go:289 msg="model weights" buffer=CPU size="1.0 GiB"
Mar 13 07:46:32 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:32.157Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server loading model"
Mar 13 07:46:32 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:32.242Z level=INFO source=ggml.go:356 msg="compute graph" backend=CPU buffer_type=CPU
Mar 13 07:46:32 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:32.242Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 07:46:32 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:32.244Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
Mar 13 07:46:32 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:32.248Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 07:46:32 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:32.248Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 07:46:32 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:32.248Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.num_channels default=0
Mar 13 07:46:32 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:32.248Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.block_count default=0
Mar 13 07:46:32 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:32.248Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.embedding_length default=0
Mar 13 07:46:32 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:32.248Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.head_count default=0
Mar 13 07:46:32 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:32.248Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 07:46:32 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:32.248Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 07:46:32 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:32.248Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.layer_norm_epsilon default=0
Mar 13 07:46:32 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:32.248Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 07:46:32 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:32.253Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.freq_scale default=1
Mar 13 07:46:32 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:32.253Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.mm_tokens_per_image default=256
Mar 13 07:46:32 ubuntu-abhishek ollama[471085]: time=2025-03-13T07:46:32.408Z level=INFO source=server.go:624 msg="llama runner started in 0.50 seconds"
Mar 13 07:46:59 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 07:46:59 | 200 | 27.854124173s |       127.0.0.1 | POST     "/api/generate"
Mar 13 07:47:20 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 07:47:20 | 200 |   39.553699ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 07:47:51 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 07:47:51 | 200 | 30.890750985s |       127.0.0.1 | POST     "/api/generate"
Mar 13 07:49:49 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 07:49:49 | 200 |   46.899702ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 07:50:34 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 07:50:34 | 200 | 44.862595953s |       127.0.0.1 | POST     "/api/generate"
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:20.214Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="5.8 GiB" free_swap="0 B"
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:20.214Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:20.214Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.key_length default=64
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:20.214Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.value_length default=64
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:20.214Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:20.214Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=13 layers.offload=0 layers.split="" memory.available="[5.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="352.9 MiB" memory.required.partial="0 B" memory.required.kv="24.0 MiB" memory.required.allocations="[352.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 1
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: model type       = ?B
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_load: vocab only - skipping tensors
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:20.246Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 1 --port 34063"
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:20.247Z level=INFO source=sched.go:450 msg="loaded runners" count=1
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:20.247Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:20.247Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:20.257Z level=INFO source=runner.go:931 msg="starting go runner"
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:20.300Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:20.301Z level=INFO source=runner.go:991 msg="Server listening on 127.0.0.1:34063"
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 0
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: n_ctx_train      = 2048
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: n_embd           = 768
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: n_layer          = 12
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: n_head           = 12
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: n_head_kv        = 12
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: n_rot            = 64
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: n_swa            = 0
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_k    = 64
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_v    = 64
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: n_gqa            = 1
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: n_embd_k_gqa     = 768
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: n_embd_v_gqa     = 768
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: f_norm_eps       = 1.0e-12
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: f_norm_rms_eps   = 0.0e+00
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: f_clamp_kqv      = 0.0e+00
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: f_max_alibi_bias = 0.0e+00
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: f_logit_scale    = 0.0e+00
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: n_ff             = 3072
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: n_expert         = 0
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: n_expert_used    = 0
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: causal attn      = 0
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: pooling type     = 1
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: rope type        = 2
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: rope scaling     = linear
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: freq_base_train  = 1000.0
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: freq_scale_train = 1
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: n_ctx_orig_yarn  = 2048
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: rope_finetuned   = unknown
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: ssm_d_conv       = 0
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: ssm_d_inner      = 0
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: ssm_d_state      = 0
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_rank      = 0
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_b_c_rms   = 0
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: model type       = 137M
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: load_tensors: loading model tensors, this can take a while... (mmap = false)
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: load_tensors:          CPU model buffer size =   260.86 MiB
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_seq_max     = 1
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx         = 8192
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq = 8192
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_batch       = 512
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ubatch      = 512
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_init_from_model: flash_attn    = 0
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_base     = 1000.0
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_scale    = 1
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_init_from_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU  output buffer size =     0.00 MiB
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU compute buffer size =    23.00 MiB
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph nodes  = 453
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph splits = 1
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:20.499Z level=INFO source=server.go:624 msg="llama runner started in 0.25 seconds"
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 08:02:20 | 200 |  345.430247ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:20.703Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="5.3 GiB" free_swap="0 B"
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:20.704Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=27 layers.offload=0 layers.split="" memory.available="[5.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="1.5 GiB" memory.required.partial="0 B" memory.required.kv="208.0 MiB" memory.required.allocations="[1.5 GiB]" memory.weights.total="664.5 MiB" memory.weights.repeating="358.5 MiB" memory.weights.nonrepeating="306.0 MiB" memory.graph.full="514.2 MiB" memory.graph.partial="750.5 MiB"
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:20.817Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:20.820Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:20.823Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:20.824Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:20.824Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.num_channels default=0
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:20.824Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.block_count default=0
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:20.824Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.embedding_length default=0
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:20.824Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.head_count default=0
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:20.824Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:20.824Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:20.824Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.layer_norm_epsilon default=0
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:20.824Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:20.830Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.freq_scale default=1
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:20.830Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.mm_tokens_per_image default=256
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:20.830Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --ollama-engine --model /usr/share/ollama/.ollama/models/blobs/sha256-dbe81da1e4bad3cc6ccb77540915ffe53e7a3ac0745ffa5e9d4626d39c15e09a --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 4 --port 42745"
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:20.830Z level=INFO source=sched.go:450 msg="loaded runners" count=2
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:20.830Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:20.830Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:20.842Z level=INFO source=runner.go:882 msg="starting ollama engine"
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:20.843Z level=INFO source=runner.go:938 msg="Server listening on 127.0.0.1:42745"
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:20.950Z level=WARN source=ggml.go:149 msg="key not found" key=general.name default=""
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:20.950Z level=WARN source=ggml.go:149 msg="key not found" key=general.description default=""
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:20.950Z level=INFO source=ggml.go:67 msg="" architecture=gemma3 file_type=Q4_K_M name="" description="" num_tensors=340 num_key_values=32
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:20.955Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 08:02:20 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:20.956Z level=INFO source=ggml.go:289 msg="model weights" buffer=CPU size="1.0 GiB"
Mar 13 08:02:21 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:21.082Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server loading model"
Mar 13 08:02:21 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:21.176Z level=INFO source=ggml.go:356 msg="compute graph" backend=CPU buffer_type=CPU
Mar 13 08:02:21 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:21.177Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 08:02:21 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:21.178Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
Mar 13 08:02:21 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:21.182Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 08:02:21 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:21.182Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 08:02:21 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:21.182Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.num_channels default=0
Mar 13 08:02:21 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:21.182Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.block_count default=0
Mar 13 08:02:21 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:21.182Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.embedding_length default=0
Mar 13 08:02:21 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:21.182Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.head_count default=0
Mar 13 08:02:21 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:21.182Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 08:02:21 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:21.182Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 08:02:21 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:21.182Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.layer_norm_epsilon default=0
Mar 13 08:02:21 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:21.182Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 08:02:21 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:21.187Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.freq_scale default=1
Mar 13 08:02:21 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:21.187Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.mm_tokens_per_image default=256
Mar 13 08:02:21 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:21.333Z level=INFO source=server.go:624 msg="llama runner started in 0.50 seconds"
Mar 13 08:02:21 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:02:21.350Z level=WARN source=runner.go:122 msg="truncating input prompt" limit=2048 prompt=2118 keep=4 new=2048
Mar 13 08:03:32 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 08:03:32 | 200 |         1m12s |       127.0.0.1 | POST     "/api/generate"
Mar 13 08:04:14 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 08:04:14 | 200 |   37.954196ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 08:05:24 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 08:05:24 | 200 |         1m10s |       127.0.0.1 | POST     "/api/generate"
Mar 13 08:06:03 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 08:06:03 | 200 |   53.706821ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 08:07:25 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 08:07:25 | 200 |         1m21s |       127.0.0.1 | POST     "/api/generate"
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:11:28.305Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:11:28.305Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.key_length default=64
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:11:28.305Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.value_length default=64
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:11:28.305Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:11:28.305Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="4.2 GiB" free_swap="0 B"
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:11:28.305Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:11:28.305Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.key_length default=64
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:11:28.305Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.value_length default=64
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:11:28.305Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:11:28.305Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=13 layers.offload=0 layers.split="" memory.available="[4.2 GiB]" memory.gpu_overhead="0 B" memory.required.full="352.9 MiB" memory.required.partial="0 B" memory.required.kv="24.0 MiB" memory.required.allocations="[352.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 1
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: model type       = ?B
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_load: vocab only - skipping tensors
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:11:28.337Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 1 --port 41443"
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:11:28.337Z level=INFO source=sched.go:450 msg="loaded runners" count=2
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:11:28.337Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:11:28.338Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:11:28.347Z level=INFO source=runner.go:931 msg="starting go runner"
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:11:28.392Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:11:28.393Z level=INFO source=runner.go:991 msg="Server listening on 127.0.0.1:41443"
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 0
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: n_ctx_train      = 2048
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: n_embd           = 768
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: n_layer          = 12
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: n_head           = 12
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: n_head_kv        = 12
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: n_rot            = 64
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: n_swa            = 0
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_k    = 64
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_v    = 64
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: n_gqa            = 1
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: n_embd_k_gqa     = 768
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: n_embd_v_gqa     = 768
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: f_norm_eps       = 1.0e-12
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: f_norm_rms_eps   = 0.0e+00
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: f_clamp_kqv      = 0.0e+00
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: f_max_alibi_bias = 0.0e+00
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: f_logit_scale    = 0.0e+00
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: n_ff             = 3072
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: n_expert         = 0
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: n_expert_used    = 0
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: causal attn      = 0
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: pooling type     = 1
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: rope type        = 2
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: rope scaling     = linear
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: freq_base_train  = 1000.0
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: freq_scale_train = 1
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: n_ctx_orig_yarn  = 2048
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: rope_finetuned   = unknown
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: ssm_d_conv       = 0
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: ssm_d_inner      = 0
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: ssm_d_state      = 0
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_rank      = 0
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_b_c_rms   = 0
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: model type       = 137M
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: load_tensors: loading model tensors, this can take a while... (mmap = false)
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: load_tensors:          CPU model buffer size =   260.86 MiB
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_seq_max     = 1
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx         = 8192
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq = 8192
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_batch       = 512
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ubatch      = 512
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_init_from_model: flash_attn    = 0
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_base     = 1000.0
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_scale    = 1
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_init_from_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU  output buffer size =     0.00 MiB
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU compute buffer size =    23.00 MiB
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph nodes  = 453
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph splits = 1
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:11:28.589Z level=INFO source=server.go:624 msg="llama runner started in 0.25 seconds"
Mar 13 08:11:28 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 08:11:28 | 200 |  348.068269ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 08:11:54 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 08:11:54 | 200 | 25.655351422s |       127.0.0.1 | POST     "/api/generate"
Mar 13 08:13:27 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 08:13:27 | 200 |   37.885208ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 08:13:55 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 08:13:55 | 200 | 27.940454234s |       127.0.0.1 | POST     "/api/generate"
Mar 13 08:14:59 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 08:14:59 | 200 |   32.404224ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 08:16:12 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 08:16:12 | 200 |         1m13s |       127.0.0.1 | POST     "/api/generate"
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:29.819Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="5.8 GiB" free_swap="0 B"
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:29.819Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:29.819Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.key_length default=64
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:29.819Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.value_length default=64
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:29.819Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:29.819Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=13 layers.offload=0 layers.split="" memory.available="[5.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="352.9 MiB" memory.required.partial="0 B" memory.required.kv="24.0 MiB" memory.required.allocations="[352.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 1
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: model type       = ?B
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_load: vocab only - skipping tensors
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:29.852Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 1 --port 34401"
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:29.852Z level=INFO source=sched.go:450 msg="loaded runners" count=1
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:29.852Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:29.852Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:29.863Z level=INFO source=runner.go:931 msg="starting go runner"
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:29.904Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:29.905Z level=INFO source=runner.go:991 msg="Server listening on 127.0.0.1:34401"
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 0
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: n_ctx_train      = 2048
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: n_embd           = 768
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: n_layer          = 12
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: n_head           = 12
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: n_head_kv        = 12
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: n_rot            = 64
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: n_swa            = 0
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_k    = 64
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_v    = 64
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: n_gqa            = 1
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: n_embd_k_gqa     = 768
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: n_embd_v_gqa     = 768
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: f_norm_eps       = 1.0e-12
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: f_norm_rms_eps   = 0.0e+00
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: f_clamp_kqv      = 0.0e+00
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: f_max_alibi_bias = 0.0e+00
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: f_logit_scale    = 0.0e+00
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: n_ff             = 3072
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: n_expert         = 0
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: n_expert_used    = 0
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: causal attn      = 0
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: pooling type     = 1
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: rope type        = 2
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: rope scaling     = linear
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: freq_base_train  = 1000.0
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: freq_scale_train = 1
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: n_ctx_orig_yarn  = 2048
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: rope_finetuned   = unknown
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: ssm_d_conv       = 0
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: ssm_d_inner      = 0
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: ssm_d_state      = 0
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_rank      = 0
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_b_c_rms   = 0
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: model type       = 137M
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: load_tensors: loading model tensors, this can take a while... (mmap = false)
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: load_tensors:          CPU model buffer size =   260.86 MiB
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_seq_max     = 1
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx         = 8192
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq = 8192
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_batch       = 512
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ubatch      = 512
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_init_from_model: flash_attn    = 0
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_base     = 1000.0
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_scale    = 1
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
Mar 13 08:21:29 ubuntu-abhishek ollama[471085]: llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: llama_init_from_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU  output buffer size =     0.00 MiB
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU compute buffer size =    23.00 MiB
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph nodes  = 453
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph splits = 1
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:30.104Z level=INFO source=server.go:624 msg="llama runner started in 0.25 seconds"
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 08:21:30 | 200 |  332.708207ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:30.299Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="5.2 GiB" free_swap="0 B"
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:30.299Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=27 layers.offload=0 layers.split="" memory.available="[5.2 GiB]" memory.gpu_overhead="0 B" memory.required.full="1.5 GiB" memory.required.partial="0 B" memory.required.kv="208.0 MiB" memory.required.allocations="[1.5 GiB]" memory.weights.total="664.5 MiB" memory.weights.repeating="358.5 MiB" memory.weights.nonrepeating="306.0 MiB" memory.graph.full="514.2 MiB" memory.graph.partial="750.5 MiB"
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:30.414Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:30.416Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:30.420Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:30.420Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:30.420Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.num_channels default=0
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:30.420Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.block_count default=0
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:30.420Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.embedding_length default=0
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:30.420Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.head_count default=0
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:30.420Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:30.420Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:30.420Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.layer_norm_epsilon default=0
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:30.420Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:30.425Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.freq_scale default=1
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:30.425Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.mm_tokens_per_image default=256
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:30.425Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --ollama-engine --model /usr/share/ollama/.ollama/models/blobs/sha256-dbe81da1e4bad3cc6ccb77540915ffe53e7a3ac0745ffa5e9d4626d39c15e09a --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 4 --port 44639"
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:30.426Z level=INFO source=sched.go:450 msg="loaded runners" count=2
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:30.426Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:30.426Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:30.437Z level=INFO source=runner.go:882 msg="starting ollama engine"
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:30.437Z level=INFO source=runner.go:938 msg="Server listening on 127.0.0.1:44639"
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:30.549Z level=WARN source=ggml.go:149 msg="key not found" key=general.name default=""
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:30.549Z level=WARN source=ggml.go:149 msg="key not found" key=general.description default=""
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:30.549Z level=INFO source=ggml.go:67 msg="" architecture=gemma3 file_type=Q4_K_M name="" description="" num_tensors=340 num_key_values=32
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:30.572Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:30.574Z level=INFO source=ggml.go:289 msg="model weights" buffer=CPU size="1.0 GiB"
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:30.695Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server loading model"
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:30.773Z level=INFO source=ggml.go:356 msg="compute graph" backend=CPU buffer_type=CPU
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:30.773Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:30.775Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:30.779Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:30.779Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:30.779Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.num_channels default=0
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:30.779Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.block_count default=0
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:30.779Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.embedding_length default=0
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:30.779Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.head_count default=0
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:30.779Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:30.779Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:30.779Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.layer_norm_epsilon default=0
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:30.779Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:30.784Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.freq_scale default=1
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:30.784Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.mm_tokens_per_image default=256
Mar 13 08:21:30 ubuntu-abhishek ollama[471085]: time=2025-03-13T08:21:30.947Z level=INFO source=server.go:624 msg="llama runner started in 0.52 seconds"
Mar 13 08:22:53 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 08:22:53 | 200 |         1m23s |       127.0.0.1 | POST     "/api/generate"
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:41:38.403Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="5.7 GiB" free_swap="0 B"
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:41:38.403Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:41:38.403Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.key_length default=64
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:41:38.403Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.value_length default=64
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:41:38.403Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:41:38.403Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=13 layers.offload=0 layers.split="" memory.available="[5.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="352.9 MiB" memory.required.partial="0 B" memory.required.kv="24.0 MiB" memory.required.allocations="[352.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 1
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: model type       = ?B
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_load: vocab only - skipping tensors
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:41:38.436Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 1 --port 43633"
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:41:38.436Z level=INFO source=sched.go:450 msg="loaded runners" count=1
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:41:38.436Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:41:38.436Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:41:38.448Z level=INFO source=runner.go:931 msg="starting go runner"
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:41:38.488Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:41:38.489Z level=INFO source=runner.go:991 msg="Server listening on 127.0.0.1:43633"
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 0
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: n_ctx_train      = 2048
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: n_embd           = 768
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: n_layer          = 12
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: n_head           = 12
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: n_head_kv        = 12
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: n_rot            = 64
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: n_swa            = 0
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_k    = 64
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_v    = 64
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: n_gqa            = 1
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: n_embd_k_gqa     = 768
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: n_embd_v_gqa     = 768
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: f_norm_eps       = 1.0e-12
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: f_norm_rms_eps   = 0.0e+00
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: f_clamp_kqv      = 0.0e+00
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: f_max_alibi_bias = 0.0e+00
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: f_logit_scale    = 0.0e+00
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: n_ff             = 3072
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: n_expert         = 0
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: n_expert_used    = 0
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: causal attn      = 0
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: pooling type     = 1
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: rope type        = 2
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: rope scaling     = linear
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: freq_base_train  = 1000.0
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: freq_scale_train = 1
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: n_ctx_orig_yarn  = 2048
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: rope_finetuned   = unknown
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: ssm_d_conv       = 0
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: ssm_d_inner      = 0
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: ssm_d_state      = 0
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_rank      = 0
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_b_c_rms   = 0
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: model type       = 137M
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: load_tensors: loading model tensors, this can take a while... (mmap = false)
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: load_tensors:          CPU model buffer size =   260.86 MiB
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_seq_max     = 1
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx         = 8192
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq = 8192
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_batch       = 512
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ubatch      = 512
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_init_from_model: flash_attn    = 0
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_base     = 1000.0
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_scale    = 1
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_init_from_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU  output buffer size =     0.00 MiB
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU compute buffer size =    23.00 MiB
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph nodes  = 453
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph splits = 1
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:41:38.688Z level=INFO source=server.go:624 msg="llama runner started in 0.25 seconds"
Mar 13 09:41:38 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 09:41:38 | 200 |  404.732315ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 09:44:12 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 09:44:12 | 200 |  109.790935ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 09:44:12 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:44:12.874Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="5.1 GiB" free_swap="0 B"
Mar 13 09:44:12 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:44:12.874Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=27 layers.offload=0 layers.split="" memory.available="[5.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="1.5 GiB" memory.required.partial="0 B" memory.required.kv="208.0 MiB" memory.required.allocations="[1.5 GiB]" memory.weights.total="664.5 MiB" memory.weights.repeating="358.5 MiB" memory.weights.nonrepeating="306.0 MiB" memory.graph.full="514.2 MiB" memory.graph.partial="750.5 MiB"
Mar 13 09:44:12 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:44:12.965Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 09:44:12 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:44:12.972Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
Mar 13 09:44:12 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:44:12.976Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 09:44:12 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:44:12.976Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 09:44:12 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:44:12.976Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.num_channels default=0
Mar 13 09:44:12 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:44:12.976Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.block_count default=0
Mar 13 09:44:12 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:44:12.976Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.embedding_length default=0
Mar 13 09:44:12 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:44:12.976Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.head_count default=0
Mar 13 09:44:12 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:44:12.976Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 09:44:12 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:44:12.976Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 09:44:12 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:44:12.976Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.layer_norm_epsilon default=0
Mar 13 09:44:12 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:44:12.976Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 09:44:12 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:44:12.987Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.freq_scale default=1
Mar 13 09:44:12 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:44:12.987Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.mm_tokens_per_image default=256
Mar 13 09:44:12 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:44:12.987Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --ollama-engine --model /usr/share/ollama/.ollama/models/blobs/sha256-dbe81da1e4bad3cc6ccb77540915ffe53e7a3ac0745ffa5e9d4626d39c15e09a --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 4 --port 44523"
Mar 13 09:44:12 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:44:12.988Z level=INFO source=sched.go:450 msg="loaded runners" count=2
Mar 13 09:44:12 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:44:12.988Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 09:44:12 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:44:12.988Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 09:44:13 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:44:13.000Z level=INFO source=runner.go:882 msg="starting ollama engine"
Mar 13 09:44:13 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:44:13.000Z level=INFO source=runner.go:938 msg="Server listening on 127.0.0.1:44523"
Mar 13 09:44:13 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:44:13.100Z level=WARN source=ggml.go:149 msg="key not found" key=general.name default=""
Mar 13 09:44:13 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:44:13.100Z level=WARN source=ggml.go:149 msg="key not found" key=general.description default=""
Mar 13 09:44:13 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:44:13.100Z level=INFO source=ggml.go:67 msg="" architecture=gemma3 file_type=Q4_K_M name="" description="" num_tensors=340 num_key_values=32
Mar 13 09:44:13 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 09:44:13 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:44:13.140Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 09:44:13 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:44:13.142Z level=INFO source=ggml.go:289 msg="model weights" buffer=CPU size="1.0 GiB"
Mar 13 09:44:13 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:44:13.295Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server loading model"
Mar 13 09:44:13 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:44:13.398Z level=INFO source=ggml.go:356 msg="compute graph" backend=CPU buffer_type=CPU
Mar 13 09:44:13 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:44:13.398Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 09:44:13 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:44:13.399Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
Mar 13 09:44:13 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:44:13.403Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 09:44:13 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:44:13.403Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 09:44:13 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:44:13.403Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.num_channels default=0
Mar 13 09:44:13 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:44:13.403Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.block_count default=0
Mar 13 09:44:13 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:44:13.403Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.embedding_length default=0
Mar 13 09:44:13 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:44:13.403Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.head_count default=0
Mar 13 09:44:13 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:44:13.403Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 09:44:13 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:44:13.403Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 09:44:13 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:44:13.404Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.layer_norm_epsilon default=0
Mar 13 09:44:13 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:44:13.404Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 09:44:13 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:44:13.409Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.freq_scale default=1
Mar 13 09:44:13 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:44:13.409Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.mm_tokens_per_image default=256
Mar 13 09:44:13 ubuntu-abhishek ollama[471085]: time=2025-03-13T09:44:13.546Z level=INFO source=server.go:624 msg="llama runner started in 0.56 seconds"
Mar 13 09:45:33 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 09:45:33 | 200 |         1m20s |       127.0.0.1 | POST     "/api/generate"
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:43.533Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="5.8 GiB" free_swap="0 B"
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:43.534Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:43.534Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.key_length default=64
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:43.534Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.value_length default=64
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:43.534Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:43.534Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=13 layers.offload=0 layers.split="" memory.available="[5.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="352.9 MiB" memory.required.partial="0 B" memory.required.kv="24.0 MiB" memory.required.allocations="[352.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 1
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: model type       = ?B
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_load: vocab only - skipping tensors
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:43.567Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 1 --port 34267"
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:43.567Z level=INFO source=sched.go:450 msg="loaded runners" count=1
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:43.568Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:43.568Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:43.598Z level=INFO source=runner.go:931 msg="starting go runner"
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:43.641Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:43.641Z level=INFO source=runner.go:991 msg="Server listening on 127.0.0.1:34267"
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 0
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: n_ctx_train      = 2048
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: n_embd           = 768
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: n_layer          = 12
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: n_head           = 12
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: n_head_kv        = 12
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: n_rot            = 64
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: n_swa            = 0
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_k    = 64
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_v    = 64
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: n_gqa            = 1
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: n_embd_k_gqa     = 768
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: n_embd_v_gqa     = 768
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: f_norm_eps       = 1.0e-12
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: f_norm_rms_eps   = 0.0e+00
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: f_clamp_kqv      = 0.0e+00
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: f_max_alibi_bias = 0.0e+00
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: f_logit_scale    = 0.0e+00
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: n_ff             = 3072
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: n_expert         = 0
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: n_expert_used    = 0
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: causal attn      = 0
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: pooling type     = 1
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: rope type        = 2
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: rope scaling     = linear
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: freq_base_train  = 1000.0
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: freq_scale_train = 1
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: n_ctx_orig_yarn  = 2048
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: rope_finetuned   = unknown
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: ssm_d_conv       = 0
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: ssm_d_inner      = 0
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: ssm_d_state      = 0
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_rank      = 0
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_b_c_rms   = 0
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: model type       = 137M
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: load_tensors: loading model tensors, this can take a while... (mmap = false)
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: load_tensors:          CPU model buffer size =   260.86 MiB
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_seq_max     = 1
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx         = 8192
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq = 8192
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_batch       = 512
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ubatch      = 512
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_init_from_model: flash_attn    = 0
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_base     = 1000.0
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_scale    = 1
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_init_from_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU  output buffer size =     0.00 MiB
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU compute buffer size =    23.00 MiB
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph nodes  = 453
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph splits = 1
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:43.819Z level=INFO source=server.go:624 msg="llama runner started in 0.25 seconds"
Mar 13 10:56:43 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 10:56:43 | 200 |  335.155074ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 10:56:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:44.010Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="5.2 GiB" free_swap="0 B"
Mar 13 10:56:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:44.011Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=27 layers.offload=0 layers.split="" memory.available="[5.2 GiB]" memory.gpu_overhead="0 B" memory.required.full="1.5 GiB" memory.required.partial="0 B" memory.required.kv="208.0 MiB" memory.required.allocations="[1.5 GiB]" memory.weights.total="664.5 MiB" memory.weights.repeating="358.5 MiB" memory.weights.nonrepeating="306.0 MiB" memory.graph.full="514.2 MiB" memory.graph.partial="750.5 MiB"
Mar 13 10:56:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:44.118Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 10:56:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:44.123Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
Mar 13 10:56:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:44.127Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 10:56:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:44.127Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 10:56:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:44.127Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.num_channels default=0
Mar 13 10:56:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:44.127Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.block_count default=0
Mar 13 10:56:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:44.127Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.embedding_length default=0
Mar 13 10:56:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:44.127Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.head_count default=0
Mar 13 10:56:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:44.127Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 10:56:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:44.127Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 10:56:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:44.127Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.layer_norm_epsilon default=0
Mar 13 10:56:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:44.127Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 10:56:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:44.132Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.freq_scale default=1
Mar 13 10:56:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:44.132Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.mm_tokens_per_image default=256
Mar 13 10:56:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:44.132Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --ollama-engine --model /usr/share/ollama/.ollama/models/blobs/sha256-dbe81da1e4bad3cc6ccb77540915ffe53e7a3ac0745ffa5e9d4626d39c15e09a --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 4 --port 43829"
Mar 13 10:56:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:44.133Z level=INFO source=sched.go:450 msg="loaded runners" count=2
Mar 13 10:56:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:44.133Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 10:56:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:44.133Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 10:56:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:44.145Z level=INFO source=runner.go:882 msg="starting ollama engine"
Mar 13 10:56:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:44.145Z level=INFO source=runner.go:938 msg="Server listening on 127.0.0.1:43829"
Mar 13 10:56:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:44.245Z level=WARN source=ggml.go:149 msg="key not found" key=general.name default=""
Mar 13 10:56:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:44.245Z level=WARN source=ggml.go:149 msg="key not found" key=general.description default=""
Mar 13 10:56:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:44.245Z level=INFO source=ggml.go:67 msg="" architecture=gemma3 file_type=Q4_K_M name="" description="" num_tensors=340 num_key_values=32
Mar 13 10:56:44 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 10:56:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:44.270Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 10:56:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:44.271Z level=INFO source=ggml.go:289 msg="model weights" buffer=CPU size="1.0 GiB"
Mar 13 10:56:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:44.421Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server loading model"
Mar 13 10:56:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:44.517Z level=INFO source=ggml.go:356 msg="compute graph" backend=CPU buffer_type=CPU
Mar 13 10:56:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:44.517Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 10:56:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:44.519Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
Mar 13 10:56:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:44.523Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 10:56:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:44.523Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 10:56:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:44.523Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.num_channels default=0
Mar 13 10:56:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:44.523Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.block_count default=0
Mar 13 10:56:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:44.523Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.embedding_length default=0
Mar 13 10:56:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:44.523Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.head_count default=0
Mar 13 10:56:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:44.523Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 10:56:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:44.523Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 10:56:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:44.523Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.layer_norm_epsilon default=0
Mar 13 10:56:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:44.523Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 10:56:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:44.528Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.freq_scale default=1
Mar 13 10:56:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:44.528Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.mm_tokens_per_image default=256
Mar 13 10:56:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T10:56:44.672Z level=INFO source=server.go:624 msg="llama runner started in 0.54 seconds"
Mar 13 10:57:03 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 10:57:03 | 200 | 19.739149888s |       127.0.0.1 | POST     "/api/generate"
Mar 13 10:57:31 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 10:57:31 | 200 |   45.524005ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 10:58:37 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 10:58:37 | 200 |          1m6s |       127.0.0.1 | POST     "/api/generate"
Mar 13 11:00:59 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 11:00:59 | 200 |   49.458247ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 11:01:25 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 11:01:25 | 200 | 26.705297675s |       127.0.0.1 | POST     "/api/generate"
Mar 13 11:01:51 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 11:01:51 | 200 |    61.08365ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 11:03:02 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 11:03:02 | 200 |         1m10s |       127.0.0.1 | POST     "/api/generate"
Mar 13 11:03:38 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 11:03:38 | 200 |   53.819419ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 11:04:28 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 11:04:28 | 200 | 50.482142278s |       127.0.0.1 | POST     "/api/generate"
Mar 13 11:08:34 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 11:08:34 | 200 |      30.597Âµs |       127.0.0.1 | HEAD     "/"
Mar 13 11:08:34 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 11:08:34 | 200 |    1.330091ms |       127.0.0.1 | GET      "/api/tags"
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:17:16.072Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="5.7 GiB" free_swap="0 B"
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:17:16.072Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:17:16.072Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.key_length default=64
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:17:16.072Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.value_length default=64
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:17:16.072Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:17:16.072Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=13 layers.offload=0 layers.split="" memory.available="[5.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="352.9 MiB" memory.required.partial="0 B" memory.required.kv="24.0 MiB" memory.required.allocations="[352.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 1
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: model type       = ?B
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_load: vocab only - skipping tensors
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:17:16.104Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 1 --port 38845"
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:17:16.104Z level=INFO source=sched.go:450 msg="loaded runners" count=1
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:17:16.104Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:17:16.105Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:17:16.115Z level=INFO source=runner.go:931 msg="starting go runner"
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:17:16.164Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:17:16.165Z level=INFO source=runner.go:991 msg="Server listening on 127.0.0.1:38845"
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 0
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: n_ctx_train      = 2048
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: n_embd           = 768
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: n_layer          = 12
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: n_head           = 12
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: n_head_kv        = 12
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: n_rot            = 64
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: n_swa            = 0
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_k    = 64
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_v    = 64
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: n_gqa            = 1
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: n_embd_k_gqa     = 768
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: n_embd_v_gqa     = 768
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: f_norm_eps       = 1.0e-12
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: f_norm_rms_eps   = 0.0e+00
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: f_clamp_kqv      = 0.0e+00
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: f_max_alibi_bias = 0.0e+00
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: f_logit_scale    = 0.0e+00
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: n_ff             = 3072
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: n_expert         = 0
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: n_expert_used    = 0
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: causal attn      = 0
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: pooling type     = 1
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: rope type        = 2
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: rope scaling     = linear
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: freq_base_train  = 1000.0
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: freq_scale_train = 1
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: n_ctx_orig_yarn  = 2048
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: rope_finetuned   = unknown
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: ssm_d_conv       = 0
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: ssm_d_inner      = 0
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: ssm_d_state      = 0
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_rank      = 0
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_b_c_rms   = 0
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: model type       = 137M
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: load_tensors: loading model tensors, this can take a while... (mmap = false)
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: load_tensors:          CPU model buffer size =   260.86 MiB
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_seq_max     = 1
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx         = 8192
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq = 8192
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_batch       = 512
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ubatch      = 512
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_init_from_model: flash_attn    = 0
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_base     = 1000.0
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_scale    = 1
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_init_from_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU  output buffer size =     0.00 MiB
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU compute buffer size =    23.00 MiB
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph nodes  = 453
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph splits = 1
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:17:16.356Z level=INFO source=server.go:624 msg="llama runner started in 0.25 seconds"
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 11:17:16 | 200 |  336.583042ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:17:16.472Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.key_length default=128
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:17:16.472Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.value_length default=128
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:17:16.473Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="5.2 GiB" free_swap="0 B"
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:17:16.473Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.key_length default=128
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:17:16.473Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.value_length default=128
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:17:16.473Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=29 layers.offload=0 layers.split="" memory.available="[5.2 GiB]" memory.gpu_overhead="0 B" memory.required.full="1.5 GiB" memory.required.partial="0 B" memory.required.kv="224.0 MiB" memory.required.allocations="[1.5 GiB]" memory.weights.total="976.1 MiB" memory.weights.repeating="793.5 MiB" memory.weights.nonrepeating="182.6 MiB" memory.graph.full="299.8 MiB" memory.graph.partial="482.3 MiB"
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc (version GGUF V3 (latest))
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = qwen2
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.type str              = model
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 1.5B
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                         general.size_label str              = 1.5B
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 1536
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 8960
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 12
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 2
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                          general.file_type u32              = 15
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ä  Ä ", "Ä Ä  Ä Ä ", "i n", "Ä  t",...
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  25:               general.quantization_version u32              = 2
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:  141 tensors
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q4_K:  169 tensors
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q6_K:   29 tensors
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: file type   = Q4_K - Medium
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: file size   = 1.04 GiB (5.00 BPW)
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 22
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.9310 MB
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: arch             = qwen2
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 1
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: model type       = ?B
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: model params     = 1.78 B
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: general.name     = DeepSeek R1 Distill Qwen 1.5B
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: vocab type       = BPE
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 151936
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 151387
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 151646 '<ï½beginâofâsentenceï½>'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: EOT token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: LF token         = 198 'Ä'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: FIM MID token    = 151660 '<|fim_middle|>'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: FIM PAD token    = 151662 '<|fim_pad|>'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: FIM REP token    = 151663 '<|repo_name|>'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: FIM SEP token    = 151664 '<|file_sep|>'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151662 '<|fim_pad|>'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151663 '<|repo_name|>'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151664 '<|file_sep|>'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: max token length = 256
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_load: vocab only - skipping tensors
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:17:16.669Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 4 --port 46143"
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:17:16.669Z level=INFO source=sched.go:450 msg="loaded runners" count=2
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:17:16.669Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:17:16.669Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:17:16.681Z level=INFO source=runner.go:931 msg="starting go runner"
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:17:16.706Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:17:16.706Z level=INFO source=runner.go:991 msg="Server listening on 127.0.0.1:46143"
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc (version GGUF V3 (latest))
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = qwen2
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.type str              = model
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 1.5B
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                         general.size_label str              = 1.5B
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 1536
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 8960
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 12
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 2
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                          general.file_type u32              = 15
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ä  Ä ", "Ä Ä  Ä Ä ", "i n", "Ä  t",...
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  25:               general.quantization_version u32              = 2
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:  141 tensors
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q4_K:  169 tensors
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q6_K:   29 tensors
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: file type   = Q4_K - Medium
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: file size   = 1.04 GiB (5.00 BPW)
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 22
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.9310 MB
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: arch             = qwen2
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 0
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: n_ctx_train      = 131072
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: n_embd           = 1536
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: n_layer          = 28
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: n_head           = 12
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: n_head_kv        = 2
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: n_rot            = 128
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: n_swa            = 0
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_k    = 128
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_v    = 128
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: n_gqa            = 6
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: n_embd_k_gqa     = 256
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: n_embd_v_gqa     = 256
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: f_norm_eps       = 0.0e+00
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: f_norm_rms_eps   = 1.0e-06
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: f_clamp_kqv      = 0.0e+00
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: f_max_alibi_bias = 0.0e+00
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: f_logit_scale    = 0.0e+00
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: n_ff             = 8960
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: n_expert         = 0
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: n_expert_used    = 0
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: causal attn      = 1
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: pooling type     = 0
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: rope type        = 2
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: rope scaling     = linear
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: freq_base_train  = 10000.0
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: freq_scale_train = 1
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: n_ctx_orig_yarn  = 131072
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: rope_finetuned   = unknown
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: ssm_d_conv       = 0
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: ssm_d_inner      = 0
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: ssm_d_state      = 0
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_rank      = 0
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_b_c_rms   = 0
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: model type       = 1.5B
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: model params     = 1.78 B
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: general.name     = DeepSeek R1 Distill Qwen 1.5B
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: vocab type       = BPE
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 151936
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 151387
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 151646 '<ï½beginâofâsentenceï½>'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: EOT token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: LF token         = 198 'Ä'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: FIM MID token    = 151660 '<|fim_middle|>'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: FIM PAD token    = 151662 '<|fim_pad|>'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: FIM REP token    = 151663 '<|repo_name|>'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: FIM SEP token    = 151664 '<|file_sep|>'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151662 '<|fim_pad|>'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151663 '<|repo_name|>'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151664 '<|file_sep|>'
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: print_info: max token length = 256
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: load_tensors: loading model tensors, this can take a while... (mmap = false)
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: load_tensors:          CPU model buffer size =  1059.89 MiB
Mar 13 11:17:16 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:17:16.920Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server loading model"
Mar 13 11:17:25 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_seq_max     = 4
Mar 13 11:17:25 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx         = 8192
Mar 13 11:17:25 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq = 2048
Mar 13 11:17:25 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_batch       = 2048
Mar 13 11:17:25 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ubatch      = 512
Mar 13 11:17:25 ubuntu-abhishek ollama[471085]: llama_init_from_model: flash_attn    = 0
Mar 13 11:17:25 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_base     = 10000.0
Mar 13 11:17:25 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_scale    = 1
Mar 13 11:17:25 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
Mar 13 11:17:25 ubuntu-abhishek ollama[471085]: llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1
Mar 13 11:17:25 ubuntu-abhishek ollama[471085]: llama_kv_cache_init:        CPU KV buffer size =   224.00 MiB
Mar 13 11:17:25 ubuntu-abhishek ollama[471085]: llama_init_from_model: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
Mar 13 11:17:25 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU  output buffer size =     2.34 MiB
Mar 13 11:17:25 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU compute buffer size =   302.75 MiB
Mar 13 11:17:25 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph nodes  = 986
Mar 13 11:17:25 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph splits = 1
Mar 13 11:17:25 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:17:25.450Z level=INFO source=server.go:624 msg="llama runner started in 8.78 seconds"
Mar 13 11:19:21 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 11:19:21 | 200 |          2m4s |       127.0.0.1 | POST     "/api/generate"
Mar 13 11:19:58 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 11:19:58 | 200 |   31.416597ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 11:21:13 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 11:21:13 | 200 |         1m15s |       127.0.0.1 | POST     "/api/generate"
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:27:50.822Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="4.8 GiB" free_swap="0 B"
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:27:50.822Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:27:50.822Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.key_length default=64
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:27:50.822Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.value_length default=64
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:27:50.822Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:27:50.823Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=13 layers.offload=0 layers.split="" memory.available="[4.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="352.9 MiB" memory.required.partial="0 B" memory.required.kv="24.0 MiB" memory.required.allocations="[352.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 1
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: model type       = ?B
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_load: vocab only - skipping tensors
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:27:50.855Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 1 --port 35485"
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:27:50.855Z level=INFO source=sched.go:450 msg="loaded runners" count=1
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:27:50.855Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:27:50.855Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:27:50.866Z level=INFO source=runner.go:931 msg="starting go runner"
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:27:50.908Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:27:50.909Z level=INFO source=runner.go:991 msg="Server listening on 127.0.0.1:35485"
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 0
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: n_ctx_train      = 2048
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: n_embd           = 768
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: n_layer          = 12
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: n_head           = 12
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: n_head_kv        = 12
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: n_rot            = 64
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: n_swa            = 0
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_k    = 64
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_v    = 64
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: n_gqa            = 1
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: n_embd_k_gqa     = 768
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: n_embd_v_gqa     = 768
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: f_norm_eps       = 1.0e-12
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: f_norm_rms_eps   = 0.0e+00
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: f_clamp_kqv      = 0.0e+00
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: f_max_alibi_bias = 0.0e+00
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: f_logit_scale    = 0.0e+00
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: n_ff             = 3072
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: n_expert         = 0
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: n_expert_used    = 0
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: causal attn      = 0
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: pooling type     = 1
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: rope type        = 2
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: rope scaling     = linear
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: freq_base_train  = 1000.0
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: freq_scale_train = 1
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: n_ctx_orig_yarn  = 2048
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: rope_finetuned   = unknown
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: ssm_d_conv       = 0
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: ssm_d_inner      = 0
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: ssm_d_state      = 0
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_rank      = 0
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_b_c_rms   = 0
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: model type       = 137M
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: load_tensors: loading model tensors, this can take a while... (mmap = false)
Mar 13 11:27:50 ubuntu-abhishek ollama[471085]: load_tensors:          CPU model buffer size =   260.86 MiB
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_seq_max     = 1
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx         = 8192
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq = 8192
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_batch       = 512
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ubatch      = 512
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_init_from_model: flash_attn    = 0
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_base     = 1000.0
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_scale    = 1
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:27:51.106Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server loading model"
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_init_from_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU  output buffer size =     0.00 MiB
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU compute buffer size =    23.00 MiB
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph nodes  = 453
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph splits = 1
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:27:51.357Z level=INFO source=server.go:624 msg="llama runner started in 0.50 seconds"
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 11:27:51 | 200 |  588.395075ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:27:51.471Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.key_length default=128
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:27:51.471Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.value_length default=128
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:27:51.471Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="4.3 GiB" free_swap="0 B"
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:27:51.471Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.key_length default=128
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:27:51.471Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.value_length default=128
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:27:51.471Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=29 layers.offload=0 layers.split="" memory.available="[4.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="1.5 GiB" memory.required.partial="0 B" memory.required.kv="224.0 MiB" memory.required.allocations="[1.5 GiB]" memory.weights.total="976.1 MiB" memory.weights.repeating="793.5 MiB" memory.weights.nonrepeating="182.6 MiB" memory.graph.full="299.8 MiB" memory.graph.partial="482.3 MiB"
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc (version GGUF V3 (latest))
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = qwen2
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.type str              = model
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 1.5B
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                         general.size_label str              = 1.5B
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 1536
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 8960
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 12
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 2
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                          general.file_type u32              = 15
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ä  Ä ", "Ä Ä  Ä Ä ", "i n", "Ä  t",...
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  25:               general.quantization_version u32              = 2
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:  141 tensors
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q4_K:  169 tensors
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q6_K:   29 tensors
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: file type   = Q4_K - Medium
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: file size   = 1.04 GiB (5.00 BPW)
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 22
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.9310 MB
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: arch             = qwen2
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 1
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: model type       = ?B
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: model params     = 1.78 B
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: general.name     = DeepSeek R1 Distill Qwen 1.5B
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: vocab type       = BPE
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 151936
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 151387
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 151646 '<ï½beginâofâsentenceï½>'
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: EOT token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: LF token         = 198 'Ä'
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: FIM MID token    = 151660 '<|fim_middle|>'
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: FIM PAD token    = 151662 '<|fim_pad|>'
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: FIM REP token    = 151663 '<|repo_name|>'
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: FIM SEP token    = 151664 '<|file_sep|>'
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151662 '<|fim_pad|>'
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151663 '<|repo_name|>'
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151664 '<|file_sep|>'
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: max token length = 256
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_load: vocab only - skipping tensors
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:27:51.649Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 4 --port 40553"
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:27:51.649Z level=INFO source=sched.go:450 msg="loaded runners" count=2
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:27:51.649Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:27:51.650Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:27:51.659Z level=INFO source=runner.go:931 msg="starting go runner"
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:27:51.700Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:27:51.701Z level=INFO source=runner.go:991 msg="Server listening on 127.0.0.1:40553"
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc (version GGUF V3 (latest))
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = qwen2
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.type str              = model
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 1.5B
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                         general.size_label str              = 1.5B
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 1536
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 8960
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 12
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 2
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                          general.file_type u32              = 15
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ä  Ä ", "Ä Ä  Ä Ä ", "i n", "Ä  t",...
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  25:               general.quantization_version u32              = 2
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:  141 tensors
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q4_K:  169 tensors
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q6_K:   29 tensors
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: file type   = Q4_K - Medium
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: file size   = 1.04 GiB (5.00 BPW)
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 22
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:27:51.901Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server loading model"
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.9310 MB
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: arch             = qwen2
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 0
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: n_ctx_train      = 131072
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: n_embd           = 1536
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: n_layer          = 28
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: n_head           = 12
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: n_head_kv        = 2
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: n_rot            = 128
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: n_swa            = 0
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_k    = 128
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_v    = 128
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: n_gqa            = 6
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: n_embd_k_gqa     = 256
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: n_embd_v_gqa     = 256
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: f_norm_eps       = 0.0e+00
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: f_norm_rms_eps   = 1.0e-06
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: f_clamp_kqv      = 0.0e+00
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: f_max_alibi_bias = 0.0e+00
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: f_logit_scale    = 0.0e+00
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: n_ff             = 8960
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: n_expert         = 0
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: n_expert_used    = 0
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: causal attn      = 1
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: pooling type     = 0
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: rope type        = 2
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: rope scaling     = linear
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: freq_base_train  = 10000.0
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: freq_scale_train = 1
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: n_ctx_orig_yarn  = 131072
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: rope_finetuned   = unknown
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: ssm_d_conv       = 0
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: ssm_d_inner      = 0
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: ssm_d_state      = 0
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_rank      = 0
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_b_c_rms   = 0
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: model type       = 1.5B
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: model params     = 1.78 B
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: general.name     = DeepSeek R1 Distill Qwen 1.5B
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: vocab type       = BPE
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 151936
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 151387
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 151646 '<ï½beginâofâsentenceï½>'
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: EOT token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: LF token         = 198 'Ä'
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: FIM MID token    = 151660 '<|fim_middle|>'
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: FIM PAD token    = 151662 '<|fim_pad|>'
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: FIM REP token    = 151663 '<|repo_name|>'
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: FIM SEP token    = 151664 '<|file_sep|>'
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151662 '<|fim_pad|>'
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151663 '<|repo_name|>'
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151664 '<|file_sep|>'
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: print_info: max token length = 256
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: load_tensors: loading model tensors, this can take a while... (mmap = false)
Mar 13 11:27:51 ubuntu-abhishek ollama[471085]: load_tensors:          CPU model buffer size =  1059.89 MiB
Mar 13 11:27:56 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_seq_max     = 4
Mar 13 11:27:56 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx         = 8192
Mar 13 11:27:56 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq = 2048
Mar 13 11:27:56 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_batch       = 2048
Mar 13 11:27:56 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ubatch      = 512
Mar 13 11:27:56 ubuntu-abhishek ollama[471085]: llama_init_from_model: flash_attn    = 0
Mar 13 11:27:56 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_base     = 10000.0
Mar 13 11:27:56 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_scale    = 1
Mar 13 11:27:56 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
Mar 13 11:27:56 ubuntu-abhishek ollama[471085]: llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1
Mar 13 11:27:56 ubuntu-abhishek ollama[471085]: llama_kv_cache_init:        CPU KV buffer size =   224.00 MiB
Mar 13 11:27:56 ubuntu-abhishek ollama[471085]: llama_init_from_model: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
Mar 13 11:27:56 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU  output buffer size =     2.34 MiB
Mar 13 11:27:56 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU compute buffer size =   302.75 MiB
Mar 13 11:27:56 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph nodes  = 986
Mar 13 11:27:56 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph splits = 1
Mar 13 11:27:56 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:27:56.667Z level=INFO source=server.go:624 msg="llama runner started in 5.02 seconds"
Mar 13 11:29:33 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 11:29:33 | 200 |         1m42s |       127.0.0.1 | POST     "/api/generate"
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:44.282Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:44.282Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.key_length default=64
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:44.282Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.value_length default=64
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:44.282Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:44.282Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="3.5 GiB" free_swap="0 B"
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:44.282Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:44.282Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.key_length default=64
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:44.282Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.value_length default=64
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:44.282Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:44.282Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=13 layers.offload=0 layers.split="" memory.available="[3.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="352.9 MiB" memory.required.partial="0 B" memory.required.kv="24.0 MiB" memory.required.allocations="[352.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 1
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: model type       = ?B
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_load: vocab only - skipping tensors
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:44.315Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 1 --port 34035"
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:44.315Z level=INFO source=sched.go:450 msg="loaded runners" count=2
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:44.315Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:44.315Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:44.332Z level=INFO source=runner.go:931 msg="starting go runner"
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:44.376Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:44.377Z level=INFO source=runner.go:991 msg="Server listening on 127.0.0.1:34035"
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 0
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: n_ctx_train      = 2048
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: n_embd           = 768
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: n_layer          = 12
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: n_head           = 12
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: n_head_kv        = 12
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: n_rot            = 64
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: n_swa            = 0
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_k    = 64
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_v    = 64
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: n_gqa            = 1
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: n_embd_k_gqa     = 768
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: n_embd_v_gqa     = 768
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: f_norm_eps       = 1.0e-12
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: f_norm_rms_eps   = 0.0e+00
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: f_clamp_kqv      = 0.0e+00
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: f_max_alibi_bias = 0.0e+00
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: f_logit_scale    = 0.0e+00
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: n_ff             = 3072
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: n_expert         = 0
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: n_expert_used    = 0
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: causal attn      = 0
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: pooling type     = 1
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: rope type        = 2
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: rope scaling     = linear
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: freq_base_train  = 1000.0
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: freq_scale_train = 1
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: n_ctx_orig_yarn  = 2048
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: rope_finetuned   = unknown
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: ssm_d_conv       = 0
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: ssm_d_inner      = 0
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: ssm_d_state      = 0
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_rank      = 0
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_b_c_rms   = 0
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: model type       = 137M
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: load_tensors: loading model tensors, this can take a while... (mmap = false)
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: load_tensors:          CPU model buffer size =   260.86 MiB
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_seq_max     = 1
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx         = 8192
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq = 8192
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_batch       = 512
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ubatch      = 512
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_init_from_model: flash_attn    = 0
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_base     = 1000.0
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_scale    = 1
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_init_from_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU  output buffer size =     0.00 MiB
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU compute buffer size =    23.00 MiB
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph nodes  = 453
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph splits = 1
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:44.566Z level=INFO source=server.go:624 msg="llama runner started in 0.25 seconds"
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 11:33:44 | 200 |  338.732166ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:44.811Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="2.9 GiB" free_swap="0 B"
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:44.811Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=27 layers.offload=0 layers.split="" memory.available="[2.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="1.5 GiB" memory.required.partial="0 B" memory.required.kv="208.0 MiB" memory.required.allocations="[1.5 GiB]" memory.weights.total="664.5 MiB" memory.weights.repeating="358.5 MiB" memory.weights.nonrepeating="306.0 MiB" memory.graph.full="514.2 MiB" memory.graph.partial="750.5 MiB"
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:44.918Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:44.931Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:44.935Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:44.935Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:44.935Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.num_channels default=0
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:44.935Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.block_count default=0
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:44.935Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.embedding_length default=0
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:44.935Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.head_count default=0
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:44.935Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:44.935Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:44.935Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.layer_norm_epsilon default=0
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:44.935Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:44.942Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.freq_scale default=1
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:44.942Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.mm_tokens_per_image default=256
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:44.942Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --ollama-engine --model /usr/share/ollama/.ollama/models/blobs/sha256-dbe81da1e4bad3cc6ccb77540915ffe53e7a3ac0745ffa5e9d4626d39c15e09a --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 4 --port 36415"
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:44.942Z level=INFO source=sched.go:450 msg="loaded runners" count=3
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:44.942Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:44.942Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:44.952Z level=INFO source=runner.go:882 msg="starting ollama engine"
Mar 13 11:33:44 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:44.954Z level=INFO source=runner.go:938 msg="Server listening on 127.0.0.1:36415"
Mar 13 11:33:45 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:45.064Z level=WARN source=ggml.go:149 msg="key not found" key=general.name default=""
Mar 13 11:33:45 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:45.065Z level=WARN source=ggml.go:149 msg="key not found" key=general.description default=""
Mar 13 11:33:45 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:45.065Z level=INFO source=ggml.go:67 msg="" architecture=gemma3 file_type=Q4_K_M name="" description="" num_tensors=340 num_key_values=32
Mar 13 11:33:45 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 11:33:45 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:45.069Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 11:33:45 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:45.070Z level=INFO source=ggml.go:289 msg="model weights" buffer=CPU size="1.0 GiB"
Mar 13 11:33:45 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:45.316Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server loading model"
Mar 13 11:33:50 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:50.149Z level=INFO source=ggml.go:356 msg="compute graph" backend=CPU buffer_type=CPU
Mar 13 11:33:50 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:50.153Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 11:33:50 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:50.155Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
Mar 13 11:33:50 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:50.160Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 11:33:50 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:50.160Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 11:33:50 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:50.160Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.num_channels default=0
Mar 13 11:33:50 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:50.160Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.block_count default=0
Mar 13 11:33:50 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:50.160Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.embedding_length default=0
Mar 13 11:33:50 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:50.160Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.head_count default=0
Mar 13 11:33:50 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:50.160Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 11:33:50 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:50.160Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 11:33:50 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:50.160Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.layer_norm_epsilon default=0
Mar 13 11:33:50 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:50.160Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 11:33:50 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:50.166Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.freq_scale default=1
Mar 13 11:33:50 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:50.166Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.mm_tokens_per_image default=256
Mar 13 11:33:50 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:33:50.186Z level=INFO source=server.go:624 msg="llama runner started in 5.24 seconds"
Mar 13 11:35:14 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 11:35:14 | 200 |         1m29s |       127.0.0.1 | POST     "/api/generate"
Mar 13 11:35:17 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 11:35:17 | 200 |   50.195522ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 11:37:04 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 11:37:04 | 200 |         1m46s |       127.0.0.1 | POST     "/api/generate"
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 11:37:58 | 200 |  134.779332ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:37:58.396Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.key_length default=128
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:37:58.396Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.value_length default=128
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:37:58.396Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="2.7 GiB" free_swap="0 B"
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:37:58.396Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.key_length default=128
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:37:58.396Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.value_length default=128
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:37:58.397Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=29 layers.offload=0 layers.split="" memory.available="[2.7 GiB]" memory.gpu_overhead="0 B" memory.required.full="1.5 GiB" memory.required.partial="0 B" memory.required.kv="224.0 MiB" memory.required.allocations="[1.5 GiB]" memory.weights.total="976.1 MiB" memory.weights.repeating="793.5 MiB" memory.weights.nonrepeating="182.6 MiB" memory.graph.full="299.8 MiB" memory.graph.partial="482.3 MiB"
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc (version GGUF V3 (latest))
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = qwen2
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.type str              = model
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 1.5B
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                         general.size_label str              = 1.5B
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 1536
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 8960
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 12
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 2
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                          general.file_type u32              = 15
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ä  Ä ", "Ä Ä  Ä Ä ", "i n", "Ä  t",...
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  25:               general.quantization_version u32              = 2
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:  141 tensors
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q4_K:  169 tensors
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q6_K:   29 tensors
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: file type   = Q4_K - Medium
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: file size   = 1.04 GiB (5.00 BPW)
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 22
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.9310 MB
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: arch             = qwen2
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 1
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: model type       = ?B
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: model params     = 1.78 B
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: general.name     = DeepSeek R1 Distill Qwen 1.5B
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: vocab type       = BPE
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 151936
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 151387
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 151646 '<ï½beginâofâsentenceï½>'
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: EOT token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: LF token         = 198 'Ä'
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: FIM MID token    = 151660 '<|fim_middle|>'
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: FIM PAD token    = 151662 '<|fim_pad|>'
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: FIM REP token    = 151663 '<|repo_name|>'
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: FIM SEP token    = 151664 '<|file_sep|>'
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151662 '<|fim_pad|>'
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151663 '<|repo_name|>'
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151664 '<|file_sep|>'
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: max token length = 256
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_load: vocab only - skipping tensors
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:37:58.596Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 4 --port 39021"
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:37:58.596Z level=INFO source=sched.go:450 msg="loaded runners" count=3
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:37:58.596Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:37:58.596Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:37:58.640Z level=INFO source=runner.go:931 msg="starting go runner"
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:37:58.688Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:37:58.689Z level=INFO source=runner.go:991 msg="Server listening on 127.0.0.1:39021"
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc (version GGUF V3 (latest))
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = qwen2
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.type str              = model
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 1.5B
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                         general.size_label str              = 1.5B
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 1536
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 8960
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 12
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 2
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                          general.file_type u32              = 15
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ä  Ä ", "Ä Ä  Ä Ä ", "i n", "Ä  t",...
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  25:               general.quantization_version u32              = 2
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:  141 tensors
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q4_K:  169 tensors
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q6_K:   29 tensors
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: file type   = Q4_K - Medium
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: file size   = 1.04 GiB (5.00 BPW)
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:37:58.848Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server loading model"
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 22
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.9310 MB
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: arch             = qwen2
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 0
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: n_ctx_train      = 131072
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: n_embd           = 1536
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: n_layer          = 28
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: n_head           = 12
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: n_head_kv        = 2
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: n_rot            = 128
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: n_swa            = 0
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_k    = 128
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_v    = 128
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: n_gqa            = 6
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: n_embd_k_gqa     = 256
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: n_embd_v_gqa     = 256
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: f_norm_eps       = 0.0e+00
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: f_norm_rms_eps   = 1.0e-06
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: f_clamp_kqv      = 0.0e+00
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: f_max_alibi_bias = 0.0e+00
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: f_logit_scale    = 0.0e+00
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: n_ff             = 8960
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: n_expert         = 0
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: n_expert_used    = 0
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: causal attn      = 1
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: pooling type     = 0
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: rope type        = 2
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: rope scaling     = linear
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: freq_base_train  = 10000.0
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: freq_scale_train = 1
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: n_ctx_orig_yarn  = 131072
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: rope_finetuned   = unknown
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: ssm_d_conv       = 0
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: ssm_d_inner      = 0
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: ssm_d_state      = 0
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_rank      = 0
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_b_c_rms   = 0
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: model type       = 1.5B
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: model params     = 1.78 B
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: general.name     = DeepSeek R1 Distill Qwen 1.5B
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: vocab type       = BPE
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 151936
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 151387
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 151646 '<ï½beginâofâsentenceï½>'
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: EOT token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: LF token         = 198 'Ä'
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: FIM MID token    = 151660 '<|fim_middle|>'
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: FIM PAD token    = 151662 '<|fim_pad|>'
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: FIM REP token    = 151663 '<|repo_name|>'
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: FIM SEP token    = 151664 '<|file_sep|>'
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151662 '<|fim_pad|>'
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151663 '<|repo_name|>'
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151664 '<|file_sep|>'
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: print_info: max token length = 256
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: load_tensors: loading model tensors, this can take a while... (mmap = false)
Mar 13 11:37:58 ubuntu-abhishek ollama[471085]: load_tensors:          CPU model buffer size =  1059.89 MiB
Mar 13 11:38:07 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_seq_max     = 4
Mar 13 11:38:07 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx         = 8192
Mar 13 11:38:07 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq = 2048
Mar 13 11:38:07 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_batch       = 2048
Mar 13 11:38:07 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ubatch      = 512
Mar 13 11:38:07 ubuntu-abhishek ollama[471085]: llama_init_from_model: flash_attn    = 0
Mar 13 11:38:07 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_base     = 10000.0
Mar 13 11:38:07 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_scale    = 1
Mar 13 11:38:07 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
Mar 13 11:38:07 ubuntu-abhishek ollama[471085]: llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1
Mar 13 11:38:07 ubuntu-abhishek ollama[471085]: llama_kv_cache_init:        CPU KV buffer size =   224.00 MiB
Mar 13 11:38:07 ubuntu-abhishek ollama[471085]: llama_init_from_model: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
Mar 13 11:38:07 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU  output buffer size =     2.34 MiB
Mar 13 11:38:07 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU compute buffer size =   302.75 MiB
Mar 13 11:38:07 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph nodes  = 986
Mar 13 11:38:07 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph splits = 1
Mar 13 11:38:07 ubuntu-abhishek ollama[471085]: time=2025-03-13T11:38:07.378Z level=INFO source=server.go:624 msg="llama runner started in 8.78 seconds"
Mar 13 11:39:55 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 11:39:55 | 200 |         1m57s |       127.0.0.1 | POST     "/api/generate"
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:16:09.146Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="4.8 GiB" free_swap="0 B"
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:16:09.146Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:16:09.146Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.key_length default=64
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:16:09.146Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.value_length default=64
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:16:09.146Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:16:09.146Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=13 layers.offload=0 layers.split="" memory.available="[4.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="352.9 MiB" memory.required.partial="0 B" memory.required.kv="24.0 MiB" memory.required.allocations="[352.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 1
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: model type       = ?B
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_load: vocab only - skipping tensors
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:16:09.180Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 1 --port 36661"
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:16:09.181Z level=INFO source=sched.go:450 msg="loaded runners" count=1
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:16:09.181Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:16:09.181Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:16:09.194Z level=INFO source=runner.go:931 msg="starting go runner"
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:16:09.240Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:16:09.241Z level=INFO source=runner.go:991 msg="Server listening on 127.0.0.1:36661"
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 0
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: n_ctx_train      = 2048
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: n_embd           = 768
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: n_layer          = 12
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: n_head           = 12
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: n_head_kv        = 12
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: n_rot            = 64
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: n_swa            = 0
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_k    = 64
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_v    = 64
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: n_gqa            = 1
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: n_embd_k_gqa     = 768
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: n_embd_v_gqa     = 768
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: f_norm_eps       = 1.0e-12
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: f_norm_rms_eps   = 0.0e+00
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: f_clamp_kqv      = 0.0e+00
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: f_max_alibi_bias = 0.0e+00
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: f_logit_scale    = 0.0e+00
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: n_ff             = 3072
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: n_expert         = 0
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: n_expert_used    = 0
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: causal attn      = 0
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: pooling type     = 1
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: rope type        = 2
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: rope scaling     = linear
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: freq_base_train  = 1000.0
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: freq_scale_train = 1
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: n_ctx_orig_yarn  = 2048
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: rope_finetuned   = unknown
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: ssm_d_conv       = 0
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: ssm_d_inner      = 0
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: ssm_d_state      = 0
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_rank      = 0
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_b_c_rms   = 0
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: model type       = 137M
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: load_tensors: loading model tensors, this can take a while... (mmap = false)
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: load_tensors:          CPU model buffer size =   260.86 MiB
Mar 13 12:16:09 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:16:09.432Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server loading model"
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_seq_max     = 1
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx         = 8192
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq = 8192
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_batch       = 512
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ubatch      = 512
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_init_from_model: flash_attn    = 0
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_base     = 1000.0
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_scale    = 1
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_init_from_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU  output buffer size =     0.00 MiB
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU compute buffer size =    23.00 MiB
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph nodes  = 453
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph splits = 1
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:16:11.438Z level=INFO source=server.go:624 msg="llama runner started in 2.26 seconds"
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 12:16:11 | 200 |  2.415174987s |       127.0.0.1 | POST     "/api/embed"
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:16:11.618Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.key_length default=128
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:16:11.618Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.value_length default=128
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:16:11.618Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="4.2 GiB" free_swap="0 B"
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:16:11.618Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.key_length default=128
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:16:11.618Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.value_length default=128
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:16:11.618Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=29 layers.offload=0 layers.split="" memory.available="[4.2 GiB]" memory.gpu_overhead="0 B" memory.required.full="1.5 GiB" memory.required.partial="0 B" memory.required.kv="224.0 MiB" memory.required.allocations="[1.5 GiB]" memory.weights.total="976.1 MiB" memory.weights.repeating="793.5 MiB" memory.weights.nonrepeating="182.6 MiB" memory.graph.full="299.8 MiB" memory.graph.partial="482.3 MiB"
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc (version GGUF V3 (latest))
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = qwen2
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.type str              = model
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 1.5B
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                         general.size_label str              = 1.5B
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 1536
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 8960
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 12
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 2
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                          general.file_type u32              = 15
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ä  Ä ", "Ä Ä  Ä Ä ", "i n", "Ä  t",...
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  25:               general.quantization_version u32              = 2
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:  141 tensors
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q4_K:  169 tensors
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q6_K:   29 tensors
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: print_info: file type   = Q4_K - Medium
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: print_info: file size   = 1.04 GiB (5.00 BPW)
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 22
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.9310 MB
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: print_info: arch             = qwen2
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 1
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: print_info: model type       = ?B
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: print_info: model params     = 1.78 B
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: print_info: general.name     = DeepSeek R1 Distill Qwen 1.5B
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: print_info: vocab type       = BPE
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 151936
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 151387
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 151646 '<ï½beginâofâsentenceï½>'
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: print_info: EOT token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: print_info: LF token         = 198 'Ä'
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: print_info: FIM MID token    = 151660 '<|fim_middle|>'
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: print_info: FIM PAD token    = 151662 '<|fim_pad|>'
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: print_info: FIM REP token    = 151663 '<|repo_name|>'
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: print_info: FIM SEP token    = 151664 '<|file_sep|>'
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151662 '<|fim_pad|>'
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151663 '<|repo_name|>'
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151664 '<|file_sep|>'
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: print_info: max token length = 256
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_load: vocab only - skipping tensors
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:16:11.811Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 4 --port 44703"
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:16:11.812Z level=INFO source=sched.go:450 msg="loaded runners" count=2
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:16:11.812Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:16:11.812Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:16:11.824Z level=INFO source=runner.go:931 msg="starting go runner"
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:16:11.864Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:16:11.865Z level=INFO source=runner.go:991 msg="Server listening on 127.0.0.1:44703"
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc (version GGUF V3 (latest))
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = qwen2
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.type str              = model
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 1.5B
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                         general.size_label str              = 1.5B
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 1536
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 8960
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 12
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 2
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                          general.file_type u32              = 15
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ä  Ä ", "Ä Ä  Ä Ä ", "i n", "Ä  t",...
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  25:               general.quantization_version u32              = 2
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:  141 tensors
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q4_K:  169 tensors
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q6_K:   29 tensors
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: print_info: file type   = Q4_K - Medium
Mar 13 12:16:11 ubuntu-abhishek ollama[471085]: print_info: file size   = 1.04 GiB (5.00 BPW)
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 22
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.9310 MB
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: arch             = qwen2
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 0
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: n_ctx_train      = 131072
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: n_embd           = 1536
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: n_layer          = 28
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: n_head           = 12
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: n_head_kv        = 2
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: n_rot            = 128
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: n_swa            = 0
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_k    = 128
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_v    = 128
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: n_gqa            = 6
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: n_embd_k_gqa     = 256
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: n_embd_v_gqa     = 256
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: f_norm_eps       = 0.0e+00
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: f_norm_rms_eps   = 1.0e-06
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: f_clamp_kqv      = 0.0e+00
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: f_max_alibi_bias = 0.0e+00
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: f_logit_scale    = 0.0e+00
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: n_ff             = 8960
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: n_expert         = 0
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: n_expert_used    = 0
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: causal attn      = 1
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: pooling type     = 0
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: rope type        = 2
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: rope scaling     = linear
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: freq_base_train  = 10000.0
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: freq_scale_train = 1
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: n_ctx_orig_yarn  = 131072
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: rope_finetuned   = unknown
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: ssm_d_conv       = 0
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: ssm_d_inner      = 0
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: ssm_d_state      = 0
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_rank      = 0
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_b_c_rms   = 0
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: model type       = 1.5B
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: model params     = 1.78 B
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: general.name     = DeepSeek R1 Distill Qwen 1.5B
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: vocab type       = BPE
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 151936
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 151387
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 151646 '<ï½beginâofâsentenceï½>'
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: EOT token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: LF token         = 198 'Ä'
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: FIM MID token    = 151660 '<|fim_middle|>'
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: FIM PAD token    = 151662 '<|fim_pad|>'
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: FIM REP token    = 151663 '<|repo_name|>'
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: FIM SEP token    = 151664 '<|file_sep|>'
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151662 '<|fim_pad|>'
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151663 '<|repo_name|>'
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151664 '<|file_sep|>'
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: print_info: max token length = 256
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: load_tensors: loading model tensors, this can take a while... (mmap = false)
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:16:12.064Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server loading model"
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: load_tensors:          CPU model buffer size =  1059.89 MiB
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_seq_max     = 4
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx         = 8192
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq = 2048
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_batch       = 2048
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ubatch      = 512
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: llama_init_from_model: flash_attn    = 0
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_base     = 10000.0
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_scale    = 1
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: llama_kv_cache_init:        CPU KV buffer size =   224.00 MiB
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: llama_init_from_model: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU  output buffer size =     2.34 MiB
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU compute buffer size =   302.75 MiB
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph nodes  = 986
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph splits = 1
Mar 13 12:16:12 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:16:12.565Z level=INFO source=server.go:624 msg="llama runner started in 0.75 seconds"
Mar 13 12:22:46 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 12:22:46 | 200 |         6m34s |       127.0.0.1 | POST     "/api/generate"
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:30:17.391Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="5.4 GiB" free_swap="0 B"
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:30:17.391Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:30:17.391Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.key_length default=64
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:30:17.391Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.value_length default=64
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:30:17.391Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:30:17.391Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=13 layers.offload=0 layers.split="" memory.available="[5.4 GiB]" memory.gpu_overhead="0 B" memory.required.full="352.9 MiB" memory.required.partial="0 B" memory.required.kv="24.0 MiB" memory.required.allocations="[352.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 1
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: model type       = ?B
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_load: vocab only - skipping tensors
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:30:17.423Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 1 --port 42355"
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:30:17.424Z level=INFO source=sched.go:450 msg="loaded runners" count=1
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:30:17.424Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:30:17.424Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:30:17.436Z level=INFO source=runner.go:931 msg="starting go runner"
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:30:17.480Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:30:17.481Z level=INFO source=runner.go:991 msg="Server listening on 127.0.0.1:42355"
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 0
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: n_ctx_train      = 2048
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: n_embd           = 768
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: n_layer          = 12
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: n_head           = 12
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: n_head_kv        = 12
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: n_rot            = 64
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: n_swa            = 0
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_k    = 64
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_v    = 64
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: n_gqa            = 1
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: n_embd_k_gqa     = 768
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: n_embd_v_gqa     = 768
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: f_norm_eps       = 1.0e-12
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: f_norm_rms_eps   = 0.0e+00
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: f_clamp_kqv      = 0.0e+00
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: f_max_alibi_bias = 0.0e+00
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: f_logit_scale    = 0.0e+00
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: n_ff             = 3072
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: n_expert         = 0
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: n_expert_used    = 0
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: causal attn      = 0
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: pooling type     = 1
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: rope type        = 2
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: rope scaling     = linear
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: freq_base_train  = 1000.0
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: freq_scale_train = 1
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: n_ctx_orig_yarn  = 2048
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: rope_finetuned   = unknown
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: ssm_d_conv       = 0
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: ssm_d_inner      = 0
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: ssm_d_state      = 0
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_rank      = 0
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_b_c_rms   = 0
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: model type       = 137M
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: load_tensors: loading model tensors, this can take a while... (mmap = false)
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: load_tensors:          CPU model buffer size =   260.86 MiB
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_seq_max     = 1
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx         = 8192
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq = 8192
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_batch       = 512
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ubatch      = 512
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_init_from_model: flash_attn    = 0
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_base     = 1000.0
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_scale    = 1
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_init_from_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU  output buffer size =     0.00 MiB
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU compute buffer size =    23.00 MiB
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph nodes  = 453
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph splits = 1
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:30:17.675Z level=INFO source=server.go:624 msg="llama runner started in 0.25 seconds"
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 12:30:17 | 200 |  404.650315ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:30:17.850Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.key_length default=128
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:30:17.850Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.value_length default=128
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:30:17.850Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="4.9 GiB" free_swap="0 B"
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:30:17.850Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.key_length default=128
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:30:17.850Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.value_length default=128
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:30:17.850Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=29 layers.offload=0 layers.split="" memory.available="[4.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="1.5 GiB" memory.required.partial="0 B" memory.required.kv="224.0 MiB" memory.required.allocations="[1.5 GiB]" memory.weights.total="976.1 MiB" memory.weights.repeating="793.5 MiB" memory.weights.nonrepeating="182.6 MiB" memory.graph.full="299.8 MiB" memory.graph.partial="482.3 MiB"
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc (version GGUF V3 (latest))
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = qwen2
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.type str              = model
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 1.5B
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                         general.size_label str              = 1.5B
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 1536
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 8960
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 12
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 2
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                          general.file_type u32              = 15
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ä  Ä ", "Ä Ä  Ä Ä ", "i n", "Ä  t",...
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  25:               general.quantization_version u32              = 2
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:  141 tensors
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q4_K:  169 tensors
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q6_K:   29 tensors
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: file type   = Q4_K - Medium
Mar 13 12:30:17 ubuntu-abhishek ollama[471085]: print_info: file size   = 1.04 GiB (5.00 BPW)
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 22
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.9310 MB
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: arch             = qwen2
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 1
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: model type       = ?B
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: model params     = 1.78 B
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: general.name     = DeepSeek R1 Distill Qwen 1.5B
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: vocab type       = BPE
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 151936
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 151387
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 151646 '<ï½beginâofâsentenceï½>'
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: EOT token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: LF token         = 198 'Ä'
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: FIM MID token    = 151660 '<|fim_middle|>'
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: FIM PAD token    = 151662 '<|fim_pad|>'
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: FIM REP token    = 151663 '<|repo_name|>'
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: FIM SEP token    = 151664 '<|file_sep|>'
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151662 '<|fim_pad|>'
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151663 '<|repo_name|>'
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151664 '<|file_sep|>'
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: max token length = 256
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_model_load: vocab only - skipping tensors
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:30:18.045Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 4 --port 46671"
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:30:18.046Z level=INFO source=sched.go:450 msg="loaded runners" count=2
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:30:18.046Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:30:18.046Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:30:18.057Z level=INFO source=runner.go:931 msg="starting go runner"
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:30:18.100Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:30:18.101Z level=INFO source=runner.go:991 msg="Server listening on 127.0.0.1:46671"
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc (version GGUF V3 (latest))
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = qwen2
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.type str              = model
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 1.5B
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                         general.size_label str              = 1.5B
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 1536
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 8960
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 12
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 2
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                          general.file_type u32              = 15
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ä  Ä ", "Ä Ä  Ä Ä ", "i n", "Ä  t",...
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  25:               general.quantization_version u32              = 2
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:  141 tensors
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q4_K:  169 tensors
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q6_K:   29 tensors
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: file type   = Q4_K - Medium
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: file size   = 1.04 GiB (5.00 BPW)
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 22
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:30:18.297Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server loading model"
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.9310 MB
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: arch             = qwen2
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 0
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: n_ctx_train      = 131072
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: n_embd           = 1536
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: n_layer          = 28
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: n_head           = 12
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: n_head_kv        = 2
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: n_rot            = 128
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: n_swa            = 0
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_k    = 128
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_v    = 128
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: n_gqa            = 6
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: n_embd_k_gqa     = 256
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: n_embd_v_gqa     = 256
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: f_norm_eps       = 0.0e+00
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: f_norm_rms_eps   = 1.0e-06
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: f_clamp_kqv      = 0.0e+00
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: f_max_alibi_bias = 0.0e+00
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: f_logit_scale    = 0.0e+00
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: n_ff             = 8960
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: n_expert         = 0
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: n_expert_used    = 0
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: causal attn      = 1
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: pooling type     = 0
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: rope type        = 2
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: rope scaling     = linear
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: freq_base_train  = 10000.0
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: freq_scale_train = 1
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: n_ctx_orig_yarn  = 131072
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: rope_finetuned   = unknown
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: ssm_d_conv       = 0
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: ssm_d_inner      = 0
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: ssm_d_state      = 0
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_rank      = 0
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_b_c_rms   = 0
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: model type       = 1.5B
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: model params     = 1.78 B
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: general.name     = DeepSeek R1 Distill Qwen 1.5B
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: vocab type       = BPE
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 151936
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 151387
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 151646 '<ï½beginâofâsentenceï½>'
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: EOT token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: LF token         = 198 'Ä'
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: FIM MID token    = 151660 '<|fim_middle|>'
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: FIM PAD token    = 151662 '<|fim_pad|>'
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: FIM REP token    = 151663 '<|repo_name|>'
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: FIM SEP token    = 151664 '<|file_sep|>'
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151662 '<|fim_pad|>'
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151663 '<|repo_name|>'
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151664 '<|file_sep|>'
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: print_info: max token length = 256
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: load_tensors: loading model tensors, this can take a while... (mmap = false)
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: load_tensors:          CPU model buffer size =  1059.89 MiB
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_seq_max     = 4
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx         = 8192
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq = 2048
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_batch       = 2048
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ubatch      = 512
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_init_from_model: flash_attn    = 0
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_base     = 10000.0
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_scale    = 1
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_kv_cache_init:        CPU KV buffer size =   224.00 MiB
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_init_from_model: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU  output buffer size =     2.34 MiB
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU compute buffer size =   302.75 MiB
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph nodes  = 986
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph splits = 1
Mar 13 12:30:18 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:30:18.548Z level=INFO source=server.go:624 msg="llama runner started in 0.50 seconds"
Mar 13 12:34:06 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 12:34:06 | 200 |         3m48s |       127.0.0.1 | POST     "/api/generate"
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:44:23.024Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="5.4 GiB" free_swap="0 B"
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:44:23.024Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:44:23.024Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.key_length default=64
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:44:23.024Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.value_length default=64
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:44:23.024Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:44:23.024Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=13 layers.offload=0 layers.split="" memory.available="[5.4 GiB]" memory.gpu_overhead="0 B" memory.required.full="352.9 MiB" memory.required.partial="0 B" memory.required.kv="24.0 MiB" memory.required.allocations="[352.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 1
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: model type       = ?B
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_load: vocab only - skipping tensors
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:44:23.057Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 1 --port 33751"
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:44:23.057Z level=INFO source=sched.go:450 msg="loaded runners" count=1
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:44:23.057Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:44:23.057Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:44:23.067Z level=INFO source=runner.go:931 msg="starting go runner"
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:44:23.108Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:44:23.109Z level=INFO source=runner.go:991 msg="Server listening on 127.0.0.1:33751"
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 0
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: n_ctx_train      = 2048
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: n_embd           = 768
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: n_layer          = 12
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: n_head           = 12
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: n_head_kv        = 12
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: n_rot            = 64
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: n_swa            = 0
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_k    = 64
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_v    = 64
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: n_gqa            = 1
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: n_embd_k_gqa     = 768
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: n_embd_v_gqa     = 768
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: f_norm_eps       = 1.0e-12
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: f_norm_rms_eps   = 0.0e+00
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: f_clamp_kqv      = 0.0e+00
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: f_max_alibi_bias = 0.0e+00
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: f_logit_scale    = 0.0e+00
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: n_ff             = 3072
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: n_expert         = 0
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: n_expert_used    = 0
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: causal attn      = 0
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: pooling type     = 1
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: rope type        = 2
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: rope scaling     = linear
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: freq_base_train  = 1000.0
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: freq_scale_train = 1
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: n_ctx_orig_yarn  = 2048
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: rope_finetuned   = unknown
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: ssm_d_conv       = 0
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: ssm_d_inner      = 0
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: ssm_d_state      = 0
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_rank      = 0
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_b_c_rms   = 0
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: model type       = 137M
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: load_tensors: loading model tensors, this can take a while... (mmap = false)
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: load_tensors:          CPU model buffer size =   260.86 MiB
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_seq_max     = 1
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx         = 8192
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq = 8192
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_batch       = 512
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ubatch      = 512
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_init_from_model: flash_attn    = 0
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_base     = 1000.0
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_scale    = 1
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_init_from_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU  output buffer size =     0.00 MiB
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU compute buffer size =    23.00 MiB
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph nodes  = 453
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph splits = 1
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:44:23.309Z level=INFO source=server.go:624 msg="llama runner started in 0.25 seconds"
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 12:44:23 | 200 |  333.235465ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:44:23.412Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.key_length default=128
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:44:23.412Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.value_length default=128
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:44:23.413Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="4.8 GiB" free_swap="0 B"
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:44:23.413Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.key_length default=128
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:44:23.413Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.value_length default=128
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:44:23.413Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=29 layers.offload=0 layers.split="" memory.available="[4.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="1.5 GiB" memory.required.partial="0 B" memory.required.kv="224.0 MiB" memory.required.allocations="[1.5 GiB]" memory.weights.total="976.1 MiB" memory.weights.repeating="793.5 MiB" memory.weights.nonrepeating="182.6 MiB" memory.graph.full="299.8 MiB" memory.graph.partial="482.3 MiB"
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc (version GGUF V3 (latest))
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = qwen2
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.type str              = model
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 1.5B
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                         general.size_label str              = 1.5B
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 1536
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 8960
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 12
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 2
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                          general.file_type u32              = 15
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ä  Ä ", "Ä Ä  Ä Ä ", "i n", "Ä  t",...
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  25:               general.quantization_version u32              = 2
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:  141 tensors
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q4_K:  169 tensors
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q6_K:   29 tensors
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: file type   = Q4_K - Medium
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: file size   = 1.04 GiB (5.00 BPW)
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 22
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.9310 MB
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: arch             = qwen2
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 1
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: model type       = ?B
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: model params     = 1.78 B
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: general.name     = DeepSeek R1 Distill Qwen 1.5B
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: vocab type       = BPE
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 151936
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 151387
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 151646 '<ï½beginâofâsentenceï½>'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: EOT token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: LF token         = 198 'Ä'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: FIM MID token    = 151660 '<|fim_middle|>'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: FIM PAD token    = 151662 '<|fim_pad|>'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: FIM REP token    = 151663 '<|repo_name|>'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: FIM SEP token    = 151664 '<|file_sep|>'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151662 '<|fim_pad|>'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151663 '<|repo_name|>'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151664 '<|file_sep|>'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: max token length = 256
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_load: vocab only - skipping tensors
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:44:23.589Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 4 --port 39257"
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:44:23.589Z level=INFO source=sched.go:450 msg="loaded runners" count=2
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:44:23.589Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:44:23.590Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:44:23.599Z level=INFO source=runner.go:931 msg="starting go runner"
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:44:23.640Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:44:23.641Z level=INFO source=runner.go:991 msg="Server listening on 127.0.0.1:39257"
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc (version GGUF V3 (latest))
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = qwen2
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.type str              = model
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 1.5B
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                         general.size_label str              = 1.5B
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 1536
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 8960
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 12
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 2
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                          general.file_type u32              = 15
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ä  Ä ", "Ä Ä  Ä Ä ", "i n", "Ä  t",...
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  25:               general.quantization_version u32              = 2
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:  141 tensors
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q4_K:  169 tensors
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q6_K:   29 tensors
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: file type   = Q4_K - Medium
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: file size   = 1.04 GiB (5.00 BPW)
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 22
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.9310 MB
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: arch             = qwen2
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 0
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: n_ctx_train      = 131072
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: n_embd           = 1536
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: n_layer          = 28
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: n_head           = 12
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: n_head_kv        = 2
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: n_rot            = 128
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: n_swa            = 0
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_k    = 128
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_v    = 128
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: n_gqa            = 6
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: n_embd_k_gqa     = 256
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: n_embd_v_gqa     = 256
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: f_norm_eps       = 0.0e+00
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: f_norm_rms_eps   = 1.0e-06
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: f_clamp_kqv      = 0.0e+00
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: f_max_alibi_bias = 0.0e+00
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: f_logit_scale    = 0.0e+00
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: n_ff             = 8960
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: n_expert         = 0
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: n_expert_used    = 0
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: causal attn      = 1
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: pooling type     = 0
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: rope type        = 2
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: rope scaling     = linear
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: freq_base_train  = 10000.0
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: freq_scale_train = 1
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: n_ctx_orig_yarn  = 131072
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: rope_finetuned   = unknown
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: ssm_d_conv       = 0
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: ssm_d_inner      = 0
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: ssm_d_state      = 0
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_rank      = 0
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_b_c_rms   = 0
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: model type       = 1.5B
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: model params     = 1.78 B
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: general.name     = DeepSeek R1 Distill Qwen 1.5B
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: vocab type       = BPE
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 151936
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 151387
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 151646 '<ï½beginâofâsentenceï½>'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: EOT token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: LF token         = 198 'Ä'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: FIM MID token    = 151660 '<|fim_middle|>'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: FIM PAD token    = 151662 '<|fim_pad|>'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: FIM REP token    = 151663 '<|repo_name|>'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: FIM SEP token    = 151664 '<|file_sep|>'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151662 '<|fim_pad|>'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151663 '<|repo_name|>'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151664 '<|file_sep|>'
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: print_info: max token length = 256
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: load_tensors: loading model tensors, this can take a while... (mmap = false)
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:44:23.841Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server loading model"
Mar 13 12:44:23 ubuntu-abhishek ollama[471085]: load_tensors:          CPU model buffer size =  1059.89 MiB
Mar 13 12:44:24 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_seq_max     = 4
Mar 13 12:44:24 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx         = 8192
Mar 13 12:44:24 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq = 2048
Mar 13 12:44:24 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_batch       = 2048
Mar 13 12:44:24 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ubatch      = 512
Mar 13 12:44:24 ubuntu-abhishek ollama[471085]: llama_init_from_model: flash_attn    = 0
Mar 13 12:44:24 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_base     = 10000.0
Mar 13 12:44:24 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_scale    = 1
Mar 13 12:44:24 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
Mar 13 12:44:24 ubuntu-abhishek ollama[471085]: llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1
Mar 13 12:44:24 ubuntu-abhishek ollama[471085]: llama_kv_cache_init:        CPU KV buffer size =   224.00 MiB
Mar 13 12:44:24 ubuntu-abhishek ollama[471085]: llama_init_from_model: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
Mar 13 12:44:24 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU  output buffer size =     2.34 MiB
Mar 13 12:44:24 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU compute buffer size =   302.75 MiB
Mar 13 12:44:24 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph nodes  = 986
Mar 13 12:44:24 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph splits = 1
Mar 13 12:44:24 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:44:24.093Z level=INFO source=server.go:624 msg="llama runner started in 0.50 seconds"
Mar 13 12:45:31 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 12:45:31 | 200 |          1m8s |       127.0.0.1 | POST     "/api/generate"
Mar 13 12:47:48 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 12:47:48 | 200 |   38.569457ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 12:47:49 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:47:49.060Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="3.5 GiB" free_swap="0 B"
Mar 13 12:47:49 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:47:49.060Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=27 layers.offload=0 layers.split="" memory.available="[3.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="1.5 GiB" memory.required.partial="0 B" memory.required.kv="208.0 MiB" memory.required.allocations="[1.5 GiB]" memory.weights.total="664.5 MiB" memory.weights.repeating="358.5 MiB" memory.weights.nonrepeating="306.0 MiB" memory.graph.full="514.2 MiB" memory.graph.partial="750.5 MiB"
Mar 13 12:47:49 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:47:49.166Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 12:47:49 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:47:49.169Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
Mar 13 12:47:49 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:47:49.173Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 12:47:49 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:47:49.173Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 12:47:49 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:47:49.173Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.num_channels default=0
Mar 13 12:47:49 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:47:49.173Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.block_count default=0
Mar 13 12:47:49 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:47:49.173Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.embedding_length default=0
Mar 13 12:47:49 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:47:49.173Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.head_count default=0
Mar 13 12:47:49 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:47:49.173Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 12:47:49 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:47:49.173Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 12:47:49 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:47:49.173Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.layer_norm_epsilon default=0
Mar 13 12:47:49 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:47:49.173Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 12:47:49 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:47:49.179Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.freq_scale default=1
Mar 13 12:47:49 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:47:49.179Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.mm_tokens_per_image default=256
Mar 13 12:47:49 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:47:49.179Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --ollama-engine --model /usr/share/ollama/.ollama/models/blobs/sha256-dbe81da1e4bad3cc6ccb77540915ffe53e7a3ac0745ffa5e9d4626d39c15e09a --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 4 --port 35183"
Mar 13 12:47:49 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:47:49.180Z level=INFO source=sched.go:450 msg="loaded runners" count=3
Mar 13 12:47:49 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:47:49.180Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 12:47:49 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:47:49.180Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 12:47:49 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:47:49.192Z level=INFO source=runner.go:882 msg="starting ollama engine"
Mar 13 12:47:49 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:47:49.192Z level=INFO source=runner.go:938 msg="Server listening on 127.0.0.1:35183"
Mar 13 12:47:49 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:47:49.293Z level=WARN source=ggml.go:149 msg="key not found" key=general.name default=""
Mar 13 12:47:49 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:47:49.293Z level=WARN source=ggml.go:149 msg="key not found" key=general.description default=""
Mar 13 12:47:49 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:47:49.294Z level=INFO source=ggml.go:67 msg="" architecture=gemma3 file_type=Q4_K_M name="" description="" num_tensors=340 num_key_values=32
Mar 13 12:47:49 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 12:47:49 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:47:49.336Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 12:47:49 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:47:49.338Z level=INFO source=ggml.go:289 msg="model weights" buffer=CPU size="1.0 GiB"
Mar 13 12:47:49 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:47:49.447Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server loading model"
Mar 13 12:47:55 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:47:55.649Z level=INFO source=ggml.go:356 msg="compute graph" backend=CPU buffer_type=CPU
Mar 13 12:47:55 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:47:55.652Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 12:47:55 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:47:55.657Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
Mar 13 12:47:55 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:47:55.662Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 12:47:55 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:47:55.662Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 12:47:55 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:47:55.662Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.num_channels default=0
Mar 13 12:47:55 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:47:55.662Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.block_count default=0
Mar 13 12:47:55 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:47:55.662Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.embedding_length default=0
Mar 13 12:47:55 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:47:55.663Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.head_count default=0
Mar 13 12:47:55 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:47:55.663Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 12:47:55 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:47:55.663Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 12:47:55 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:47:55.663Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.layer_norm_epsilon default=0
Mar 13 12:47:55 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:47:55.663Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 12:47:55 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:47:55.671Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.freq_scale default=1
Mar 13 12:47:55 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:47:55.671Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.mm_tokens_per_image default=256
Mar 13 12:47:55 ubuntu-abhishek ollama[471085]: time=2025-03-13T12:47:55.721Z level=INFO source=server.go:624 msg="llama runner started in 6.54 seconds"
Mar 13 12:48:23 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 12:48:23 | 200 | 34.315625575s |       127.0.0.1 | POST     "/api/generate"
Mar 13 12:48:59 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 12:48:59 | 200 |   34.497687ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 12:50:34 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 12:50:34 | 200 |         1m34s |       127.0.0.1 | POST     "/api/generate"
Mar 13 12:52:05 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 12:52:05 | 200 |   22.757713ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 12:53:34 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 12:53:34 | 200 |         1m28s |       127.0.0.1 | POST     "/api/generate"
Mar 13 12:54:16 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 12:54:16 | 200 |   33.167379ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 12:55:46 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 12:55:46 | 200 |         1m30s |       127.0.0.1 | POST     "/api/generate"
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:03.174Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="5.4 GiB" free_swap="0 B"
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:03.174Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:03.175Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.key_length default=64
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:03.175Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.value_length default=64
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:03.175Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:03.175Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=13 layers.offload=0 layers.split="" memory.available="[5.4 GiB]" memory.gpu_overhead="0 B" memory.required.full="352.9 MiB" memory.required.partial="0 B" memory.required.kv="24.0 MiB" memory.required.allocations="[352.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 1
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: model type       = ?B
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_load: vocab only - skipping tensors
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:03.214Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 1 --port 43107"
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:03.215Z level=INFO source=sched.go:450 msg="loaded runners" count=1
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:03.215Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:03.216Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:03.252Z level=INFO source=runner.go:931 msg="starting go runner"
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:03.304Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:03.306Z level=INFO source=runner.go:991 msg="Server listening on 127.0.0.1:43107"
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 0
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: n_ctx_train      = 2048
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: n_embd           = 768
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: n_layer          = 12
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: n_head           = 12
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: n_head_kv        = 12
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: n_rot            = 64
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: n_swa            = 0
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_k    = 64
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_v    = 64
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: n_gqa            = 1
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: n_embd_k_gqa     = 768
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: n_embd_v_gqa     = 768
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: f_norm_eps       = 1.0e-12
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: f_norm_rms_eps   = 0.0e+00
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: f_clamp_kqv      = 0.0e+00
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: f_max_alibi_bias = 0.0e+00
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: f_logit_scale    = 0.0e+00
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: n_ff             = 3072
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: n_expert         = 0
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: n_expert_used    = 0
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: causal attn      = 0
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: pooling type     = 1
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: rope type        = 2
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: rope scaling     = linear
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: freq_base_train  = 1000.0
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: freq_scale_train = 1
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: n_ctx_orig_yarn  = 2048
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: rope_finetuned   = unknown
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: ssm_d_conv       = 0
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: ssm_d_inner      = 0
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: ssm_d_state      = 0
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_rank      = 0
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_b_c_rms   = 0
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: model type       = 137M
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: load_tensors: loading model tensors, this can take a while... (mmap = false)
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: load_tensors:          CPU model buffer size =   260.86 MiB
Mar 13 13:03:03 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:03.481Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server loading model"
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_seq_max     = 1
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx         = 8192
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq = 8192
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_batch       = 512
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ubatch      = 512
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: llama_init_from_model: flash_attn    = 0
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_base     = 1000.0
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_scale    = 1
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: llama_init_from_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU  output buffer size =     0.00 MiB
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU compute buffer size =    23.00 MiB
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph nodes  = 453
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph splits = 1
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:05.487Z level=INFO source=server.go:624 msg="llama runner started in 2.27 seconds"
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 13:03:05 | 200 |  2.378519419s |       127.0.0.1 | POST     "/api/embed"
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:05.695Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="4.9 GiB" free_swap="0 B"
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:05.695Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=27 layers.offload=0 layers.split="" memory.available="[4.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="1.5 GiB" memory.required.partial="0 B" memory.required.kv="208.0 MiB" memory.required.allocations="[1.5 GiB]" memory.weights.total="664.5 MiB" memory.weights.repeating="358.5 MiB" memory.weights.nonrepeating="306.0 MiB" memory.graph.full="514.2 MiB" memory.graph.partial="750.5 MiB"
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:05.814Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:05.817Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:05.821Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:05.821Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:05.821Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.num_channels default=0
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:05.821Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.block_count default=0
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:05.821Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.embedding_length default=0
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:05.821Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.head_count default=0
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:05.821Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:05.821Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:05.821Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.layer_norm_epsilon default=0
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:05.821Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:05.829Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.freq_scale default=1
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:05.829Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.mm_tokens_per_image default=256
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:05.829Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --ollama-engine --model /usr/share/ollama/.ollama/models/blobs/sha256-dbe81da1e4bad3cc6ccb77540915ffe53e7a3ac0745ffa5e9d4626d39c15e09a --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 4 --port 45351"
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:05.829Z level=INFO source=sched.go:450 msg="loaded runners" count=2
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:05.829Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:05.829Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:05.842Z level=INFO source=runner.go:882 msg="starting ollama engine"
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:05.843Z level=INFO source=runner.go:938 msg="Server listening on 127.0.0.1:45351"
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:05.947Z level=WARN source=ggml.go:149 msg="key not found" key=general.name default=""
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:05.947Z level=WARN source=ggml.go:149 msg="key not found" key=general.description default=""
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:05.947Z level=INFO source=ggml.go:67 msg="" architecture=gemma3 file_type=Q4_K_M name="" description="" num_tensors=340 num_key_values=32
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:05.988Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 13:03:05 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:05.989Z level=INFO source=ggml.go:289 msg="model weights" buffer=CPU size="1.0 GiB"
Mar 13 13:03:06 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:06.081Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server loading model"
Mar 13 13:03:10 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:10.742Z level=INFO source=ggml.go:356 msg="compute graph" backend=CPU buffer_type=CPU
Mar 13 13:03:10 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:10.742Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 13:03:10 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:10.744Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
Mar 13 13:03:10 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:10.748Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 13:03:10 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:10.748Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 13:03:10 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:10.748Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.num_channels default=0
Mar 13 13:03:10 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:10.748Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.block_count default=0
Mar 13 13:03:10 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:10.748Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.embedding_length default=0
Mar 13 13:03:10 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:10.748Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.head_count default=0
Mar 13 13:03:10 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:10.748Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 13 13:03:10 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:10.748Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 13 13:03:10 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:10.748Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.layer_norm_epsilon default=0
Mar 13 13:03:10 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:10.748Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 13 13:03:10 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:10.754Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.freq_scale default=1
Mar 13 13:03:10 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:10.754Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.mm_tokens_per_image default=256
Mar 13 13:03:10 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:03:10.848Z level=INFO source=server.go:624 msg="llama runner started in 5.02 seconds"
Mar 13 13:04:17 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 13:04:17 | 200 |         1m12s |       127.0.0.1 | POST     "/api/generate"
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:10:28.101Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="5.4 GiB" free_swap="0 B"
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:10:28.101Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:10:28.101Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.key_length default=64
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:10:28.101Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.value_length default=64
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:10:28.101Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:10:28.101Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=13 layers.offload=0 layers.split="" memory.available="[5.4 GiB]" memory.gpu_overhead="0 B" memory.required.full="352.9 MiB" memory.required.partial="0 B" memory.required.kv="24.0 MiB" memory.required.allocations="[352.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 1
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: model type       = ?B
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_load: vocab only - skipping tensors
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:10:28.135Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 1 --port 42403"
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:10:28.135Z level=INFO source=sched.go:450 msg="loaded runners" count=1
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:10:28.135Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:10:28.135Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:10:28.147Z level=INFO source=runner.go:931 msg="starting go runner"
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:10:28.188Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:10:28.189Z level=INFO source=runner.go:991 msg="Server listening on 127.0.0.1:42403"
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 0
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: n_ctx_train      = 2048
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: n_embd           = 768
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: n_layer          = 12
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: n_head           = 12
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: n_head_kv        = 12
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: n_rot            = 64
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: n_swa            = 0
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_k    = 64
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_v    = 64
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: n_gqa            = 1
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: n_embd_k_gqa     = 768
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: n_embd_v_gqa     = 768
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: f_norm_eps       = 1.0e-12
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: f_norm_rms_eps   = 0.0e+00
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: f_clamp_kqv      = 0.0e+00
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: f_max_alibi_bias = 0.0e+00
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: f_logit_scale    = 0.0e+00
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: n_ff             = 3072
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: n_expert         = 0
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: n_expert_used    = 0
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: causal attn      = 0
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: pooling type     = 1
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: rope type        = 2
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: rope scaling     = linear
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: freq_base_train  = 1000.0
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: freq_scale_train = 1
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: n_ctx_orig_yarn  = 2048
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: rope_finetuned   = unknown
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: ssm_d_conv       = 0
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: ssm_d_inner      = 0
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: ssm_d_state      = 0
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_rank      = 0
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_b_c_rms   = 0
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: model type       = 137M
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: load_tensors: loading model tensors, this can take a while... (mmap = false)
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: load_tensors:          CPU model buffer size =   260.86 MiB
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_seq_max     = 1
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx         = 8192
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq = 8192
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_batch       = 512
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ubatch      = 512
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_init_from_model: flash_attn    = 0
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_base     = 1000.0
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_scale    = 1
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_init_from_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU  output buffer size =     0.00 MiB
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU compute buffer size =    23.00 MiB
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph nodes  = 453
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph splits = 1
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:10:28.386Z level=INFO source=server.go:624 msg="llama runner started in 0.25 seconds"
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 13:10:28 | 200 |   315.75854ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:10:28.485Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.key_length default=128
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:10:28.485Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.value_length default=128
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:10:28.485Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="4.8 GiB" free_swap="0 B"
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:10:28.485Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.key_length default=128
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:10:28.485Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.value_length default=128
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:10:28.486Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=29 layers.offload=0 layers.split="" memory.available="[4.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="1.5 GiB" memory.required.partial="0 B" memory.required.kv="224.0 MiB" memory.required.allocations="[1.5 GiB]" memory.weights.total="976.1 MiB" memory.weights.repeating="793.5 MiB" memory.weights.nonrepeating="182.6 MiB" memory.graph.full="299.8 MiB" memory.graph.partial="482.3 MiB"
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc (version GGUF V3 (latest))
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = qwen2
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.type str              = model
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 1.5B
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                         general.size_label str              = 1.5B
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 1536
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 8960
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 12
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 2
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                          general.file_type u32              = 15
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ä  Ä ", "Ä Ä  Ä Ä ", "i n", "Ä  t",...
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  25:               general.quantization_version u32              = 2
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:  141 tensors
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q4_K:  169 tensors
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q6_K:   29 tensors
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: file type   = Q4_K - Medium
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: file size   = 1.04 GiB (5.00 BPW)
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 22
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.9310 MB
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: arch             = qwen2
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 1
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: model type       = ?B
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: model params     = 1.78 B
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: general.name     = DeepSeek R1 Distill Qwen 1.5B
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: vocab type       = BPE
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 151936
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 151387
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 151646 '<ï½beginâofâsentenceï½>'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: EOT token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: LF token         = 198 'Ä'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: FIM MID token    = 151660 '<|fim_middle|>'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: FIM PAD token    = 151662 '<|fim_pad|>'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: FIM REP token    = 151663 '<|repo_name|>'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: FIM SEP token    = 151664 '<|file_sep|>'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151662 '<|fim_pad|>'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151663 '<|repo_name|>'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151664 '<|file_sep|>'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: max token length = 256
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_load: vocab only - skipping tensors
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:10:28.659Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 4 --port 40641"
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:10:28.659Z level=INFO source=sched.go:450 msg="loaded runners" count=2
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:10:28.660Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:10:28.660Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:10:28.670Z level=INFO source=runner.go:931 msg="starting go runner"
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:10:28.674Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:10:28.675Z level=INFO source=runner.go:991 msg="Server listening on 127.0.0.1:40641"
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc (version GGUF V3 (latest))
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = qwen2
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.type str              = model
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 1.5B
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                         general.size_label str              = 1.5B
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 1536
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 8960
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 12
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 2
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                          general.file_type u32              = 15
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ä  Ä ", "Ä Ä  Ä Ä ", "i n", "Ä  t",...
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  25:               general.quantization_version u32              = 2
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:  141 tensors
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q4_K:  169 tensors
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q6_K:   29 tensors
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: file type   = Q4_K - Medium
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: file size   = 1.04 GiB (5.00 BPW)
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 22
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.9310 MB
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: arch             = qwen2
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 0
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: n_ctx_train      = 131072
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: n_embd           = 1536
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: n_layer          = 28
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: n_head           = 12
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: n_head_kv        = 2
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: n_rot            = 128
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: n_swa            = 0
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_k    = 128
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_v    = 128
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: n_gqa            = 6
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: n_embd_k_gqa     = 256
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: n_embd_v_gqa     = 256
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: f_norm_eps       = 0.0e+00
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: f_norm_rms_eps   = 1.0e-06
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: f_clamp_kqv      = 0.0e+00
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: f_max_alibi_bias = 0.0e+00
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: f_logit_scale    = 0.0e+00
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: n_ff             = 8960
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: n_expert         = 0
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: n_expert_used    = 0
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: causal attn      = 1
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: pooling type     = 0
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: rope type        = 2
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: rope scaling     = linear
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: freq_base_train  = 10000.0
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: freq_scale_train = 1
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: n_ctx_orig_yarn  = 131072
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: rope_finetuned   = unknown
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: ssm_d_conv       = 0
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: ssm_d_inner      = 0
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: ssm_d_state      = 0
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_rank      = 0
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_b_c_rms   = 0
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: model type       = 1.5B
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: model params     = 1.78 B
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: general.name     = DeepSeek R1 Distill Qwen 1.5B
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: vocab type       = BPE
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 151936
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 151387
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 151646 '<ï½beginâofâsentenceï½>'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: EOT token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: LF token         = 198 'Ä'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: FIM MID token    = 151660 '<|fim_middle|>'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: FIM PAD token    = 151662 '<|fim_pad|>'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: FIM REP token    = 151663 '<|repo_name|>'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: FIM SEP token    = 151664 '<|file_sep|>'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151662 '<|fim_pad|>'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151663 '<|repo_name|>'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151664 '<|file_sep|>'
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: print_info: max token length = 256
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: load_tensors: loading model tensors, this can take a while... (mmap = false)
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: load_tensors:          CPU model buffer size =  1059.89 MiB
Mar 13 13:10:28 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:10:28.911Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server loading model"
Mar 13 13:10:37 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_seq_max     = 4
Mar 13 13:10:37 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx         = 8192
Mar 13 13:10:37 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq = 2048
Mar 13 13:10:37 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_batch       = 2048
Mar 13 13:10:37 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ubatch      = 512
Mar 13 13:10:37 ubuntu-abhishek ollama[471085]: llama_init_from_model: flash_attn    = 0
Mar 13 13:10:37 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_base     = 10000.0
Mar 13 13:10:37 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_scale    = 1
Mar 13 13:10:37 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
Mar 13 13:10:37 ubuntu-abhishek ollama[471085]: llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1
Mar 13 13:10:37 ubuntu-abhishek ollama[471085]: llama_kv_cache_init:        CPU KV buffer size =   224.00 MiB
Mar 13 13:10:37 ubuntu-abhishek ollama[471085]: llama_init_from_model: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
Mar 13 13:10:37 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU  output buffer size =     2.34 MiB
Mar 13 13:10:37 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU compute buffer size =   302.75 MiB
Mar 13 13:10:37 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph nodes  = 986
Mar 13 13:10:37 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph splits = 1
Mar 13 13:10:37 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:10:37.440Z level=INFO source=server.go:624 msg="llama runner started in 8.78 seconds"
Mar 13 13:12:04 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 13:12:04 | 200 |         1m35s |       127.0.0.1 | POST     "/api/generate"
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:22:10.276Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="5.4 GiB" free_swap="0 B"
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:22:10.276Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:22:10.276Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.key_length default=64
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:22:10.276Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.value_length default=64
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:22:10.276Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:22:10.276Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=13 layers.offload=0 layers.split="" memory.available="[5.4 GiB]" memory.gpu_overhead="0 B" memory.required.full="352.9 MiB" memory.required.partial="0 B" memory.required.kv="24.0 MiB" memory.required.allocations="[352.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 1
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: model type       = ?B
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_load: vocab only - skipping tensors
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:22:10.308Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 1 --port 33777"
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:22:10.309Z level=INFO source=sched.go:450 msg="loaded runners" count=1
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:22:10.309Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:22:10.309Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:22:10.319Z level=INFO source=runner.go:931 msg="starting go runner"
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:22:10.364Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:22:10.365Z level=INFO source=runner.go:991 msg="Server listening on 127.0.0.1:33777"
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 0
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: n_ctx_train      = 2048
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: n_embd           = 768
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: n_layer          = 12
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: n_head           = 12
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: n_head_kv        = 12
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: n_rot            = 64
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: n_swa            = 0
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_k    = 64
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_v    = 64
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: n_gqa            = 1
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: n_embd_k_gqa     = 768
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: n_embd_v_gqa     = 768
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: f_norm_eps       = 1.0e-12
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: f_norm_rms_eps   = 0.0e+00
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: f_clamp_kqv      = 0.0e+00
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: f_max_alibi_bias = 0.0e+00
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: f_logit_scale    = 0.0e+00
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: n_ff             = 3072
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: n_expert         = 0
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: n_expert_used    = 0
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: causal attn      = 0
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: pooling type     = 1
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: rope type        = 2
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: rope scaling     = linear
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: freq_base_train  = 1000.0
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: freq_scale_train = 1
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: n_ctx_orig_yarn  = 2048
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: rope_finetuned   = unknown
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: ssm_d_conv       = 0
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: ssm_d_inner      = 0
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: ssm_d_state      = 0
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_rank      = 0
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_b_c_rms   = 0
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: model type       = 137M
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: load_tensors: loading model tensors, this can take a while... (mmap = false)
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: load_tensors:          CPU model buffer size =   260.86 MiB
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_seq_max     = 1
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx         = 8192
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq = 8192
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_batch       = 512
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ubatch      = 512
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_init_from_model: flash_attn    = 0
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_base     = 1000.0
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_scale    = 1
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_init_from_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU  output buffer size =     0.00 MiB
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU compute buffer size =    23.00 MiB
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph nodes  = 453
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph splits = 1
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:22:10.560Z level=INFO source=server.go:624 msg="llama runner started in 0.25 seconds"
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 13:22:10 | 200 |  348.743479ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:22:10.678Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.key_length default=128
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:22:10.678Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.value_length default=128
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:22:10.678Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="4.9 GiB" free_swap="0 B"
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:22:10.678Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.key_length default=128
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:22:10.678Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.value_length default=128
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:22:10.678Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=29 layers.offload=0 layers.split="" memory.available="[4.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="1.5 GiB" memory.required.partial="0 B" memory.required.kv="224.0 MiB" memory.required.allocations="[1.5 GiB]" memory.weights.total="976.1 MiB" memory.weights.repeating="793.5 MiB" memory.weights.nonrepeating="182.6 MiB" memory.graph.full="299.8 MiB" memory.graph.partial="482.3 MiB"
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc (version GGUF V3 (latest))
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = qwen2
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.type str              = model
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 1.5B
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                         general.size_label str              = 1.5B
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 1536
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 8960
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 12
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 2
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                          general.file_type u32              = 15
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ä  Ä ", "Ä Ä  Ä Ä ", "i n", "Ä  t",...
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  25:               general.quantization_version u32              = 2
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:  141 tensors
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q4_K:  169 tensors
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q6_K:   29 tensors
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: file type   = Q4_K - Medium
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: file size   = 1.04 GiB (5.00 BPW)
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 22
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.9310 MB
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: arch             = qwen2
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 1
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: model type       = ?B
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: model params     = 1.78 B
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: general.name     = DeepSeek R1 Distill Qwen 1.5B
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: vocab type       = BPE
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 151936
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 151387
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 151646 '<ï½beginâofâsentenceï½>'
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: EOT token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: LF token         = 198 'Ä'
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: FIM MID token    = 151660 '<|fim_middle|>'
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: FIM PAD token    = 151662 '<|fim_pad|>'
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: FIM REP token    = 151663 '<|repo_name|>'
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: FIM SEP token    = 151664 '<|file_sep|>'
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151662 '<|fim_pad|>'
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151663 '<|repo_name|>'
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151664 '<|file_sep|>'
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: max token length = 256
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_load: vocab only - skipping tensors
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:22:10.871Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 4 --port 33069"
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:22:10.872Z level=INFO source=sched.go:450 msg="loaded runners" count=2
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:22:10.872Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:22:10.872Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:22:10.883Z level=INFO source=runner.go:931 msg="starting go runner"
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:22:10.924Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:22:10.925Z level=INFO source=runner.go:991 msg="Server listening on 127.0.0.1:33069"
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc (version GGUF V3 (latest))
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = qwen2
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.type str              = model
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 1.5B
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                         general.size_label str              = 1.5B
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 1536
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 8960
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 12
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 2
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                          general.file_type u32              = 15
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ä  Ä ", "Ä Ä  Ä Ä ", "i n", "Ä  t",...
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  25:               general.quantization_version u32              = 2
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:  141 tensors
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q4_K:  169 tensors
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q6_K:   29 tensors
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: file type   = Q4_K - Medium
Mar 13 13:22:10 ubuntu-abhishek ollama[471085]: print_info: file size   = 1.04 GiB (5.00 BPW)
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 22
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.9310 MB
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: arch             = qwen2
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 0
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: n_ctx_train      = 131072
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: n_embd           = 1536
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: n_layer          = 28
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: n_head           = 12
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: n_head_kv        = 2
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: n_rot            = 128
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: n_swa            = 0
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_k    = 128
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_v    = 128
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: n_gqa            = 6
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: n_embd_k_gqa     = 256
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: n_embd_v_gqa     = 256
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: f_norm_eps       = 0.0e+00
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: f_norm_rms_eps   = 1.0e-06
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: f_clamp_kqv      = 0.0e+00
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: f_max_alibi_bias = 0.0e+00
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: f_logit_scale    = 0.0e+00
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: n_ff             = 8960
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: n_expert         = 0
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: n_expert_used    = 0
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: causal attn      = 1
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: pooling type     = 0
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: rope type        = 2
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: rope scaling     = linear
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: freq_base_train  = 10000.0
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: freq_scale_train = 1
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: n_ctx_orig_yarn  = 131072
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: rope_finetuned   = unknown
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: ssm_d_conv       = 0
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: ssm_d_inner      = 0
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: ssm_d_state      = 0
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_rank      = 0
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_b_c_rms   = 0
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: model type       = 1.5B
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: model params     = 1.78 B
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: general.name     = DeepSeek R1 Distill Qwen 1.5B
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: vocab type       = BPE
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 151936
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 151387
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 151646 '<ï½beginâofâsentenceï½>'
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: EOT token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: LF token         = 198 'Ä'
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: FIM MID token    = 151660 '<|fim_middle|>'
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: FIM PAD token    = 151662 '<|fim_pad|>'
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: FIM REP token    = 151663 '<|repo_name|>'
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: FIM SEP token    = 151664 '<|file_sep|>'
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151662 '<|fim_pad|>'
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151663 '<|repo_name|>'
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151664 '<|file_sep|>'
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: print_info: max token length = 256
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: load_tensors: loading model tensors, this can take a while... (mmap = false)
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: load_tensors:          CPU model buffer size =  1059.89 MiB
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:22:11.123Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server loading model"
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_seq_max     = 4
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx         = 8192
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq = 2048
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_batch       = 2048
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ubatch      = 512
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: llama_init_from_model: flash_attn    = 0
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_base     = 10000.0
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_scale    = 1
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: llama_kv_cache_init:        CPU KV buffer size =   224.00 MiB
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: llama_init_from_model: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU  output buffer size =     2.34 MiB
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU compute buffer size =   302.75 MiB
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph nodes  = 986
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph splits = 1
Mar 13 13:22:11 ubuntu-abhishek ollama[471085]: time=2025-03-13T13:22:11.626Z level=INFO source=server.go:624 msg="llama runner started in 0.75 seconds"
Mar 13 13:24:16 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 13:24:16 | 200 |          2m6s |       127.0.0.1 | POST     "/api/generate"
Mar 13 13:25:13 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 13:25:13 | 200 |   45.222612ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 13:27:17 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 13:27:17 | 200 |          2m4s |       127.0.0.1 | POST     "/api/generate"
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:10:58.719Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="5.9 GiB" free_swap="0 B"
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:10:58.719Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:10:58.719Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.key_length default=64
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:10:58.719Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.value_length default=64
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:10:58.719Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:10:58.719Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=13 layers.offload=0 layers.split="" memory.available="[5.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="352.9 MiB" memory.required.partial="0 B" memory.required.kv="24.0 MiB" memory.required.allocations="[352.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 1
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: model type       = ?B
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_load: vocab only - skipping tensors
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:10:58.752Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 1 --port 36487"
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:10:58.752Z level=INFO source=sched.go:450 msg="loaded runners" count=1
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:10:58.752Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:10:58.752Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:10:58.764Z level=INFO source=runner.go:931 msg="starting go runner"
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:10:58.804Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:10:58.805Z level=INFO source=runner.go:991 msg="Server listening on 127.0.0.1:36487"
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 0
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: n_ctx_train      = 2048
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: n_embd           = 768
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: n_layer          = 12
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: n_head           = 12
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: n_head_kv        = 12
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: n_rot            = 64
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: n_swa            = 0
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_k    = 64
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_v    = 64
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: n_gqa            = 1
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: n_embd_k_gqa     = 768
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: n_embd_v_gqa     = 768
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: f_norm_eps       = 1.0e-12
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: f_norm_rms_eps   = 0.0e+00
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: f_clamp_kqv      = 0.0e+00
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: f_max_alibi_bias = 0.0e+00
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: f_logit_scale    = 0.0e+00
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: n_ff             = 3072
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: n_expert         = 0
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: n_expert_used    = 0
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: causal attn      = 0
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: pooling type     = 1
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: rope type        = 2
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: rope scaling     = linear
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: freq_base_train  = 1000.0
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: freq_scale_train = 1
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: n_ctx_orig_yarn  = 2048
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: rope_finetuned   = unknown
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: ssm_d_conv       = 0
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: ssm_d_inner      = 0
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: ssm_d_state      = 0
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_rank      = 0
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_b_c_rms   = 0
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: model type       = 137M
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: load_tensors: loading model tensors, this can take a while... (mmap = false)
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: load_tensors:          CPU model buffer size =   260.86 MiB
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_seq_max     = 1
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx         = 8192
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq = 8192
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_batch       = 512
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ubatch      = 512
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_init_from_model: flash_attn    = 0
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_base     = 1000.0
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_scale    = 1
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_init_from_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU  output buffer size =     0.00 MiB
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU compute buffer size =    23.00 MiB
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph nodes  = 453
Mar 13 14:10:58 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph splits = 1
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:10:59.003Z level=INFO source=server.go:624 msg="llama runner started in 0.25 seconds"
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 14:10:59 | 200 |  317.549132ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:10:59.093Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.key_length default=128
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:10:59.093Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.value_length default=128
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:10:59.093Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="5.3 GiB" free_swap="0 B"
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:10:59.094Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.key_length default=128
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:10:59.094Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.value_length default=128
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:10:59.094Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=29 layers.offload=0 layers.split="" memory.available="[5.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="1.5 GiB" memory.required.partial="0 B" memory.required.kv="224.0 MiB" memory.required.allocations="[1.5 GiB]" memory.weights.total="976.1 MiB" memory.weights.repeating="793.5 MiB" memory.weights.nonrepeating="182.6 MiB" memory.graph.full="299.8 MiB" memory.graph.partial="482.3 MiB"
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc (version GGUF V3 (latest))
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = qwen2
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.type str              = model
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 1.5B
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                         general.size_label str              = 1.5B
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 1536
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 8960
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 12
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 2
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                          general.file_type u32              = 15
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ä  Ä ", "Ä Ä  Ä Ä ", "i n", "Ä  t",...
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  25:               general.quantization_version u32              = 2
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:  141 tensors
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q4_K:  169 tensors
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q6_K:   29 tensors
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: file type   = Q4_K - Medium
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: file size   = 1.04 GiB (5.00 BPW)
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 22
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.9310 MB
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: arch             = qwen2
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 1
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: model type       = ?B
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: model params     = 1.78 B
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: general.name     = DeepSeek R1 Distill Qwen 1.5B
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: vocab type       = BPE
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 151936
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 151387
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 151646 '<ï½beginâofâsentenceï½>'
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: EOT token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: LF token         = 198 'Ä'
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: FIM MID token    = 151660 '<|fim_middle|>'
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: FIM PAD token    = 151662 '<|fim_pad|>'
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: FIM REP token    = 151663 '<|repo_name|>'
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: FIM SEP token    = 151664 '<|file_sep|>'
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151662 '<|fim_pad|>'
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151663 '<|repo_name|>'
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151664 '<|file_sep|>'
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: max token length = 256
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_load: vocab only - skipping tensors
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:10:59.269Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 4 --port 34543"
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:10:59.269Z level=INFO source=sched.go:450 msg="loaded runners" count=2
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:10:59.269Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:10:59.269Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:10:59.280Z level=INFO source=runner.go:931 msg="starting go runner"
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:10:59.285Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:10:59.285Z level=INFO source=runner.go:991 msg="Server listening on 127.0.0.1:34543"
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc (version GGUF V3 (latest))
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = qwen2
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.type str              = model
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 1.5B
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                         general.size_label str              = 1.5B
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 1536
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 8960
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 12
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 2
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                          general.file_type u32              = 15
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ä  Ä ", "Ä Ä  Ä Ä ", "i n", "Ä  t",...
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  25:               general.quantization_version u32              = 2
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:  141 tensors
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q4_K:  169 tensors
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q6_K:   29 tensors
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: file type   = Q4_K - Medium
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: file size   = 1.04 GiB (5.00 BPW)
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 22
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.9310 MB
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: arch             = qwen2
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 0
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: n_ctx_train      = 131072
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: n_embd           = 1536
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: n_layer          = 28
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: n_head           = 12
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: n_head_kv        = 2
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: n_rot            = 128
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: n_swa            = 0
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_k    = 128
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_v    = 128
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: n_gqa            = 6
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: n_embd_k_gqa     = 256
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: n_embd_v_gqa     = 256
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: f_norm_eps       = 0.0e+00
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: f_norm_rms_eps   = 1.0e-06
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: f_clamp_kqv      = 0.0e+00
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: f_max_alibi_bias = 0.0e+00
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: f_logit_scale    = 0.0e+00
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: n_ff             = 8960
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: n_expert         = 0
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: n_expert_used    = 0
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: causal attn      = 1
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: pooling type     = 0
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: rope type        = 2
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: rope scaling     = linear
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: freq_base_train  = 10000.0
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: freq_scale_train = 1
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: n_ctx_orig_yarn  = 131072
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: rope_finetuned   = unknown
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: ssm_d_conv       = 0
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: ssm_d_inner      = 0
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: ssm_d_state      = 0
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_rank      = 0
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_b_c_rms   = 0
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: model type       = 1.5B
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: model params     = 1.78 B
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: general.name     = DeepSeek R1 Distill Qwen 1.5B
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: vocab type       = BPE
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 151936
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 151387
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 151646 '<ï½beginâofâsentenceï½>'
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: EOT token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: LF token         = 198 'Ä'
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: FIM MID token    = 151660 '<|fim_middle|>'
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: FIM PAD token    = 151662 '<|fim_pad|>'
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: FIM REP token    = 151663 '<|repo_name|>'
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: FIM SEP token    = 151664 '<|file_sep|>'
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151643 '<ï½endâofâsentenceï½>'
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151662 '<|fim_pad|>'
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151663 '<|repo_name|>'
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151664 '<|file_sep|>'
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: print_info: max token length = 256
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: load_tensors: loading model tensors, this can take a while... (mmap = false)
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: load_tensors:          CPU model buffer size =  1059.89 MiB
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:10:59.520Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server loading model"
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_seq_max     = 4
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx         = 8192
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq = 2048
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_batch       = 2048
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ubatch      = 512
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_init_from_model: flash_attn    = 0
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_base     = 10000.0
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_scale    = 1
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_kv_cache_init:        CPU KV buffer size =   224.00 MiB
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_init_from_model: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU  output buffer size =     2.34 MiB
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU compute buffer size =   302.75 MiB
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph nodes  = 986
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph splits = 1
Mar 13 14:10:59 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:10:59.771Z level=INFO source=server.go:624 msg="llama runner started in 0.50 seconds"
Mar 13 14:12:42 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 14:12:42 | 200 |         1m43s |       127.0.0.1 | POST     "/api/generate"
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:16:45.830Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:16:45.830Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.key_length default=64
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:16:45.830Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.value_length default=64
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:16:45.830Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:16:45.830Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="4.3 GiB" free_swap="0 B"
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:16:45.830Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:16:45.830Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.key_length default=64
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:16:45.830Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.value_length default=64
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:16:45.831Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:16:45.831Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=13 layers.offload=0 layers.split="" memory.available="[4.3 GiB]" memory.gpu_overhead="0 B" memory.required.full="352.9 MiB" memory.required.partial="0 B" memory.required.kv="24.0 MiB" memory.required.allocations="[352.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 1
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: model type       = ?B
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_load: vocab only - skipping tensors
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:16:45.863Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 1 --port 38191"
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:16:45.863Z level=INFO source=sched.go:450 msg="loaded runners" count=2
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:16:45.863Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:16:45.864Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:16:45.874Z level=INFO source=runner.go:931 msg="starting go runner"
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:16:45.916Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:16:45.917Z level=INFO source=runner.go:991 msg="Server listening on 127.0.0.1:38191"
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 0
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: n_ctx_train      = 2048
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: n_embd           = 768
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: n_layer          = 12
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: n_head           = 12
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: n_head_kv        = 12
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: n_rot            = 64
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: n_swa            = 0
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_k    = 64
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_v    = 64
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: n_gqa            = 1
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: n_embd_k_gqa     = 768
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: n_embd_v_gqa     = 768
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: f_norm_eps       = 1.0e-12
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: f_norm_rms_eps   = 0.0e+00
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: f_clamp_kqv      = 0.0e+00
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: f_max_alibi_bias = 0.0e+00
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: f_logit_scale    = 0.0e+00
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: n_ff             = 3072
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: n_expert         = 0
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: n_expert_used    = 0
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: causal attn      = 0
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: pooling type     = 1
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: rope type        = 2
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: rope scaling     = linear
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: freq_base_train  = 1000.0
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: freq_scale_train = 1
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: n_ctx_orig_yarn  = 2048
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: rope_finetuned   = unknown
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: ssm_d_conv       = 0
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: ssm_d_inner      = 0
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: ssm_d_state      = 0
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_rank      = 0
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_b_c_rms   = 0
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: model type       = 137M
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: load_tensors: loading model tensors, this can take a while... (mmap = false)
Mar 13 14:16:45 ubuntu-abhishek ollama[471085]: load_tensors:          CPU model buffer size =   260.86 MiB
Mar 13 14:16:46 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_seq_max     = 1
Mar 13 14:16:46 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx         = 8192
Mar 13 14:16:46 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq = 8192
Mar 13 14:16:46 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_batch       = 512
Mar 13 14:16:46 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ubatch      = 512
Mar 13 14:16:46 ubuntu-abhishek ollama[471085]: llama_init_from_model: flash_attn    = 0
Mar 13 14:16:46 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_base     = 1000.0
Mar 13 14:16:46 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_scale    = 1
Mar 13 14:16:46 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
Mar 13 14:16:46 ubuntu-abhishek ollama[471085]: llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
Mar 13 14:16:46 ubuntu-abhishek ollama[471085]: llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB
Mar 13 14:16:46 ubuntu-abhishek ollama[471085]: llama_init_from_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
Mar 13 14:16:46 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU  output buffer size =     0.00 MiB
Mar 13 14:16:46 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU compute buffer size =    23.00 MiB
Mar 13 14:16:46 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph nodes  = 453
Mar 13 14:16:46 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph splits = 1
Mar 13 14:16:46 ubuntu-abhishek ollama[471085]: time=2025-03-13T14:16:46.114Z level=INFO source=server.go:624 msg="llama runner started in 0.25 seconds"
Mar 13 14:16:46 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 14:16:46 | 200 |  321.701074ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 14:18:16 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 14:18:16 | 200 |         1m30s |       127.0.0.1 | POST     "/api/generate"
Mar 13 14:18:33 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 14:18:33 | 200 |   47.556142ms |       127.0.0.1 | POST     "/api/embed"
Mar 13 14:20:04 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/13 - 14:20:04 | 200 |         1m30s |       127.0.0.1 | POST     "/api/generate"
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:19:27.553Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="5.9 GiB" free_swap="0 B"
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:19:27.553Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:19:27.553Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.key_length default=64
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:19:27.553Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.value_length default=64
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:19:27.553Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:19:27.553Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=13 layers.offload=0 layers.split="" memory.available="[5.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="352.9 MiB" memory.required.partial="0 B" memory.required.kv="24.0 MiB" memory.required.allocations="[352.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 1
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: model type       = ?B
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_load: vocab only - skipping tensors
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:19:27.586Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 1 --port 36871"
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:19:27.586Z level=INFO source=sched.go:450 msg="loaded runners" count=1
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:19:27.586Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:19:27.586Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:19:27.599Z level=INFO source=runner.go:931 msg="starting go runner"
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:19:27.640Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:19:27.641Z level=INFO source=runner.go:991 msg="Server listening on 127.0.0.1:36871"
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 0
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: n_ctx_train      = 2048
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: n_embd           = 768
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: n_layer          = 12
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: n_head           = 12
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: n_head_kv        = 12
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: n_rot            = 64
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: n_swa            = 0
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_k    = 64
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_v    = 64
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: n_gqa            = 1
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: n_embd_k_gqa     = 768
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: n_embd_v_gqa     = 768
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: f_norm_eps       = 1.0e-12
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: f_norm_rms_eps   = 0.0e+00
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: f_clamp_kqv      = 0.0e+00
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: f_max_alibi_bias = 0.0e+00
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: f_logit_scale    = 0.0e+00
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: n_ff             = 3072
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: n_expert         = 0
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: n_expert_used    = 0
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: causal attn      = 0
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: pooling type     = 1
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: rope type        = 2
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: rope scaling     = linear
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: freq_base_train  = 1000.0
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: freq_scale_train = 1
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: n_ctx_orig_yarn  = 2048
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: rope_finetuned   = unknown
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: ssm_d_conv       = 0
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: ssm_d_inner      = 0
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: ssm_d_state      = 0
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_rank      = 0
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_b_c_rms   = 0
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: model type       = 137M
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: load_tensors: loading model tensors, this can take a while... (mmap = false)
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: load_tensors:          CPU model buffer size =   260.86 MiB
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_seq_max     = 1
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx         = 8192
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq = 8192
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_batch       = 512
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ubatch      = 512
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_init_from_model: flash_attn    = 0
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_base     = 1000.0
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_scale    = 1
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_init_from_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU  output buffer size =     0.00 MiB
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU compute buffer size =    23.00 MiB
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph nodes  = 453
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph splits = 1
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:19:27.837Z level=INFO source=server.go:624 msg="llama runner started in 0.25 seconds"
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/14 - 05:19:27 | 200 |  366.036752ms |       127.0.0.1 | POST     "/api/embed"
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:19:27.978Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.key_length default=128
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:19:27.978Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.value_length default=128
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:19:27.978Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="5.4 GiB" free_swap="0 B"
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:19:27.979Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.key_length default=128
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:19:27.979Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.value_length default=128
Mar 14 05:19:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:19:27.979Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=29 layers.offload=0 layers.split="" memory.available="[5.4 GiB]" memory.gpu_overhead="0 B" memory.required.full="1.5 GiB" memory.required.partial="0 B" memory.required.kv="224.0 MiB" memory.required.allocations="[1.5 GiB]" memory.weights.total="976.1 MiB" memory.weights.repeating="793.5 MiB" memory.weights.nonrepeating="182.6 MiB" memory.graph.full="299.8 MiB" memory.graph.partial="482.3 MiB"
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc (version GGUF V3 (latest))
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = qwen2
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.type str              = model
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 1.5B
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                         general.size_label str              = 1.5B
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 1536
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 8960
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 12
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 2
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                          general.file_type u32              = 15
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ä  Ä ", "Ä Ä  Ä Ä ", "i n", "Ä  t",...
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  25:               general.quantization_version u32              = 2
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:  141 tensors
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q4_K:  169 tensors
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q6_K:   29 tensors
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: file type   = Q4_K - Medium
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: file size   = 1.04 GiB (5.00 BPW)
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 22
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.9310 MB
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: arch             = qwen2
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 1
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: model type       = ?B
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: model params     = 1.78 B
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: general.name     = DeepSeek R1 Distill Qwen 1.5B
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: vocab type       = BPE
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 151936
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 151387
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 151646 '<ï½beginâofâsentenceï½>'
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 151643 '<ï½endâofâsentenceï½>'
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: EOT token        = 151643 '<ï½endâofâsentenceï½>'
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 151643 '<ï½endâofâsentenceï½>'
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: LF token         = 198 'Ä'
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: FIM MID token    = 151660 '<|fim_middle|>'
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: FIM PAD token    = 151662 '<|fim_pad|>'
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: FIM REP token    = 151663 '<|repo_name|>'
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: FIM SEP token    = 151664 '<|file_sep|>'
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151643 '<ï½endâofâsentenceï½>'
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151662 '<|fim_pad|>'
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151663 '<|repo_name|>'
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151664 '<|file_sep|>'
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: max token length = 256
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_load: vocab only - skipping tensors
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:19:28.154Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 4 --port 42487"
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:19:28.155Z level=INFO source=sched.go:450 msg="loaded runners" count=2
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:19:28.155Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:19:28.155Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:19:28.166Z level=INFO source=runner.go:931 msg="starting go runner"
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:19:28.192Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:19:28.193Z level=INFO source=runner.go:991 msg="Server listening on 127.0.0.1:42487"
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc (version GGUF V3 (latest))
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = qwen2
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.type str              = model
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 1.5B
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                         general.size_label str              = 1.5B
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 1536
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 8960
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 12
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 2
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                          general.file_type u32              = 15
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ä  Ä ", "Ä Ä  Ä Ä ", "i n", "Ä  t",...
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  25:               general.quantization_version u32              = 2
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:  141 tensors
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q4_K:  169 tensors
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q6_K:   29 tensors
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: file type   = Q4_K - Medium
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: file size   = 1.04 GiB (5.00 BPW)
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 22
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.9310 MB
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: arch             = qwen2
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 0
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: n_ctx_train      = 131072
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: n_embd           = 1536
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: n_layer          = 28
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: n_head           = 12
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: n_head_kv        = 2
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: n_rot            = 128
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: n_swa            = 0
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_k    = 128
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_v    = 128
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: n_gqa            = 6
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: n_embd_k_gqa     = 256
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: n_embd_v_gqa     = 256
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: f_norm_eps       = 0.0e+00
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: f_norm_rms_eps   = 1.0e-06
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: f_clamp_kqv      = 0.0e+00
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: f_max_alibi_bias = 0.0e+00
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: f_logit_scale    = 0.0e+00
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: n_ff             = 8960
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: n_expert         = 0
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: n_expert_used    = 0
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: causal attn      = 1
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: pooling type     = 0
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: rope type        = 2
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: rope scaling     = linear
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: freq_base_train  = 10000.0
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: freq_scale_train = 1
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: n_ctx_orig_yarn  = 131072
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: rope_finetuned   = unknown
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: ssm_d_conv       = 0
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: ssm_d_inner      = 0
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: ssm_d_state      = 0
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_rank      = 0
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_b_c_rms   = 0
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: model type       = 1.5B
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: model params     = 1.78 B
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: general.name     = DeepSeek R1 Distill Qwen 1.5B
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: vocab type       = BPE
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 151936
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 151387
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 151646 '<ï½beginâofâsentenceï½>'
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 151643 '<ï½endâofâsentenceï½>'
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: EOT token        = 151643 '<ï½endâofâsentenceï½>'
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 151643 '<ï½endâofâsentenceï½>'
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: LF token         = 198 'Ä'
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: FIM MID token    = 151660 '<|fim_middle|>'
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: FIM PAD token    = 151662 '<|fim_pad|>'
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: FIM REP token    = 151663 '<|repo_name|>'
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: FIM SEP token    = 151664 '<|file_sep|>'
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151643 '<ï½endâofâsentenceï½>'
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151662 '<|fim_pad|>'
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151663 '<|repo_name|>'
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151664 '<|file_sep|>'
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: print_info: max token length = 256
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: load_tensors: loading model tensors, this can take a while... (mmap = false)
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: load_tensors:          CPU model buffer size =  1059.89 MiB
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:19:28.406Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server loading model"
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_seq_max     = 4
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx         = 8192
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq = 2048
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_batch       = 2048
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ubatch      = 512
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_init_from_model: flash_attn    = 0
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_base     = 10000.0
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_scale    = 1
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_kv_cache_init:        CPU KV buffer size =   224.00 MiB
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_init_from_model: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU  output buffer size =     2.34 MiB
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU compute buffer size =   302.75 MiB
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph nodes  = 986
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph splits = 1
Mar 14 05:19:28 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:19:28.657Z level=INFO source=server.go:624 msg="llama runner started in 0.50 seconds"
Mar 14 05:21:13 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/14 - 05:21:13 | 200 |         1m45s |       127.0.0.1 | POST     "/api/generate"
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:31:09.953Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="5.5 GiB" free_swap="0 B"
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:31:09.953Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:31:09.953Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.key_length default=64
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:31:09.953Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.value_length default=64
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:31:09.953Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:31:09.953Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=13 layers.offload=0 layers.split="" memory.available="[5.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="352.9 MiB" memory.required.partial="0 B" memory.required.kv="24.0 MiB" memory.required.allocations="[352.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 1
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: print_info: model type       = ?B
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: llama_model_load: vocab only - skipping tensors
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:31:09.986Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 1 --port 36241"
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:31:09.986Z level=INFO source=sched.go:450 msg="loaded runners" count=1
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:31:09.986Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:31:09.986Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 14 05:31:09 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:31:09.998Z level=INFO source=runner.go:931 msg="starting go runner"
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:31:10.040Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:31:10.041Z level=INFO source=runner.go:991 msg="Server listening on 127.0.0.1:36241"
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 0
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: n_ctx_train      = 2048
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: n_embd           = 768
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: n_layer          = 12
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: n_head           = 12
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: n_head_kv        = 12
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: n_rot            = 64
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: n_swa            = 0
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_k    = 64
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_v    = 64
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: n_gqa            = 1
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: n_embd_k_gqa     = 768
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: n_embd_v_gqa     = 768
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: f_norm_eps       = 1.0e-12
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: f_norm_rms_eps   = 0.0e+00
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: f_clamp_kqv      = 0.0e+00
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: f_max_alibi_bias = 0.0e+00
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: f_logit_scale    = 0.0e+00
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: n_ff             = 3072
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: n_expert         = 0
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: n_expert_used    = 0
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: causal attn      = 0
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: pooling type     = 1
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: rope type        = 2
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: rope scaling     = linear
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: freq_base_train  = 1000.0
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: freq_scale_train = 1
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: n_ctx_orig_yarn  = 2048
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: rope_finetuned   = unknown
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: ssm_d_conv       = 0
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: ssm_d_inner      = 0
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: ssm_d_state      = 0
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_rank      = 0
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_b_c_rms   = 0
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: model type       = 137M
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: load_tensors: loading model tensors, this can take a while... (mmap = false)
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: load_tensors:          CPU model buffer size =   260.86 MiB
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_seq_max     = 1
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx         = 8192
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq = 8192
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_batch       = 512
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ubatch      = 512
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_init_from_model: flash_attn    = 0
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_base     = 1000.0
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_scale    = 1
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_init_from_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU  output buffer size =     0.00 MiB
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU compute buffer size =    23.00 MiB
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph nodes  = 453
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph splits = 1
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:31:10.238Z level=INFO source=server.go:624 msg="llama runner started in 0.25 seconds"
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/14 - 05:31:10 | 200 |  356.340835ms |       127.0.0.1 | POST     "/api/embed"
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:31:10.364Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.key_length default=128
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:31:10.364Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.value_length default=128
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:31:10.364Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="4.9 GiB" free_swap="0 B"
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:31:10.364Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.key_length default=128
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:31:10.364Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.value_length default=128
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:31:10.365Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=29 layers.offload=0 layers.split="" memory.available="[4.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="1.5 GiB" memory.required.partial="0 B" memory.required.kv="224.0 MiB" memory.required.allocations="[1.5 GiB]" memory.weights.total="976.1 MiB" memory.weights.repeating="793.5 MiB" memory.weights.nonrepeating="182.6 MiB" memory.graph.full="299.8 MiB" memory.graph.partial="482.3 MiB"
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc (version GGUF V3 (latest))
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = qwen2
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.type str              = model
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 1.5B
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                         general.size_label str              = 1.5B
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 1536
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 8960
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 12
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 2
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                          general.file_type u32              = 15
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ä  Ä ", "Ä Ä  Ä Ä ", "i n", "Ä  t",...
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  25:               general.quantization_version u32              = 2
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:  141 tensors
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q4_K:  169 tensors
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q6_K:   29 tensors
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: file type   = Q4_K - Medium
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: file size   = 1.04 GiB (5.00 BPW)
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 22
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.9310 MB
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: arch             = qwen2
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 1
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: model type       = ?B
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: model params     = 1.78 B
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: general.name     = DeepSeek R1 Distill Qwen 1.5B
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: vocab type       = BPE
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 151936
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 151387
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 151646 '<ï½beginâofâsentenceï½>'
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 151643 '<ï½endâofâsentenceï½>'
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: EOT token        = 151643 '<ï½endâofâsentenceï½>'
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 151643 '<ï½endâofâsentenceï½>'
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: LF token         = 198 'Ä'
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: FIM MID token    = 151660 '<|fim_middle|>'
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: FIM PAD token    = 151662 '<|fim_pad|>'
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: FIM REP token    = 151663 '<|repo_name|>'
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: FIM SEP token    = 151664 '<|file_sep|>'
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151643 '<ï½endâofâsentenceï½>'
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151662 '<|fim_pad|>'
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151663 '<|repo_name|>'
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151664 '<|file_sep|>'
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: max token length = 256
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_load: vocab only - skipping tensors
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:31:10.555Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 4 --port 46055"
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:31:10.555Z level=INFO source=sched.go:450 msg="loaded runners" count=2
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:31:10.555Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:31:10.556Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:31:10.566Z level=INFO source=runner.go:931 msg="starting go runner"
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:31:10.608Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:31:10.609Z level=INFO source=runner.go:991 msg="Server listening on 127.0.0.1:46055"
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc (version GGUF V3 (latest))
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = qwen2
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.type str              = model
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 1.5B
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                         general.size_label str              = 1.5B
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 1536
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 8960
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 12
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 2
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                          general.file_type u32              = 15
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ä  Ä ", "Ä Ä  Ä Ä ", "i n", "Ä  t",...
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  25:               general.quantization_version u32              = 2
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:  141 tensors
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q4_K:  169 tensors
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q6_K:   29 tensors
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: file type   = Q4_K - Medium
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: file size   = 1.04 GiB (5.00 BPW)
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 22
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:31:10.806Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server loading model"
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.9310 MB
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: arch             = qwen2
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 0
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: n_ctx_train      = 131072
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: n_embd           = 1536
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: n_layer          = 28
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: n_head           = 12
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: n_head_kv        = 2
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: n_rot            = 128
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: n_swa            = 0
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_k    = 128
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_v    = 128
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: n_gqa            = 6
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: n_embd_k_gqa     = 256
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: n_embd_v_gqa     = 256
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: f_norm_eps       = 0.0e+00
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: f_norm_rms_eps   = 1.0e-06
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: f_clamp_kqv      = 0.0e+00
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: f_max_alibi_bias = 0.0e+00
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: f_logit_scale    = 0.0e+00
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: n_ff             = 8960
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: n_expert         = 0
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: n_expert_used    = 0
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: causal attn      = 1
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: pooling type     = 0
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: rope type        = 2
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: rope scaling     = linear
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: freq_base_train  = 10000.0
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: freq_scale_train = 1
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: n_ctx_orig_yarn  = 131072
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: rope_finetuned   = unknown
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: ssm_d_conv       = 0
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: ssm_d_inner      = 0
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: ssm_d_state      = 0
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_rank      = 0
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_b_c_rms   = 0
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: model type       = 1.5B
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: model params     = 1.78 B
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: general.name     = DeepSeek R1 Distill Qwen 1.5B
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: vocab type       = BPE
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 151936
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 151387
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 151646 '<ï½beginâofâsentenceï½>'
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 151643 '<ï½endâofâsentenceï½>'
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: EOT token        = 151643 '<ï½endâofâsentenceï½>'
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 151643 '<ï½endâofâsentenceï½>'
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: LF token         = 198 'Ä'
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: FIM MID token    = 151660 '<|fim_middle|>'
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: FIM PAD token    = 151662 '<|fim_pad|>'
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: FIM REP token    = 151663 '<|repo_name|>'
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: FIM SEP token    = 151664 '<|file_sep|>'
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151643 '<ï½endâofâsentenceï½>'
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151662 '<|fim_pad|>'
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151663 '<|repo_name|>'
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151664 '<|file_sep|>'
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: print_info: max token length = 256
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: load_tensors: loading model tensors, this can take a while... (mmap = false)
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: load_tensors:          CPU model buffer size =  1059.89 MiB
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_seq_max     = 4
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx         = 8192
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq = 2048
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_batch       = 2048
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ubatch      = 512
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_init_from_model: flash_attn    = 0
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_base     = 10000.0
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_scale    = 1
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
Mar 14 05:31:10 ubuntu-abhishek ollama[471085]: llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1
Mar 14 05:31:11 ubuntu-abhishek ollama[471085]: llama_kv_cache_init:        CPU KV buffer size =   224.00 MiB
Mar 14 05:31:11 ubuntu-abhishek ollama[471085]: llama_init_from_model: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
Mar 14 05:31:11 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU  output buffer size =     2.34 MiB
Mar 14 05:31:11 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU compute buffer size =   302.75 MiB
Mar 14 05:31:11 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph nodes  = 986
Mar 14 05:31:11 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph splits = 1
Mar 14 05:31:11 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:31:11.057Z level=INFO source=server.go:624 msg="llama runner started in 0.50 seconds"
Mar 14 05:32:30 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/14 - 05:32:30 | 200 |         1m20s |       127.0.0.1 | POST     "/api/generate"
Mar 14 05:33:15 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/14 - 05:33:15 | 200 |   61.582704ms |       127.0.0.1 | POST     "/api/embed"
Mar 14 05:35:15 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/14 - 05:35:15 | 200 |          2m0s |       127.0.0.1 | POST     "/api/generate"
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:44:45.966Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="5.5 GiB" free_swap="0 B"
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:44:45.966Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:44:45.966Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.key_length default=64
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:44:45.966Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.value_length default=64
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:44:45.966Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:44:45.966Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=13 layers.offload=0 layers.split="" memory.available="[5.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="352.9 MiB" memory.required.partial="0 B" memory.required.kv="24.0 MiB" memory.required.allocations="[352.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 1
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: print_info: model type       = ?B
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: llama_model_load: vocab only - skipping tensors
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:44:45.999Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 1 --port 33017"
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:44:45.999Z level=INFO source=sched.go:450 msg="loaded runners" count=1
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:44:45.999Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 14 05:44:45 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:44:45.999Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:44:46.010Z level=INFO source=runner.go:931 msg="starting go runner"
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:44:46.052Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:44:46.053Z level=INFO source=runner.go:991 msg="Server listening on 127.0.0.1:33017"
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 0
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: n_ctx_train      = 2048
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: n_embd           = 768
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: n_layer          = 12
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: n_head           = 12
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: n_head_kv        = 12
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: n_rot            = 64
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: n_swa            = 0
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_k    = 64
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_v    = 64
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: n_gqa            = 1
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: n_embd_k_gqa     = 768
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: n_embd_v_gqa     = 768
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: f_norm_eps       = 1.0e-12
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: f_norm_rms_eps   = 0.0e+00
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: f_clamp_kqv      = 0.0e+00
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: f_max_alibi_bias = 0.0e+00
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: f_logit_scale    = 0.0e+00
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: n_ff             = 3072
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: n_expert         = 0
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: n_expert_used    = 0
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: causal attn      = 0
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: pooling type     = 1
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: rope type        = 2
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: rope scaling     = linear
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: freq_base_train  = 1000.0
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: freq_scale_train = 1
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: n_ctx_orig_yarn  = 2048
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: rope_finetuned   = unknown
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: ssm_d_conv       = 0
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: ssm_d_inner      = 0
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: ssm_d_state      = 0
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_rank      = 0
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_b_c_rms   = 0
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: model type       = 137M
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: load_tensors: loading model tensors, this can take a while... (mmap = false)
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: load_tensors:          CPU model buffer size =   260.86 MiB
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_seq_max     = 1
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx         = 8192
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq = 8192
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_batch       = 512
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ubatch      = 512
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_init_from_model: flash_attn    = 0
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_base     = 1000.0
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_scale    = 1
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_init_from_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU  output buffer size =     0.00 MiB
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU compute buffer size =    23.00 MiB
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph nodes  = 453
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph splits = 1
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:44:46.251Z level=INFO source=server.go:624 msg="llama runner started in 0.25 seconds"
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/14 - 05:44:46 | 200 |  330.024805ms |       127.0.0.1 | POST     "/api/embed"
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:44:46.350Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.key_length default=128
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:44:46.350Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.value_length default=128
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:44:46.350Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="4.9 GiB" free_swap="0 B"
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:44:46.350Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.key_length default=128
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:44:46.350Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.value_length default=128
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:44:46.350Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=29 layers.offload=0 layers.split="" memory.available="[4.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="1.5 GiB" memory.required.partial="0 B" memory.required.kv="224.0 MiB" memory.required.allocations="[1.5 GiB]" memory.weights.total="976.1 MiB" memory.weights.repeating="793.5 MiB" memory.weights.nonrepeating="182.6 MiB" memory.graph.full="299.8 MiB" memory.graph.partial="482.3 MiB"
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc (version GGUF V3 (latest))
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = qwen2
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.type str              = model
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 1.5B
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                         general.size_label str              = 1.5B
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 1536
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 8960
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 12
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 2
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                          general.file_type u32              = 15
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ä  Ä ", "Ä Ä  Ä Ä ", "i n", "Ä  t",...
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  25:               general.quantization_version u32              = 2
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:  141 tensors
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q4_K:  169 tensors
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q6_K:   29 tensors
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: file type   = Q4_K - Medium
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: file size   = 1.04 GiB (5.00 BPW)
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 22
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.9310 MB
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: arch             = qwen2
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 1
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: model type       = ?B
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: model params     = 1.78 B
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: general.name     = DeepSeek R1 Distill Qwen 1.5B
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: vocab type       = BPE
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 151936
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 151387
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 151646 '<ï½beginâofâsentenceï½>'
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 151643 '<ï½endâofâsentenceï½>'
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: EOT token        = 151643 '<ï½endâofâsentenceï½>'
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 151643 '<ï½endâofâsentenceï½>'
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: LF token         = 198 'Ä'
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: FIM MID token    = 151660 '<|fim_middle|>'
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: FIM PAD token    = 151662 '<|fim_pad|>'
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: FIM REP token    = 151663 '<|repo_name|>'
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: FIM SEP token    = 151664 '<|file_sep|>'
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151643 '<ï½endâofâsentenceï½>'
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151662 '<|fim_pad|>'
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151663 '<|repo_name|>'
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151664 '<|file_sep|>'
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: max token length = 256
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_load: vocab only - skipping tensors
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:44:46.525Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 4 --port 46569"
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:44:46.525Z level=INFO source=sched.go:450 msg="loaded runners" count=2
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:44:46.525Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:44:46.526Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:44:46.537Z level=INFO source=runner.go:931 msg="starting go runner"
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:44:46.542Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:44:46.542Z level=INFO source=runner.go:991 msg="Server listening on 127.0.0.1:46569"
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc (version GGUF V3 (latest))
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = qwen2
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.type str              = model
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 1.5B
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                         general.size_label str              = 1.5B
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 1536
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 8960
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 12
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 2
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                          general.file_type u32              = 15
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ä  Ä ", "Ä Ä  Ä Ä ", "i n", "Ä  t",...
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  25:               general.quantization_version u32              = 2
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:  141 tensors
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q4_K:  169 tensors
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q6_K:   29 tensors
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: file type   = Q4_K - Medium
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: file size   = 1.04 GiB (5.00 BPW)
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 22
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.9310 MB
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: arch             = qwen2
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 0
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: n_ctx_train      = 131072
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: n_embd           = 1536
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: n_layer          = 28
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: n_head           = 12
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: n_head_kv        = 2
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: n_rot            = 128
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: n_swa            = 0
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_k    = 128
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_v    = 128
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: n_gqa            = 6
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: n_embd_k_gqa     = 256
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: n_embd_v_gqa     = 256
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: f_norm_eps       = 0.0e+00
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: f_norm_rms_eps   = 1.0e-06
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: f_clamp_kqv      = 0.0e+00
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: f_max_alibi_bias = 0.0e+00
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: f_logit_scale    = 0.0e+00
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: n_ff             = 8960
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: n_expert         = 0
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: n_expert_used    = 0
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: causal attn      = 1
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: pooling type     = 0
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: rope type        = 2
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: rope scaling     = linear
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: freq_base_train  = 10000.0
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: freq_scale_train = 1
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: n_ctx_orig_yarn  = 131072
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: rope_finetuned   = unknown
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: ssm_d_conv       = 0
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: ssm_d_inner      = 0
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: ssm_d_state      = 0
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_rank      = 0
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_b_c_rms   = 0
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: model type       = 1.5B
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: model params     = 1.78 B
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: general.name     = DeepSeek R1 Distill Qwen 1.5B
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: vocab type       = BPE
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 151936
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 151387
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 151646 '<ï½beginâofâsentenceï½>'
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 151643 '<ï½endâofâsentenceï½>'
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: EOT token        = 151643 '<ï½endâofâsentenceï½>'
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 151643 '<ï½endâofâsentenceï½>'
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: LF token         = 198 'Ä'
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: FIM MID token    = 151660 '<|fim_middle|>'
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: FIM PAD token    = 151662 '<|fim_pad|>'
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: FIM REP token    = 151663 '<|repo_name|>'
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: FIM SEP token    = 151664 '<|file_sep|>'
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151643 '<ï½endâofâsentenceï½>'
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151662 '<|fim_pad|>'
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151663 '<|repo_name|>'
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151664 '<|file_sep|>'
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: print_info: max token length = 256
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: load_tensors: loading model tensors, this can take a while... (mmap = false)
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: load_tensors:          CPU model buffer size =  1059.89 MiB
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:44:46.777Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server loading model"
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_seq_max     = 4
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx         = 8192
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq = 2048
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_batch       = 2048
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ubatch      = 512
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_init_from_model: flash_attn    = 0
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_base     = 10000.0
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_scale    = 1
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_kv_cache_init:        CPU KV buffer size =   224.00 MiB
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_init_from_model: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU  output buffer size =     2.34 MiB
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU compute buffer size =   302.75 MiB
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph nodes  = 986
Mar 14 05:44:46 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph splits = 1
Mar 14 05:44:47 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:44:47.028Z level=INFO source=server.go:624 msg="llama runner started in 0.50 seconds"
Mar 14 05:44:47 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:44:47.037Z level=WARN source=runner.go:130 msg="truncating input prompt" limit=2048 prompt=2146 keep=5 new=2048
Mar 14 05:46:33 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/14 - 05:46:33 | 200 |         1m47s |       127.0.0.1 | POST     "/api/generate"
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:51:43.719Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="5.5 GiB" free_swap="0 B"
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:51:43.719Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:51:43.719Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.key_length default=64
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:51:43.719Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.value_length default=64
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:51:43.719Z level=WARN source=ggml.go:149 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:51:43.719Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=13 layers.offload=0 layers.split="" memory.available="[5.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="352.9 MiB" memory.required.partial="0 B" memory.required.kv="24.0 MiB" memory.required.allocations="[352.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 1
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: model type       = ?B
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_load: vocab only - skipping tensors
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:51:43.752Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 1 --port 37205"
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:51:43.752Z level=INFO source=sched.go:450 msg="loaded runners" count=1
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:51:43.752Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:51:43.752Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:51:43.765Z level=INFO source=runner.go:931 msg="starting go runner"
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:51:43.804Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:51:43.805Z level=INFO source=runner.go:991 msg="Server listening on 127.0.0.1:37205"
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                          general.file_type u32              = 1
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:   51 tensors
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f16:   61 tensors
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: file type   = F16
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: file size   = 260.86 MiB (16.00 BPW)
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 5
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.2032 MB
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: arch             = nomic-bert
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 0
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: n_ctx_train      = 2048
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: n_embd           = 768
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: n_layer          = 12
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: n_head           = 12
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: n_head_kv        = 12
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: n_rot            = 64
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: n_swa            = 0
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_k    = 64
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_v    = 64
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: n_gqa            = 1
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: n_embd_k_gqa     = 768
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: n_embd_v_gqa     = 768
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: f_norm_eps       = 1.0e-12
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: f_norm_rms_eps   = 0.0e+00
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: f_clamp_kqv      = 0.0e+00
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: f_max_alibi_bias = 0.0e+00
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: f_logit_scale    = 0.0e+00
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: n_ff             = 3072
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: n_expert         = 0
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: n_expert_used    = 0
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: causal attn      = 0
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: pooling type     = 1
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: rope type        = 2
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: rope scaling     = linear
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: freq_base_train  = 1000.0
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: freq_scale_train = 1
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: n_ctx_orig_yarn  = 2048
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: rope_finetuned   = unknown
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: ssm_d_conv       = 0
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: ssm_d_inner      = 0
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: ssm_d_state      = 0
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_rank      = 0
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_b_c_rms   = 0
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: model type       = 137M
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: model params     = 136.73 M
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: general.name     = nomic-embed-text-v1.5
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: vocab type       = WPM
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 30522
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 0
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 101 '[CLS]'
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 102 '[SEP]'
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: UNK token        = 100 '[UNK]'
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: SEP token        = 102 '[SEP]'
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 0 '[PAD]'
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: MASK token       = 103 '[MASK]'
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: LF token         = 0 '[PAD]'
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 102 '[SEP]'
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: print_info: max token length = 21
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: load_tensors: loading model tensors, this can take a while... (mmap = false)
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: load_tensors:          CPU model buffer size =   260.86 MiB
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_seq_max     = 1
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx         = 8192
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq = 8192
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_batch       = 512
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ubatch      = 512
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_init_from_model: flash_attn    = 0
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_base     = 1000.0
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_scale    = 1
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_kv_cache_init:        CPU KV buffer size =   288.00 MiB
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_init_from_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU  output buffer size =     0.00 MiB
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU compute buffer size =    23.00 MiB
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph nodes  = 453
Mar 14 05:51:43 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph splits = 1
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:51:44.003Z level=INFO source=server.go:624 msg="llama runner started in 0.25 seconds"
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/14 - 05:51:44 | 200 |  343.500147ms |       127.0.0.1 | POST     "/api/embed"
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:51:44.118Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.key_length default=128
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:51:44.118Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.value_length default=128
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:51:44.118Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="4.9 GiB" free_swap="0 B"
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:51:44.118Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.key_length default=128
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:51:44.118Z level=WARN source=ggml.go:149 msg="key not found" key=qwen2.attention.value_length default=128
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:51:44.119Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=29 layers.offload=0 layers.split="" memory.available="[4.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="1.5 GiB" memory.required.partial="0 B" memory.required.kv="224.0 MiB" memory.required.allocations="[1.5 GiB]" memory.weights.total="976.1 MiB" memory.weights.repeating="793.5 MiB" memory.weights.nonrepeating="182.6 MiB" memory.graph.full="299.8 MiB" memory.graph.partial="482.3 MiB"
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc (version GGUF V3 (latest))
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = qwen2
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.type str              = model
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 1.5B
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                         general.size_label str              = 1.5B
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 1536
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 8960
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 12
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 2
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                          general.file_type u32              = 15
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ä  Ä ", "Ä Ä  Ä Ä ", "i n", "Ä  t",...
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  25:               general.quantization_version u32              = 2
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:  141 tensors
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q4_K:  169 tensors
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q6_K:   29 tensors
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: file type   = Q4_K - Medium
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: file size   = 1.04 GiB (5.00 BPW)
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 22
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.9310 MB
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: arch             = qwen2
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 1
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: model type       = ?B
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: model params     = 1.78 B
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: general.name     = DeepSeek R1 Distill Qwen 1.5B
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: vocab type       = BPE
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 151936
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 151387
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 151646 '<ï½beginâofâsentenceï½>'
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 151643 '<ï½endâofâsentenceï½>'
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: EOT token        = 151643 '<ï½endâofâsentenceï½>'
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 151643 '<ï½endâofâsentenceï½>'
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: LF token         = 198 'Ä'
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: FIM MID token    = 151660 '<|fim_middle|>'
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: FIM PAD token    = 151662 '<|fim_pad|>'
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: FIM REP token    = 151663 '<|repo_name|>'
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: FIM SEP token    = 151664 '<|file_sep|>'
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151643 '<ï½endâofâsentenceï½>'
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151662 '<|fim_pad|>'
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151663 '<|repo_name|>'
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151664 '<|file_sep|>'
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: max token length = 256
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_load: vocab only - skipping tensors
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:51:44.295Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /usr/share/ollama/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 4 --port 41093"
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:51:44.295Z level=INFO source=sched.go:450 msg="loaded runners" count=2
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:51:44.295Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:51:44.295Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:51:44.308Z level=INFO source=runner.go:931 msg="starting go runner"
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:51:44.312Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:51:44.313Z level=INFO source=runner.go:991 msg="Server listening on 127.0.0.1:41093"
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from /usr/share/ollama/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc (version GGUF V3 (latest))
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   0:                       general.architecture str              = qwen2
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   1:                               general.type str              = model
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 1.5B
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   4:                         general.size_label str              = 1.5B
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   5:                          qwen2.block_count u32              = 28
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 1536
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 8960
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 12
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 2
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 10000.000000
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  13:                          general.file_type u32              = 15
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ä  Ä ", "Ä Ä  Ä Ä ", "i n", "Ä  t",...
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - kv  25:               general.quantization_version u32              = 2
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - type  f32:  141 tensors
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q4_K:  169 tensors
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_model_loader: - type q6_K:   29 tensors
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: file format = GGUF V3 (latest)
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: file type   = Q4_K - Medium
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: file size   = 1.04 GiB (5.00 BPW)
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: load: special tokens cache size = 22
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: load: token to piece cache size = 0.9310 MB
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: arch             = qwen2
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: vocab_only       = 0
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: n_ctx_train      = 131072
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: n_embd           = 1536
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: n_layer          = 28
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: n_head           = 12
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: n_head_kv        = 2
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: n_rot            = 128
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: n_swa            = 0
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_k    = 128
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: n_embd_head_v    = 128
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: n_gqa            = 6
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: n_embd_k_gqa     = 256
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: n_embd_v_gqa     = 256
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: f_norm_eps       = 0.0e+00
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: f_norm_rms_eps   = 1.0e-06
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: f_clamp_kqv      = 0.0e+00
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: f_max_alibi_bias = 0.0e+00
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: f_logit_scale    = 0.0e+00
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: n_ff             = 8960
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: n_expert         = 0
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: n_expert_used    = 0
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: causal attn      = 1
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: pooling type     = 0
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: rope type        = 2
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: rope scaling     = linear
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: freq_base_train  = 10000.0
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: freq_scale_train = 1
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: n_ctx_orig_yarn  = 131072
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: rope_finetuned   = unknown
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: ssm_d_conv       = 0
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: ssm_d_inner      = 0
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: ssm_d_state      = 0
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_rank      = 0
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: ssm_dt_b_c_rms   = 0
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: model type       = 1.5B
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: model params     = 1.78 B
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: general.name     = DeepSeek R1 Distill Qwen 1.5B
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: vocab type       = BPE
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: n_vocab          = 151936
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: n_merges         = 151387
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: BOS token        = 151646 '<ï½beginâofâsentenceï½>'
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: EOS token        = 151643 '<ï½endâofâsentenceï½>'
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: EOT token        = 151643 '<ï½endâofâsentenceï½>'
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: PAD token        = 151643 '<ï½endâofâsentenceï½>'
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: LF token         = 198 'Ä'
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: FIM MID token    = 151660 '<|fim_middle|>'
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: FIM PAD token    = 151662 '<|fim_pad|>'
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: FIM REP token    = 151663 '<|repo_name|>'
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: FIM SEP token    = 151664 '<|file_sep|>'
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151643 '<ï½endâofâsentenceï½>'
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151662 '<|fim_pad|>'
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151663 '<|repo_name|>'
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: EOG token        = 151664 '<|file_sep|>'
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: print_info: max token length = 256
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: load_tensors: loading model tensors, this can take a while... (mmap = false)
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: load_tensors:          CPU model buffer size =  1059.89 MiB
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:51:44.546Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server loading model"
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_seq_max     = 4
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx         = 8192
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq = 2048
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_batch       = 2048
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ubatch      = 512
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_init_from_model: flash_attn    = 0
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_base     = 10000.0
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_init_from_model: freq_scale    = 1
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_kv_cache_init:        CPU KV buffer size =   224.00 MiB
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_init_from_model: KV self size  =  224.00 MiB, K (f16):  112.00 MiB, V (f16):  112.00 MiB
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU  output buffer size =     2.34 MiB
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_init_from_model:        CPU compute buffer size =   302.75 MiB
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph nodes  = 986
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: llama_init_from_model: graph splits = 1
Mar 14 05:51:44 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:51:44.797Z level=INFO source=server.go:624 msg="llama runner started in 0.50 seconds"
Mar 14 05:51:55 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/14 - 05:51:55 | 200 | 11.085910954s |       127.0.0.1 | POST     "/api/generate"
Mar 14 05:55:04 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/14 - 05:55:04 | 200 |   59.194358ms |       127.0.0.1 | POST     "/api/embed"
Mar 14 05:55:04 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:55:04.085Z level=WARN source=runner.go:130 msg="truncating input prompt" limit=2048 prompt=4090 keep=5 new=2048
Mar 14 05:57:05 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/14 - 05:57:05 | 200 |          2m1s |       127.0.0.1 | POST     "/api/generate"
Mar 14 05:58:27 ubuntu-abhishek ollama[471085]: [GIN] 2025/03/14 - 05:58:27 | 200 |   60.601058ms |       127.0.0.1 | POST     "/api/embed"
Mar 14 05:58:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:27.633Z level=INFO source=server.go:105 msg="system memory" total="7.7 GiB" free="3.5 GiB" free_swap="0 B"
Mar 14 05:58:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:27.633Z level=INFO source=server.go:138 msg=offload library=cpu layers.requested=-1 layers.model=27 layers.offload=0 layers.split="" memory.available="[3.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="1.5 GiB" memory.required.partial="0 B" memory.required.kv="208.0 MiB" memory.required.allocations="[1.5 GiB]" memory.weights.total="664.5 MiB" memory.weights.repeating="358.5 MiB" memory.weights.nonrepeating="306.0 MiB" memory.graph.full="514.2 MiB" memory.graph.partial="750.5 MiB"
Mar 14 05:58:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:27.734Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 14 05:58:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:27.738Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
Mar 14 05:58:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:27.742Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 14 05:58:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:27.742Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 14 05:58:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:27.742Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.num_channels default=0
Mar 14 05:58:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:27.742Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.block_count default=0
Mar 14 05:58:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:27.742Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.embedding_length default=0
Mar 14 05:58:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:27.742Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.head_count default=0
Mar 14 05:58:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:27.742Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 14 05:58:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:27.742Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 14 05:58:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:27.742Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.layer_norm_epsilon default=0
Mar 14 05:58:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:27.742Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 14 05:58:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:27.748Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.freq_scale default=1
Mar 14 05:58:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:27.748Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.mm_tokens_per_image default=256
Mar 14 05:58:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:27.749Z level=INFO source=server.go:405 msg="starting llama server" cmd="/usr/local/bin/ollama runner --ollama-engine --model /usr/share/ollama/.ollama/models/blobs/sha256-dbe81da1e4bad3cc6ccb77540915ffe53e7a3ac0745ffa5e9d4626d39c15e09a --ctx-size 8192 --batch-size 512 --threads 2 --no-mmap --parallel 4 --port 37287"
Mar 14 05:58:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:27.749Z level=INFO source=sched.go:450 msg="loaded runners" count=3
Mar 14 05:58:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:27.749Z level=INFO source=server.go:585 msg="waiting for llama runner to start responding"
Mar 14 05:58:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:27.749Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server error"
Mar 14 05:58:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:27.759Z level=INFO source=runner.go:882 msg="starting ollama engine"
Mar 14 05:58:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:27.760Z level=INFO source=runner.go:938 msg="Server listening on 127.0.0.1:37287"
Mar 14 05:58:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:27.861Z level=WARN source=ggml.go:149 msg="key not found" key=general.name default=""
Mar 14 05:58:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:27.861Z level=WARN source=ggml.go:149 msg="key not found" key=general.description default=""
Mar 14 05:58:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:27.861Z level=INFO source=ggml.go:67 msg="" architecture=gemma3 file_type=Q4_K_M name="" description="" num_tensors=340 num_key_values=32
Mar 14 05:58:27 ubuntu-abhishek ollama[471085]: load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-haswell.so
Mar 14 05:58:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:27.904Z level=INFO source=ggml.go:109 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 compiler=cgo(gcc)
Mar 14 05:58:27 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:27.906Z level=INFO source=ggml.go:289 msg="model weights" buffer=CPU size="1.0 GiB"
Mar 14 05:58:28 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:28.076Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server loading model"
Mar 14 05:58:29 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:29.662Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server not responding"
Mar 14 05:58:29 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:29.914Z level=INFO source=server.go:619 msg="waiting for server to become available" status="llm server loading model"
Mar 14 05:58:32 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:32.356Z level=INFO source=ggml.go:356 msg="compute graph" backend=CPU buffer_type=CPU
Mar 14 05:58:32 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:32.362Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 14 05:58:32 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:32.367Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.add_eot_token default=false
Mar 14 05:58:32 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:32.373Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 14 05:58:32 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:32.373Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 14 05:58:32 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:32.373Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.num_channels default=0
Mar 14 05:58:32 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:32.373Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.block_count default=0
Mar 14 05:58:32 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:32.373Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.embedding_length default=0
Mar 14 05:58:32 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:32.373Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.head_count default=0
Mar 14 05:58:32 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:32.373Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.image_size default=0
Mar 14 05:58:32 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:32.373Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.patch_size default=0
Mar 14 05:58:32 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:32.373Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.vision.attention.layer_norm_epsilon default=0
Mar 14 05:58:32 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:32.373Z level=WARN source=ggml.go:149 msg="key not found" key=tokenizer.ggml.pretokenizer default="(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+"
Mar 14 05:58:32 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:32.382Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.rope.freq_scale default=1
Mar 14 05:58:32 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:32.382Z level=WARN source=ggml.go:149 msg="key not found" key=gemma3.mm_tokens_per_image default=256
Mar 14 05:58:32 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:32.422Z level=INFO source=server.go:624 msg="llama runner started in 4.67 seconds"
Mar 14 05:58:32 ubuntu-abhishek ollama[471085]: time=2025-03-14T05:58:32.466Z level=WARN source=runner.go:122 msg="truncating input prompt" limit=2048 prompt=4384 keep=4 new=2048
